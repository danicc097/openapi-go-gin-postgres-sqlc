#!/bin/bash
# shellcheck disable=1091,2155,2068,2086,2031

export PROC=$$

source "${BASH_SOURCE%/*}/.helpers.sh"
source "${BASH_SOURCE%/*}/scripts/deps-check.sh"

# FIXME m flag will prevent background process killing with ctrl-c
set -Eeo pipefail

trap killgroup SIGINT

trap errtrap SIGUSR1

killgroup() {
  printf "\nkilling spawned processes...\n"

  # kill $(jobs -p) 2>/dev/null # doesn't really kill all children, just process group leaders
  pgrep -P $PROC | xargs kill
  exit 1
}

errtrap() {
  printf "\nExiting due to propagated error...\n"
  exit 1
}

ensure_pwd_is_top_level

source ".envrc"

readonly SPEC="openapi.yaml"
readonly PROTO_DIR="internal/pb"
readonly MIGRATIONS_DIR="db/migrations"
readonly PG_REPO="internal/repos/postgresql"
readonly PG_REPO_GEN="internal/repos/postgresql/gen"
readonly GEN_SCHEMA_CMD="cmd/gen-schema"
readonly MAX_COMMENT_LEN=88
readonly MAX_FNAME_LOG_LEN=13
readonly DUMPS_FOLDER="$HOME/openapi_go_gin_postgres_dumps"
readonly GEN_POSTGRES_DB="gen_db"
pkg="$(head -1 go.mod)"
readonly GOMOD_PKG="${pkg#module *}"

pids=()
env="dev"
dump_prefix="dump_${env}_"

# log for any function output.
xlog() {
  fname="${FUNCNAME[1]#*.}"
  [[ "$CMD" = "$fname" ]] && cat && return
  if [[ ${#fname} -gt $MAX_FNAME_LOG_LEN ]]; then
    fname="${fname:0:$MAX_FNAME_LOG_LEN}â€¦"
  fi

  local fn=$(printf "%*s |\n" $((MAX_FNAME_LOG_LEN + 1)) "$fname")
  sed -ue "s/^/${BLUE}$fname >${OFF} /"
}

# log stderr for any function output.
# sed is buffering by default (without -u) so streams dont preserve order
# > >(one) 2> >(two) are background processes so it will break our parallel code.
xerr() {
  fname="${FUNCNAME[1]#*.}"
  [[ "$CMD" = "$fname" ]] && cat && return
  if [[ ${#fname} -gt $MAX_FNAME_LOG_LEN ]]; then
    fname="${fname:0:$MAX_FNAME_LOG_LEN}â€¦"
  fi

  local fn=$(printf "%*s |\n" $((MAX_FNAME_LOG_LEN + 1)) "$fname")
  sed -ue "s/^/${RED}$fname >${OFF} /" >&2
}

# Check build dependencies are met.
x.check-build-deps() {
  { { {
    local -i fails
    check.column || { ((fails++)) && true; }
    check.protoc || { ((fails++)) && true; }
    check.bash || { ((fails++)) && true; }
    check.go || { ((fails++)) && true; }
    check.curl || { ((fails++)) && true; }
    check.docker || { ((fails++)) && true; }
    check.docker-compose || { ((fails++)) && true; }
    check.direnv || { ((fails++)) && true; }
    check.yq || { ((fails++)) && true; }
    check.pg_format || { ((fails++)) && true; }
    ((fails == 0)) && echo "${GREEN}ðŸŽ‰ All build dependencies met.${OFF}"
    { ((fails != 0)) && err "${RED}âŒ Missing dependencies.${OFF}"; } || true
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Check dependencies and fetch required tools.
x.bootstrap() {
  { { {
    x.check-build-deps
    x.install-tools
    x.fetch.swagger-ui
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Install go libraries as runnable programs.
x.install-tools() {
  { { {
    go install -tags 'postgres' github.com/golang-migrate/migrate/v4/cmd/migrate@v4.15.2
    # go install github.com/kyleconroy/sqlc/cmd/sqlc@v1.16.0
    # TODO until merged
    go install github.com/danicc097/sqlc/cmd/sqlc@custom
    go install github.com/golangci/golangci-lint/cmd/golangci-lint@v1.47.2
    go install github.com/joho/godotenv/cmd/godotenv@latest
    go install github.com/tufin/oasdiff@latest
    go install golang.org/x/tools/cmd/goimports@latest
    go install mvdan.cc/gofumpt@latest
    go install github.com/danicc097/air@latest
    go install github.com/danicc097/xo@latest
    go install github.com/mikefarah/yq/v4@v4.27.2
    go install github.com/hexdigest/gowrap/cmd/gowrap@latest

    go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.28.1
    go install github.com/planetscale/vtprotobuf/cmd/protoc-gen-go-vtproto@v0.2.0
    go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.2

    GO111MODULE=off go get -u github.com/maxbrunsfeld/counterfeiter
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Fetch latest Swagger UI bundle.
x.fetch.swagger-ui() {
  { { {
    name="$(curl --silent "https://api.github.com/repos/swagger-api/swagger-ui/releases/latest" | jq -r ".. .tag_name? // empty")"
    curl -fsSL "github.com/swagger-api/swagger-ui/archive/refs/tags/$name.tar.gz" -o swagger-ui.tar.gz
    tar xf swagger-ui.tar.gz swagger-ui-"${name#*v}"/dist --one-top-level=swagger-ui --strip-components=2
    rm swagger-ui.tar.gz
    mkdir -p internal/static/swagger-ui
    mv swagger-ui/* internal/static/swagger-ui/
    rm -r swagger-ui
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Run pre-generation scripts in the internal package.
x.pregen() {
  { { {
    echo "Running generation"

    x.db.drop
    x.migrate up

    ######## Sync enums from db with spec
    # example: `x-db-enum: user_role` will use the db's "user_role" enum
    vext="x-db-enum"
    for enum in $(yq e ".. | select(has(\"$vext\")).$vext" $SPEC); do
      local db_enums=()
      path="$(enum_name=$enum yq e ".. | select(has(\"$vext\") and .$vext == env(enum_name)) | (path | join(\".\"))" $SPEC)"
      mapfile -t db_enums < <(dockerdb_psql -d $POSTGRES_DB -c "SELECT unnest(enum_range(NULL::\"$enum\"));" 2>/dev/null)

      [[ ${#db_enums[@]} -gt 0 ]] || err "$vext: Enum '$enum' not found in database $POSTGRES_DB"
      enums=$(printf ",\"%s\"" "${db_enums[@]}")
      enums="[${enums:1}]"
      echo "Replacing enum '$enum' in $SPEC with: $enums"
      enums=$enums yq e ".$path.enum = env(enums)" -i $SPEC
    done

    ######## Ensure consistent style for future codegen
    echo "Applying PascalCase to operation IDs in $SPEC"
    for opid in $(yq e ".. | select(has(\"operationId\")).operationId" $SPEC); do
      new_opid="$(to_pascal $opid)"
      paths=$(opid=$opid yq e ".. | select(has(\"operationId\") and .operationId == env(opid)) | (path)" $SPEC)
      readarray -t paths <<<$paths
      escaped_path="$(join_yq_paths paths)"
      new_opid=$new_opid yq e ".$escaped_path.operationId = env(new_opid)" -i $SPEC
    done

    ######## Sync enums with external sources and validate
    ######## external json files are the source of truth
    # frontend scopes: 1) replace all "-_" by " " 2) Sort 3) range over and split(":") -> map[left] = append(right) and show aggregated
    # arrays can't be nested
    declare -A enum_src_files=(
      [Scope]="scopes.json"
      [Role]="roles.json"
    )
    declare -A enum_vext=(
      [Scope]="x-required-scopes"
      [Role]="x-required-role"
    )
    for enum in ${!enum_src_files[@]}; do
      [[ $(yq e ".components.schemas | has(\"$enum\")" $SPEC) = "false" ]] &&
        yq e ".components.schemas.$enum.type = \"string\"" -i $SPEC

      src_file="${enum_src_files[$enum]}"
      vext="${enum_vext[$enum]}"

      enums=$(yq -P '.[] | key' $src_file)
      readarray -t enums <<<$enums

      echo "Replacing '$enum' enum in $SPEC with $src_file"
      yq e "
        (.components.schemas.$enum | key) line_comment=\"Generated from $src_file keys. DO NOT EDIT.\"
      " -i $SPEC
      enums_arr=$(printf ",\"%s\"" "${enums[@]}")
      enums_arr="[${enums_arr:1}]"
      enums_arr=$enums_arr yq e ".components.schemas.$enum.enum = env(enums_arr)" -i $SPEC

      readarray spec_enums < <(yq e ".paths[][].$vext | select(length > 0)" openapi.yaml)
      spec_enums=("${spec_enums[*]//- /}")
      mapfile -t spec_enums < <(printf "\"%s\"\n" ${spec_enums[*]})
      mapfile -t clean_enums < <(printf "\"%s\"\n" ${enums[*]})
      for spec_enum in "${spec_enums[@]}"; do
        [[ ! " ${clean_enums[*]} " =~ " ${spec_enum} " ]] && err "$spec_enum is not a valid '$enum'"
      done
    done

    ######## Generate shared policies once the spec has been validated
    # https://mikefarah.gitbook.io/yq/operators/reduce
    opid_auth="operationAuthorization.gen.json"
    echo "Writing shared auth policies"
    echo "// Code generated by '$(basename $0) ${FUNCNAME[0]##*.}'. DO NOT EDIT." >$opid_auth
    yq -o=json e "
    .paths[][]
    | {.operationId: { \"scopes\": .x-required-scopes, \"role\": .x-required-role}}
    | select(.[]) as \$i ireduce ({}; . + \$i)
  " openapi.yaml >>$opid_auth

    yq --prettyPrint '.. style="single"' -i $SPEC
    sed -i 's/!!merge //' $SPEC

    # https://mikefarah.gitbook.io/yq/operators/anchor-and-alias-operators
    # but some tooling is not aware of this and raises errors, e.g. openapi-typescript-codegen, vscode yaml

    # don't want this to block when getting bad gen
    go build -o pregen "$PWD"/cmd/pregen/main.go 2>/dev/null || echo "Could not rebuild pregen" >&2

    ./pregen -env=".env.$env"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# outputs safe double-quoted paths for yq
# fixes paths where a dot is present
join_yq_paths() {
  declare -n arr="$1"
  arr=("${arr[*]//- /}")
  mapfile -t arr < <(printf "\"%s\"\n" ${arr[*]})
  join_by "." ${arr[*]}
}

clean_yq_array() {
  declare -n arr="$1"
  arr=("${arr[*]//- /}")
  mapfile -t arr < <(printf "\"%s\"\n" ${arr[*]})
  echo ${arr[@]}
}

# Run post-generation scripts in the internal package.
x.postgen() {
  { { {
    echo "Running generation"

    generate_db_structs_map

    update_spec_with_db_structs

    go build -o postgen "$PWD"/cmd/postgen/main.go
    ./postgen -env=".env.$env"
  } \
    2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1

}

# Generate type-safe Go code from SQL.
x.gen.sqlc() {
  { { {
    echo "Running generation"
    sqlc generate -f "$PG_REPO"/sqlc.yaml || err "Failed sqlc generation"
    rm "$PG_REPO_GEN"/db/models.go # sqlc enums

  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Automatically generate CRUD and index queries with joins based on existing indexes from a Postgres schema.
x.gen.xo() {
  { { {
    echo "Running generation"
    # xo schema --help

    mkdir -p "$PG_REPO_GEN"/db
    xo_schema -o "$PG_REPO_GEN"/db \
      --ignore "*.created_at" \
      --ignore "*.updated_at" || err "Failed xo public schema generation" &

    mkdir -p "$PG_REPO_GEN"/db/v
    xo_schema -o "$PG_REPO_GEN"/db/v \
      --schema v \
      --main-schema-pkg $GOMOD_PKG/"$PG_REPO_GEN"/db ||
      err "Failed xo v schema generation" &

    mkdir -p "$PG_REPO_GEN"/db/cache
    xo_schema -o "$PG_REPO_GEN"/db/cache \
      --schema cache \
      --main-schema-pkg $GOMOD_PKG/"$PG_REPO_GEN"/db ||
      err "Failed xo cache schema generation" &

    wait
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Generate interface wrappers with common logic: tracing, timeout...
x.gen.gowrap() {
  { { {
    echo "Running generation"
    repo_interfaces="User"
    for iface in ${repo_interfaces[@]}; do
      gowrap gen \
        -g \
        -p $GOMOD_PKG/internal/repos \
        -i $iface \
        -t internal/gowrap-templates/otel-with-timeout.tmpl \
        -o internal/repos/${iface,,}_with_tracing_and_timeout.gen.go
    done
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Generate Go client and server.
x.gen.client-server() {
  { { {
    local paths types

    go build -o oapi-codegen "$PWD"/cmd/oapi-codegen

    echo "Running generation"
    paths=".openapi.paths.yaml"
    types=".openapi.types.yaml"

    sed "s/\$ref: '\#\//\$ref: '$types\#\//g" $SPEC >$paths
    yq e 'del(.components)' -i $paths
    cp $SPEC $types
    # yq e 'with_entries(select(.key == "components"))' $SPEC >$types # must use spec itself else not all types gen'ed

    ./oapi-codegen --config internal/models/oapi-codegen-types.yaml "$SPEC" || err "Failed types generation"
    ./oapi-codegen --config internal/rest/oapi-codegen-server.yaml --models-pkg models "$paths" || err "Failed server generation"
    ./oapi-codegen --config internal/client/oapi-codegen-client.yaml "$SPEC" || err "Failed client generation"

  } \
    2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Generate mocks for specified interfaces. Runs are cached to `cachedir`.
# Args: cachedir
x.gen.counterfeiter() {
  # This shouldn't pose any problems, the interface is the only input to counterfeiter.
  { { {
    cache="$1"

    envvar="internal/envvar/envvar.go"
    repos="internal/repos/repos.go"
    tfidfpb="internal/pb/python-ml-app-protos/tfidf/v1/service_grpc.pb.go"

    echo "Running generation"

    if ! md5sum -c "$cache/counterfeiter.md5" >/dev/null || [[ $force_regen -eq 1 ]]; then
      echo "Recreating mocks"
      {
        counterfeiter -o internal/envvar/envvartesting/provider.gen.go $envvar Provider &
        counterfeiter -o internal/repos/repostesting/user.gen.go $repos User &
        counterfeiter -o internal/pb/python-ml-app-protos/tfidf/v1/v1testing/movie_genre_client.gen.go $tfidfpb MovieGenreClient &
        counterfeiter -o internal/pb/python-ml-app-protos/tfidf/v1/v1testing/movie_genre_server.gen.go $tfidfpb MovieGenreServer &
        md5sum $envvar $repos $tfidfpb >"$cache/counterfeiter.md5" &
        wait
      } 2>&1 # outputs to stderr for some reason

      # counterfeiter is unaware of grpc's obscure mustEmbedUnimplemented***() for forward server compatibility
      sed -i '/type FakeMovieGenreServer struct {/a v1\.UnimplementedMovieGenreServer' \
        internal/pb/python-ml-app-protos/tfidf/v1/v1testing/movie_genre_server.gen.go
    fi
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Generate the required servers/clients for relevant services.
x.gen.proto() {
  { { {
    local import_path="python-ml-app-protos/tfidf/v1"
    local filename="internal/python-ml-app-protos/tfidf/v1/service.proto"

    mkdir -p internal/pb
    # Plugins are no longer supported by protoc-gen-go.
    # Instead protoc-gen-go-grpc and the go package (in proto or via M flag) are required
    echo "Running generation"
    protoc \
      --go-grpc_out=internal/pb/. \
      --go_out=internal/pb/. \
      --go-grpc_opt=M$filename=$import_path,paths=import \
      --go_opt=M$filename=$import_path,paths=import \
      internal/python-ml-app-protos/tfidf/v1/service.proto || err "Failed proto generation"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Run all codegen and postgen commands for the project.
x.gen() {
  { { {
    echo "Running code generation"
    # must always be consistent regardless of current env
    (
      POSTGRES_DB=$GEN_POSTGRES_DB

      local cache=".generate.cache"

      mkdir -p "$cache"

      x.db.recreate

      x.pregen

      rm -rf "$PG_REPO_GEN"

      # TODO try use gnu parallel and exit when anyone fails
      # (with current setup the whole x.gen is executed and only then
      # the trap on SIGUSR1 coming from `err` is run - now temporarily using kill 0 in `err` instead on sending SIGUSR1)

      go generate ./... &
      x.gen.gowrap &
      x.gen.sqlc &
      # xo cannot use db files as input, needs an up-to-date schema
      # not recreating db on every gen can lead to plain wrong generation based on an old dev schema
      # use a unique db to prevent cosmic accidents
      x.gen.proto &
      wait

      x.gen.xo

      x.postgen &
      x.gen.counterfeiter "$cache" & # delay since it depends on generated output too

      x.gen.client-server # depends on schemas generated in postgen

      wait

      # vscode will randomly lose connection when restarting
      # for pid in $(pidof gopls); do
      #   restart_pid $pid &
      # done

      x.lint
    )
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Lint the entire project.
x.lint() {
  { { {
    x.lint.sql &
    x.lint.go &
    wait
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Format Go files.
x.lint.go() {
  { { {
    files=$(find . \
      -not -path "**/$PROTO_DIR/*" \
      -not -path "**/$PG_REPO_GEN/*" \
      -not -path "**/testdata/*" \
      -not -path "**/*.cache/*" \
      -not -path "**/*.gen.*" \
      -name "*.go")

    goimports -w $files || echo Linting failed
    gofumpt -w $files || echo Linting failed
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Format SQL files.
x.lint.sql() {
  { { {
    SQL_DIRS=(
      "$PG_REPO/queries"
      "db"
    )
    for slq_dir in ${SQL_DIRS[@]}; do
      pg_format \
        --spaces 2 \
        --wrap-limit 130 \
        --function-case 2 \
        --keyword-case 1 \
        --placeholder "sqlc\\.(arg|narg)\\(:?[^)]*\\)" \
        --inplace \
        --nogrouping \
        --keep-newline \
        --comma-start \
        $(find "$slq_dir" -maxdepth 3 -name '*.*sql')
    done
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Run required backend pre-test setup: services, database cleanup, codegen...
# Can be called independently, e.g. before running tests through an IDE.
x.test.backend-setup() {
  { { {
    # NOTE: tests run independently in Go so we can't have a function be called and run
    # only once before any test starts
    run_shared_services up -d --build --remove-orphans
    [[ -z $NO_GEN ]] && x.gen
    drop_and_recreate_db "postgres_test"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Test the entire project. Accepts `go test` parameters.
# Args: [...]
x.test() {
  { { {
    APP_ENV="$env" go test "$@" ./...
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Test and build the entire project.
x.build() {
  { { {
    x.lint
    x.test ""
    go build -o rest-server "$PWD"/cmd/rest-server
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Run backend with hot-reloading.
x.run.backend-hr() {
  run_shared_services up -d --build --remove-orphans
  # TODO new include_files flag in fork, e.g. for openapi.yaml.
  # else generated .yaml files trigger rebuild.
  # --build.include_files "openapi.yaml" \
  # NOTE: building binary very unreliable, leads to bin not found.
  [[ -z $NO_GEN ]] && pre_build_cmd="bin/project gen"
  air \
    --build.pre_build_cmd "$pre_build_cmd" \
    --build.cmd "" \
    --build.bin "go run ./cmd/rest-server/ -env=.env.$env" \
    --build.include_ext "go" \
    --build.exclude_regex ".gen.go,_test.go" \
    --build.exclude_dir ".git,tmp,$PROTO_DIR,"$PG_REPO_GEN",**/testdata,frontend,*.cache" \
    --build.stop_watch "internal/rest/,internal/services/" \
    --build.delay 1000 \
    --build.exclude_unchanged "true" |
    sed -e "s/^/${BLUE}[Air]${OFF} /"
}

# Run frontend with hot-reloading.
x.run.frontend() {
  set -a
  # export to replace config with envvars
  source ".env.$env"
  set +a

  cd frontend
  pnpm run generate

  pnpm run dev |
    sed -e "s/^/${GREEN}[Vite]${OFF} /"
}

# Run all project services with hot reload enabled in dev mode.
x.run-dev() {
  env="dev"

  run_hot_reload

  next_allowed_run=$(date +%s)
  latency=3
  # close_write event, else duplicated, tripl. events -> race condition
  while true; do
    inotifywait \
      --monitor $SPEC \
      --event=close_write \
      --format='%T %f' \
      --timefmt='%s' |
      while read -r event_time event_file 2>/dev/null || sleep $latency; do
        if [[ $event_time -ge $next_allowed_run ]]; then
          next_allowed_run=$(date --date="${latency}sec" +%s)

          for pid in "${pids[@]}"; do
            # air and vite spawn processes as well, need to kill those (whose parent is pid), kill $pid will not kill children. pkill -P would also work
            list_descendants $pid
            kill "$(list_descendants $pid)"
          done
          pids=()

          run_hot_reload
        fi
      done
  done
}

# Run project in production mode.
x.run-prod() {
  env="prod"

  docker network create traefik-net 2>/dev/null || true
  run_shared_services up -d --build --remove-orphans
  x.db.recreate

  cd frontend && pnpm run generate && cd - >/dev/null
  x.gen

  DOCKER_BUILDKIT=1 BUILDKIT_PROGRESS=plain docker-compose \
    --project-name "$PROJECT_PREFIX"_"$env" \
    -f docker-compose."$env".yml \
    --env-file ".env.$env" \
    up -d --build 2>&1 # https://github.com/docker/compose/issues/7346

  echo "Migrations:"
  x.migrate up
}

# Remove running project containers, including shared ones between environments.
x.stop-project() {
  { { {
    run_shared_services down --remove-orphans
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Checks before release:
# - Magic keyword "STOPSHIP" not found in tracked files.
x.release() {
  { { {
    x.search-stopship "STOPSHIP" &
    go mod verify &

    wait
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Block build if magic keyword is found in any file
# Args: keyword
x.search-stopship() {
  { { {
    stopship_keyword="$1"
    local matches
    matches=$(find "$(git rev-parse --show-toplevel)" \
      -type f \
      -not -path '**/.git/*' \
      -not -path '**/.venv/*' \
      -not -path '**/node_modules/*' \
      -not -path '**/build/*' \
      -not -path '**/*.pyc' \
      -not -path "$0" \
      -not -exec git check-ignore -q --no-index {} \; \
      -exec grep -i --files-with-matches --regexp="$stopship_keyword" {} \;)
    if [[ -n $matches ]]; then
      echo "${RED}'$stopship_keyword'${OFF} found in tracked files."
      echo "Please fix all related issues in the following files:"
      printf "\t %s\n" $matches
      exit 1
    fi
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

########################## migrations ##########################

# Wrapper for golang-migrate with predefined configuration.
x.migrate() {
  { { {
    migrate \
      -path $MIGRATIONS_DIR/ \
      -database "postgres://$POSTGRES_USER:$POSTGRES_PASSWORD@localhost:$DB_PORT/$POSTGRES_DB?sslmode=disable" \
      "$@" 2>&1 # migrate outputs everything to stderr

    if [[ "${*:1}" =~ up* ]]; then
      echo "Running post-migration scripts"
      for file in $(find db/post-migration -maxdepth 1 -name '*.sql' | sort); do
        dockerdb_psql -U postgres -d "$POSTGRES_DB" <$file
      done
    fi
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Create a new migration file with the given `name`.
# Args: name
x.migrate.create() {
  { { {
    tmp="$*"
    tmp="${tmp// /_}"
    name="${tmp,,}"
    [[ -z $name ]] && err "Please provide a migration name"
    x.migrate create -ext sql -dir $MIGRATIONS_DIR/ -seq -digits 7 "$name"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

########################## db ##########################

# Show active and max number of connections for the current environment.
x.db.conns() {
  { { {
    current_conns=$(dockerdb_psql -d $POSTGRES_DB -c "SELECT count(*) FROM pg_stat_activity WHERE datname = '$POSTGRES_DB';")
    max_conns=$(dockerdb_psql -d $POSTGRES_DB -c "SHOW max_connections;")
    echo "$current_conns/$max_conns active connections in '$POSTGRES_DB'"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Create a new database in the current environment if it doesn't exist
# and stops its running processes if any.
x.db.recreate() {
  { { {
    create_db_if_not_exists $POSTGRES_DB
    stop_db_processes $POSTGRES_DB
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Drop and recreate the database in the current environment.
x.db.drop() {
  [[ $env = "prod" ]] && confirm "This will drop all database data. Continue?"
  { { {
    drop_and_recreate_db "$POSTGRES_DB"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Drop and recreate the database used for code generation up to N, N-1 revisions or none.
# Useful to get correct analyses on migration scripts.
# Args: {up|up-1|drop}
x.db.gen() {
  { { {
    latest_rev=$(find $MIGRATIONS_DIR/*.sql -maxdepth 0 | wc -l)
    second_latest_rev=$((latest_rev - 2)) # up+down
    POSTGRES_DB=$GEN_POSTGRES_DB x.db.drop
    # TODO should be able to autocomplete x function nested cases
    # if it's called as $CMD
    case $1 in
    up)
      POSTGRES_DB=$GEN_POSTGRES_DB x.migrate up
      ;;
    up-1)
      POSTGRES_DB=$GEN_POSTGRES_DB x.migrate up $second_latest_rev
      ;;
    drop) ;;
    *)
      err "Valid options {up|up-1|drop}, got: $1"
      ;;
    esac
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Seed gen database.
x.db.gen.initial-data() {
  { { {
    POSTGRES_DB=$GEN_POSTGRES_DB x.db.initial-data
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Seed gen database.
x.db.initial-data() {
  # xlog eats up read -p (prompt)
  { { {
    x.db.drop
    x.migrate up
    echo "Loading initial data to $POSTGRES_DB"
    dockerdb_psql -d $POSTGRES_DB <"./db/initial_data_$env.pgsql"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Backup the database for the current environment.
x.db.dump() {
  { { {
    running_dumps=$(dockerdb_psql -P pager=off -U postgres -d "postgres_$env" \
      -c "SELECT pid FROM pg_stat_activity WHERE application_name = 'pg_dump';")
    if [[ "$running_dumps" != "" ]]; then
      err "pg_dump is already running, aborting new dump"
    fi

    mkdir -p "$DUMPS_FOLDER"
    schema_v=$(dockerdb_psql -P pager=off -U postgres -d "postgres_$env" \
      -c "SELECT version FROM schema_migrations;")
    dump_file="${dump_prefix}$(date +%Y-%m-%dT%H-%M-%S)_version${schema_v}.gz"

    echo "Dumping database to $dump_file"
    dockerdb pg_dump -U postgres -d "postgres_$env" |
      gzip >"$DUMPS_FOLDER/$dump_file"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Restore the database with the latest dump or `file` for the current environment.
# Args: [file]
x.db.restore() {
  # xlog eats up read -p (prompt)
  dump_file="$1"
  if [[ -n $dump_file ]]; then
    [[ ! -f $dump_file ]] && err "$dump_file does not exist"
    [[ "$dump_file" != *"$dump_prefix"* ]] && confirm "${RED}Dump doesn't match prefix '$dump_prefix'. Continue?${OFF}"
  else
    mkdir -p "$DUMPS_FOLDER"
    latest_dump_file=$(find "$DUMPS_FOLDER"/ -name "$dump_prefix*.gz" | sort -r | head -n 1)
    if [[ -z "$latest_dump_file" ]]; then
      err "No $dump_prefix* file found in $DUMPS_FOLDER"
    fi
    dump_file="$latest_dump_file"
  fi

  confirm "Do you want to restore ${YELLOW}$dump_file${OFF} in the ${RED}$env${OFF} environment?"

  x.db.drop
  gunzip -c "$dump_file" | dockerdb_psql -U postgres -d "postgres_$env"
  # sanity check, but probably better to do it before restoring...
  dump_schema_v=$(dockerdb_psql -P pager=off -U postgres -d "postgres_$env" -c "SELECT version FROM schema_migrations;")
  file_schema_v=$(echo "$dump_file" | sed -E 's/.*_version([0-9]+)\..*/\1/')
  echo "Migration revision: $dump_schema_v"
  if [[ "$dump_schema_v" != "$file_schema_v" ]]; then
    err "Schema version mismatch: dump $dump_schema_v != file $file_schema_v"
  fi
}

########################## e2e ##########################

# Setup E2E Python environment.
x.e2e.sync-deps() {
  { { {
    cd e2e
    python -m venv .venv
    source .venv/bin/activate
    pip install pip-tools
    pip-compile requirements.in
    pip-compile requirements-dev.in
    pip-sync requirements-dev.txt requirements.txt
    cd - >/dev/null
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Run E2E tests. Accepts `pytest` parameters.
# Args: [...]
x.e2e.run() {
  { { {
    name="$PROJECT_PREFIX-e2e"
    cd e2e
    DOCKER_BUILDKIT=1 BUILDKIT_PROGRESS=plain \
      docker build -t "$name" .
    docker run -it --rm --ipc=host -v "$(pwd):/src" "$name" bash -c "pytest $*"
    cd - >/dev/null
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

########################## openapi ##########################

# Run a diff against the previous OpenAPI spec in the main branch.
# Can also be used to generate changelogs when upgrading major versions.
x.diff-oas() {
  { { {
    base_spec="/tmp/openapi.yaml"
    git show "main:$SPEC" >"$base_spec"

    tmp="$(yq .info.version "$base_spec")"
    base_v="${tmp%%.*}"
    tmp=$(yq .info.version "$SPEC")
    rev_v="${tmp%%.*}"
    ((rev_v != base_v)) &&
      echo "${YELLOW}Revision mismatch $rev_v and $base_v, skipping diff.${OFF}" && return

    args="-format text -breaking-only -fail-on-diff -exclude-description -exclude-examples"
    if oasdiff $args -base "$base_spec" -revision $SPEC; then
      echo "${GREEN}No breaking changes found in $SPEC${OFF}"
    else
      echo "${RED}Breaking changes found in $SPEC${OFF}"
      return 1
    fi
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

########################### helpers ###########################

# IMPORTANT: bug in declare -F returns line number of last nested function, if any.
# extracting function here instead...
run_hot_reload() {
  x.run.backend-hr &
  pids+=("$!")
  x.run.frontend &
  pids+=("$!")
}

run_shared_services() {
  cd docker
  DOCKER_BUILDKIT=1 BUILDKIT_PROGRESS=plain docker-compose \
    -p "$PROJECT_PREFIX" \
    -f docker-compose.shared.yml \
    -f docker-compose.oidc.yml \
    --env-file ../.env."$env" \
    "$@" 2>&1 # https://github.com/docker/compose/issues/7346
  cd - >/dev/null
}

# generate db structs for use with swaggest/openapi-go.
# no need for ast parsing since all code in db is generated and predictable
generate_db_structs_map() {
  readarray -t structs < <(find "$PG_REPO_GEN/db/" -maxdepth 1 -name "*.go" -exec sed -n 's/[\s]*type\(.*\)struct.*/\1/p' {} \;)
  for struct in ${structs[@]}; do
    map_fields+=("\"$struct\": db.$struct{},")
  done

  out="internal/postgen/db_structs.gen.go"
  cat <<EOF >$out
// Code generated by 'project postgen'. DO NOT EDIT.

package postgen

import (
  db "$GOMOD_PKG/$PG_REPO_GEN/db"
  )
    var DbStructs = map[string]any{
$(printf "%s\n" "${map_fields[@]}")
  }
EOF

  gofumpt -w $out
}

update_spec_with_db_structs() {
  vext="x-db-struct"
  struct_names=$(yq e ".components.schemas[] | select(has(\"$vext\")).$vext" $SPEC)
  struct_names=$(join_by "," ${struct_names[*]})
  ((${#struct_names} == 0)) && return

  go build -o gen-schema cmd/gen-schema/main.go
  ./gen-schema --struct-names $struct_names | yq '
    with_entries(select(.key == "components")) |
    (.components.schemas[] | key) line_comment="Generated from db package structs. DO NOT EDIT."' \
    >/tmp/openapi.yaml

  for schema in $(yq '.components.schemas[] | key' /tmp/openapi.yaml); do
    yq eval-all "(
        select(fi==1).components.schemas.$schema
        ) as \$newSchema
        | select(fi == 0)
        | .components.schemas.$schema = \$newSchema
      " "$SPEC" /tmp/openapi.yaml >/tmp/final-spec

    mv /tmp/final-spec "$SPEC"
  done

  sed -i 's/!!merge //' $SPEC
}

xo_schema() {
  POSTGRES_DB=$GEN_POSTGRES_DB xo schema "postgres://$POSTGRES_USER:$POSTGRES_PASSWORD@localhost:$DB_PORT/$GEN_POSTGRES_DB?sslmode=disable" \
    --src "$PG_REPO"/xo-templates \
    "$@"
}

usage() {
  command_comments_parser() {
    head -$((${lns[$i]} - 1)) $0 |
      tac |
      sed -n '/#/!q;p' |
      tac |
      awk '{$1=$1;print}'
  }

  command_options_comments_parser() {
    tail -n +$((${lns[$i]} + 1)) $0 |
      sed -n '/^[[:blank:]]*#/!q;p' |
      awk '{$1=$1;print}'
  }

  construct_column() {
    comment_parser="$1"
    for i in ${!lns[@]}; do
      comment_paragraph="$($comment_parser)"
      ROWS["${rows[$i]}"]="$comment_paragraph"
      mapfile -t comments <<<"${ROWS[${rows[$i]}]}"
      for comment in "${comments[@]}"; do
        comment="$(clean_comment "$comment")"
        args="-"
        if [[ ${comment,,} == args:* ]]; then
          args=$(clean_args "$comment")
        fi
        ROW_ARGS[${rows[$i]}]="$args"
      done
    done

    for i in "${!rows[@]}"; do
      mapfile -t comments <<<"${ROWS[${rows[$i]}]}"
      for j in "${!comments[@]}"; do
        comment="$(clean_comment "${comments[$j]}")"
        if [[ ${comment,,} == args:* ]]; then
          continue
        fi

        if [[ $j = 0 ]]; then
          docs+=("$(
            printf -- "%s\t%s\t%s" \
              "${rows[$i]}" \
              "${ROW_ARGS[${rows[$i]}]}" \
              "$comment"
          )")
          continue
        fi

        docs+=("$(
          printf -- "%s\t%s\t%s" \
            "" \
            "" \
            "$comment"
        )")
      done
    done

    column -t \
      --separator $'\t' \
      --output-width 150 \
      --table-noextreme C2 \
      --table-noheadings \
      --table-wrap C3 \
      --table-columns C1,C2,C3 < <(printf "    %s\n" "${docs[@]}")
  }

  declare -A ROWS ROW_ARGS
  declare docs rows X_OPTIONS

  for c in "${COMMANDS[@]}"; do
    shopt -s extdebug
    lns+=("$(declare -F x.$c | awk '{print $2}')")
    rows+=("${c}")
    shopt -u extdebug
  done

  x_functions="$(construct_column command_comments_parser)"

  lns=()
  rows=()
  docs=()

  parse_x_options X_OPTIONS

  for c in "${X_OPTIONS[@]}"; do
    lns+=("${c##*)}")
    rows+=("${c%%)*}")
  done

  x_options="$(construct_column command_options_comments_parser)"

  cat <<EOF

$BOLD$UNDERSCORE$(basename $0)$OFF centralizes all relevant project commands.

${BOLD}USAGE:
    $RED$(basename $0) x.function [--x-option ...] args [optional args]$OFF

${BOLD}x.functions:$OFF
$(echo "${x_functions}" |
    sed -E 's/    ([[:alnum:][:punct:]]*)(.*)/    '"$BLUE$BOLD"'\1'"$OFF"'\2''/')

${BOLD}--x-options:$OFF
$(echo "${x_options}" |
      sed -E 's/    ([[:alnum:][:punct:]]*)(.*)/    '"$GREEN$BOLD"'\1'"$OFF"'\2''/')
EOF

}

# gets all --x-options values
parse_x_options() {
  declare -n list="$1" # pass ref by name
  while IFS= read -r line; do
    list+=("$(awk '{$1=$1;print $1 $NF}' <<<"$line")")
  done < <(sed -nr '/.*(--x-[=*[:alnum:]_-]+[)]+).*/{p;=}' $0 | sed '{N;s/\n/ /}')
  mapfile -t list < \
    <(LC_COLLATE=C sort < <(printf "%s\n" "${list[@]}"))
}

clean_comment() {
  tmp="$1"
  tmp="${tmp//\#/}"
  comment="${tmp#* }"
  [[ -z $comment ]] && comment="Â·"
  # TODO split in % MAX_COMMENT_LEN lines instead, on spaces only.
  ((${#comment} > MAX_COMMENT_LEN)) && comment="${comment:0:MAX_COMMENT_LEN}..."
  echo "$comment"
}

clean_args() {
  tmp="$1"
  tmp="${tmp,,##*args\:}"
  args="${tmp#* }"
  echo "$args"
}

# --------------------- completion and delegation --------------------
#      `complete -C foo foo` > `source <(foo bloated_completion)`

while IFS= read -r line; do
  [[ $line =~ ^declare\ -f\ x\. ]] || continue
  COMMANDS+=("${line##declare -f x.}")
done < <(declare -F)
# sort the array. Mimic file input to sort
mapfile -t COMMANDS < \
  <(LC_COLLATE=C sort < <(printf "%s\n" "${COMMANDS[@]}"))

MAX_XFN_LEN=0 # for logging purposes
for c in "${COMMANDS[@]}"; do
  len=${#c}
  ((len > MAX_XFN_LEN)) && MAX_XFN_LEN=$((len - 1)) # remove "x." but account for extra last space appended.
done

if [[ -n $COMP_LINE ]]; then
  pre="${COMP_LINE##* }" # the part after the last space in the current command
  cur_commands=(${COMP_LINE%"$pre"})

  for c in "${COMMANDS[@]}"; do
    if [[ " ${cur_commands[*]} " =~ " ${c} " ]]; then
      xfn_specified=true
      break
    fi
  done

  for c in "${COMMANDS[@]}"; do
    test -z "${xfn_specified}" || break
    test -z "${pre}" -o "${c}" != "${c#"${pre}"}" -a "${pre}" != "${c}" && echo "${c}"
  done

  test -z "${xfn_specified}" && exit

  declare __x_options x_options_lines

  parse_x_options x_options_lines

  for c in "${x_options_lines[@]}"; do
    tmp="${c%%)*}"
    xopt="${tmp//\*/}"
    __x_options+=("$xopt")
  done

  declare -A __x_opts_seen
  for cmd in "${cur_commands[@]}"; do
    for opt in ${__x_options[@]}; do
      if [[ "$cmd" == *"$opt"* ]]; then
        __x_opts_seen[$opt]=true
        break
      fi
    done
  done

  for opt in ${__x_options[@]}; do
    [[ -n "${__x_opts_seen[$opt]}" ]] && continue
    # TODO prevent opts that accept values having whitespace added
    [[ ${opt:0:${#pre}} == "${pre,,}" ]] && echo "${opt}"
  done

  exit
fi

declare CMD="$1"

# First comment lines automatically added to usage docs.
set +e
while [[ "$#" -gt 0 ]]; do
  case $1 in
  --x-help)
    # Show help for a particular x function.
    COMMANDS=("$CMD")
    usage
    exit
    ;;
  --x-force-regen) # this comment should be ignored
    # Removes generation cache, forcing a new run.
    force_regen=1
    ;;
  --x-no-confirmation)
    # Bypasses confirmation messages.
    export NO_CONFIRMATION=1
    ;;
  --x-no-gen)
    # Skips code generation steps.
    export NO_GEN=1
    ;;
  --x-env=*)
    # Environment to run commands in. Defaults to "dev".
    # Args: env
    env="${1#--x-env=}"
    valid_envs="dev staging prod ci"
    if [[ ! " ${valid_envs[*]} " =~ " $env " ]]; then
      err "Valid environments: $valid_envs"
    fi
    ;;
  *)
    # will set everything else back
    args+=("$1")
    ;;
  esac
  shift
done
set -e
for arg in ${args[@]}; do
  set -- "$@" "$arg"
done
# applicable to any command
ensure_envvars_set ".env.template" ".env.${env}"
source ".env.$env"

# handle executing x functions
if [[ -n "$1" ]]; then
  shift
  for c in "${COMMANDS[@]}"; do
    declare cmd=$(command -v "x.$c")
    if [[ $c == "$CMD" && -n "$cmd" ]]; then
      "x.$CMD" "$@"
      exit $?
    fi
  done
fi

# default to show usage if its a noop
usage
