#!/bin/bash
# shellcheck disable=1091,2155,2068,2086,2031

export PROC=$$

source "${BASH_SOURCE%/*}/.helpers.sh"
source "${BASH_SOURCE%/*}/scripts/deps-check.sh"

# FIXME m flag will prevent background process killing with ctrl-c
set -Eeo pipefail

trap killgroup SIGINT

trap errtrap SIGUSR1

killgroup() {
  printf "\nkilling spawned processes...\n"

  kill $(jobs -p) 2>/dev/null # doesn't really kill all children, just process group leaders

  kill 0
  exit 1
}

errtrap() {
  printf "\nExiting due to propagated error...\n"
  exit 1
}

ensure_pwd_is_top_level

source ".envrc"

readonly SPEC="openapi.yaml"
readonly PROTO_DIR="internal/pb"
readonly MIGRATIONS_DIR="db/migrations"
readonly PG_REPO="internal/repos/postgresql"
readonly PG_REPO_GEN="internal/repos/postgresql/gen"
readonly MAX_COMMENT_LEN=88
readonly MAX_FNAME_LOG_LEN=13
readonly DUMPS_FOLDER="$HOME/openapi_go_gin_postgres_dumps"
readonly GEN_POSTGRES_DB="gen_db"
pkg=$(head -1 go.mod)
readonly GOMOD_PKG="${pkg#module*}"

pids=()
env="dev"
dump_prefix="dump_${env}_"

# log for any function output.
xlog() {
  fname="${FUNCNAME[1]#*.}"
  [[ "$CMD" = "$fname" ]] && cat && return
  if [[ ${#fname} -gt $MAX_FNAME_LOG_LEN ]]; then
    fname="${fname:0:$MAX_FNAME_LOG_LEN}…"
  fi

  local fn=$(printf "%*s |\n" $((MAX_FNAME_LOG_LEN + 1)) "$fname")
  sed -ue "s/^/${BLUE}$fname >${OFF} /"
}

# log stderr for any function output.
# sed is buffering by default (without -u) so streams dont preserve order
# > >(one) 2> >(two) are background processes so it will break our parallel code.
xerr() {
  fname="${FUNCNAME[1]#*.}"
  [[ "$CMD" = "$fname" ]] && cat && return
  if [[ ${#fname} -gt $MAX_FNAME_LOG_LEN ]]; then
    fname="${fname:0:$MAX_FNAME_LOG_LEN}…"
  fi

  local fn=$(printf "%*s |\n" $((MAX_FNAME_LOG_LEN + 1)) "$fname")
  sed -ue "s/^/${RED}$fname >${OFF} /" >&2
}

# Check build dependencies are met.
x.check-build-deps() {
  { { {
    local -i fails
    check.column || { ((fails++)) && true; }
    check.protoc || { ((fails++)) && true; }
    check.bash || { ((fails++)) && true; }
    check.go || { ((fails++)) && true; }
    check.curl || { ((fails++)) && true; }
    check.docker || { ((fails++)) && true; }
    check.docker-compose || { ((fails++)) && true; }
    check.direnv || { ((fails++)) && true; }
    check.yq || { ((fails++)) && true; }
    check.pg_format || { ((fails++)) && true; }
    ((fails == 0)) && echo "${GREEN}🎉 All build dependencies met.${OFF}"
    { ((fails != 0)) && err "${RED}❌ Missing dependencies.${OFF}"; } || true
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Check dependencies and fetch required tools.
x.bootstrap() {
  { { {
    x.check-build-deps
    x.install-tools
    x.fetch.swagger-ui
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Install go libraries as runnable programs.
x.install-tools() {
  { { {
    set -o errexit -eo pipefail

    go install -tags 'postgres' github.com/golang-migrate/migrate/v4/cmd/migrate@v4.15.2
    # go install github.com/kyleconroy/sqlc/cmd/sqlc@v1.15.0
    # TODO until merged
    wget https://github.com/danicc097/sqlc/releases/download/v1.2-custom/sqlc -O $GOBIN/sqlc &&
      chmod +x $GOBIN/sqlc
    go install github.com/golangci/golangci-lint/cmd/golangci-lint@v1.47.2
    go install github.com/joho/godotenv/cmd/godotenv@latest
    go install github.com/tufin/oasdiff@latest
    go install golang.org/x/tools/cmd/goimports@latest
    go install mvdan.cc/gofumpt@latest
    go install github.com/danicc097/air@latest
    go install github.com/danicc097/xo@latest
    go install github.com/mikefarah/yq/v4@v4.27.2
    go install github.com/hexdigest/gowrap/cmd/gowrap@latest

    go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.28.1
    go install github.com/planetscale/vtprotobuf/cmd/protoc-gen-go-vtproto@v0.2.0
    go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.2

    GO111MODULE=off go get -u github.com/maxbrunsfeld/counterfeiter
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Fetch latest Swagger UI bundle.
x.fetch.swagger-ui() {
  { { {
    name="$(curl --silent "https://api.github.com/repos/swagger-api/swagger-ui/releases/latest" | jq -r ".. .tag_name? // empty")"
    curl -fsSL "github.com/swagger-api/swagger-ui/archive/refs/tags/$name.tar.gz" -o swagger-ui.tar.gz
    tar xf swagger-ui.tar.gz swagger-ui-"${name#*v}"/dist --one-top-level=swagger-ui --strip-components=2
    rm swagger-ui.tar.gz
    mkdir -p internal/static/swagger-ui
    mv swagger-ui/* internal/static/swagger-ui/
    rm -r swagger-ui
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Run pre-generation scripts in the internal package.
x.pregen() {
  { { {
    echo "Running generation"

    x.db.drop
    x.migrate up

    ######## Sync enums from db with spec
    # example: `x-db-enum: user_role` will use the db's "user_role" enum
    vext="x-db-enum"
    for enum in $(yq e ".. | select(has(\"$vext\")).$vext" $SPEC); do
      local db_enums=()
      path="$(enum_name=$enum yq e ".. | select(has(\"$vext\") and .$vext == env(enum_name)) | (path | join(\".\"))" $SPEC)"
      mapfile -t db_enums < <(dockerdb_psql -d $POSTGRES_DB -c "SELECT unnest(enum_range(NULL::\"$enum\"));" 2>/dev/null)

      [[ ${#db_enums[@]} -gt 0 ]] || err "$vext: Enum '$enum' not found in database $POSTGRES_DB"
      enums=$(printf ",\"%s\"" "${db_enums[@]}")
      enums="[${enums:1}]"
      echo "Replacing enum '$enum' in $SPEC with: $enums"
      enums=$enums yq e ".$path.enum = env(enums)" -i $SPEC
    done

    ######## Ensure consistent style for future codegen
    echo "Applying PascalCase to operation IDs in $SPEC"
    for opid in $(yq e ".. | select(has(\"operationId\")).operationId" $SPEC); do
      new_opid="$(to_pascal $opid)"
      paths=$(opid=$opid yq e ".. | select(has(\"operationId\") and .operationId == env(opid)) | (path)" $SPEC)
      readarray -t paths <<<$paths
      escaped_path="$(join_yq_paths paths)"
      new_opid=$new_opid yq e ".$escaped_path.operationId = env(new_opid)" -i $SPEC
    done

    ######## Sync enums with external sources and validate
    # frontend scopes: 1) replace all "-_" by " " 2) Sort 3) range over and split(":") -> map[left] = append(right) and show aggregated
    # arrays can't be nested
    declare -A enum_src_files=(
      [Scope]="scopes.json"
      [Role]="roles.json"
    )
    declare -A enum_vext=(
      [Scope]="x-required-scopes"
      [Role]="x-required-role"
    )
    for enum in ${!enum_src_files[@]}; do
      [[ $(yq e ".components.schemas | has(\"$enum\")" $SPEC) = "false" ]] &&
        yq e ".components.schemas.$enum.type = \"string\"" -i $SPEC

      src_file="${enum_src_files[$enum]}"
      vext="${enum_vext[$enum]}"

      enums=$(yq -P '.[] | key' $src_file)
      readarray -t enums <<<$enums

      echo "Replacing '$enum' enum in $SPEC with $src_file"
      yq e ".components.schemas.$enum.description = \"$enum automatically generated from $src_file keys\"" -i $SPEC
      enums_arr=$(printf ",\"%s\"" "${enums[@]}")
      enums_arr="[${enums_arr:1}]"
      enums_arr=$enums_arr yq e ".components.schemas.$enum.enum = env(enums_arr)" -i $SPEC

      readarray spec_enums < <(yq e ".paths[][].$vext | select(length > 0)" openapi.yaml)
      spec_enums=("${spec_enums[*]//- /}")
      mapfile -t spec_enums < <(printf "\"%s\"\n" ${spec_enums[*]})
      mapfile -t clean_enums < <(printf "\"%s\"\n" ${enums[*]})
      for spec_enum in "${spec_enums[@]}"; do
        [[ ! " ${clean_enums[*]} " =~ " ${spec_enum} " ]] && err "$spec_enum is not a valid '$enum'"
      done
    done

    ######## Generate shared policies once the spec has been validated
    # https://mikefarah.gitbook.io/yq/operators/reduce
    opid_auth="operationAuthorization.gen.json"
    echo "Writing shared auth policies"
    echo "// Code generated by '$(basename $0) ${FUNCNAME[0]##*.}'. DO NOT EDIT." >$opid_auth
    yq -o=json e "
    .paths[][]
    | {.operationId: { \"scopes\": .x-required-scopes, \"role\": .x-required-role}}
    | select(.[]) as \$i ireduce ({}; . + \$i)
  " openapi.yaml >>$opid_auth

    yq --prettyPrint '.. style="single"' -i $SPEC
    # https://mikefarah.gitbook.io/yq/operators/anchor-and-alias-operators
    # but some tooling is not aware of this and raises errors, e.g. openapi-typescript-codegen, vscode yaml
    sed -i 's/!!merge //' $SPEC

    # go build -o pregen "$PWD"/cmd/pregen/main.go
    # ./pregen -env=".env.$env"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# outputs safe double-quoted paths for yq
# fixes paths where a dot is present
join_yq_paths() {
  declare -n arr="$1"
  arr=("${arr[*]//- /}")
  mapfile -t arr < <(printf "\"%s\"\n" ${arr[*]})
  join_by "." ${arr[*]}
}

clean_yq_array() {
  declare -n arr="$1"
  arr=("${arr[*]//- /}")
  mapfile -t arr < <(printf "\"%s\"\n" ${arr[*]})
  echo ${arr[@]}
}

# Run post-generation scripts in the internal package.
x.postgen() {
  { { {
    echo "Running generation"
    cache="$1"
    mkdir -p "$cache"
    go build -o postgen "$PWD"/cmd/postgen/main.go
    ./postgen -cachedir="$cache" -env=".env.$env"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Generate type-safe Go code from SQL.
x.gen.sqlc() {
  { { {
    echo "Running generation"
    sqlc generate -f "$PG_REPO"/sqlc.yaml || err "Failed sqlc generation"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Generate interface wrappers with common logic: tracing, timeout...
x.gen.gowrap() {
  { { {
    echo "Running generation"
    repo_interfaces="User"
    for iface in ${repo_interfaces[@]}; do
      gowrap gen \
        -g \
        -p $GOMOD_PKG/internal/repos \
        -i $iface \
        -t internal/gowrap-templates/otel-with-timeout.tmpl \
        -o internal/repos/${iface,,}_with_tracing_and_timeout.gen.go
    done
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Generate Go client and server.
x.gen.client-server() {
  { { {
    go build -o oapi-codegen "$PWD"/cmd/oapi-codegen

    echo "Running generation"
    ./oapi-codegen --config internal/client/oapi-codegen-client.yaml "$SPEC" || err "Failed client generation"
    ./oapi-codegen --config internal/client/oapi-codegen-types.yaml "$SPEC" || err "Failed client generation"

    ./oapi-codegen --config internal/rest/oapi-codegen-server.yaml "$SPEC" || err "Failed server generation"

    ./oapi-codegen --config internal/models/oapi-codegen-types.yaml "$SPEC" || err "Failed server generation"

  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Generate mocks for specified interfaces. Runs are cached to `cachedir`.
# Args: cachedir
x.gen.counterfeiter() {
  # This shouldn't pose any problems, the interface is the only input to counterfeiter.
  { { {
    cache="$1"

    envvar="internal/envvar/envvar.go"
    repos="internal/repos/repos.go"
    tfidfpb="internal/pb/python-ml-app-protos/tfidf/v1/service_grpc.pb.go"

    echo "Running generation"

    if ! md5sum -c "$cache/counterfeiter.md5" >/dev/null || [[ $force_regen -eq 1 ]]; then
      echo "Recreating mocks"
      {
        counterfeiter -o internal/envvar/envvartesting/provider.gen.go $envvar Provider &
        counterfeiter -o internal/repos/repostesting/user.gen.go $repos User &
        counterfeiter -o internal/pb/python-ml-app-protos/tfidf/v1/v1testing/movie_genre_client.gen.go $tfidfpb MovieGenreClient &
        counterfeiter -o internal/pb/python-ml-app-protos/tfidf/v1/v1testing/movie_genre_server.gen.go $tfidfpb MovieGenreServer &
        md5sum $envvar $repos $tfidfpb >"$cache/counterfeiter.md5" &
        wait
      } 2>&1 # outputs to stderr for some reason

      # counterfeiter is unaware of grpc's obscure mustEmbedUnimplemented***() for forward server compatibility
      sed -i '/type FakeMovieGenreServer struct {/a v1\.UnimplementedMovieGenreServer' \
        internal/pb/python-ml-app-protos/tfidf/v1/v1testing/movie_genre_server.gen.go
    fi
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Generate the required servers/clients for relevant services.
x.gen.proto() {
  { { {
    local import_path="python-ml-app-protos/tfidf/v1"
    local filename="internal/python-ml-app-protos/tfidf/v1/service.proto"

    mkdir -p internal/pb
    # Plugins are no longer supported by protoc-gen-go.
    # Instead protoc-gen-go-grpc and the go package (in proto or via M flag) are required
    echo "Running generation"
    protoc \
      --go-grpc_out=internal/pb/. \
      --go_out=internal/pb/. \
      --go-grpc_opt=M$filename=$import_path,paths=import \
      --go_opt=M$filename=$import_path,paths=import \
      internal/python-ml-app-protos/tfidf/v1/service.proto || err "Failed proto generation"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Run all codegen and postgen commands for the project.
x.gen() {
  { { {
    echo "Running code generation"
    local cache=".generate.cache"

    mkdir -p "$cache"

    x.db.recreate

    POSTGRES_DB=$GEN_POSTGRES_DB x.pregen

    rm -rf "$PG_REPO_GEN"

    # TODO try use gnu parallel and exit when anyone fails
    # (with current setup the whole x.gen is executed and only then
    # the trap on SIGUSR1 coming from `err` is run - now temporarily using kill 0 in `err` instead on sending SIGUSR1)

    go generate ./... &
    x.gen.gowrap &
    x.gen.sqlc &
    # xo cannot use db files as input, needs an up-to-date schema
    # not recreating db on every gen can lead to plain wrong generation based on an old dev schema
    # use a unique db to prevent cosmic accidents
    x.gen.xo &
    x.gen.client-server &
    x.gen.proto &
    wait

    x.postgen "$cache" &
    x.gen.counterfeiter "$cache" & # delay since it depends on generated output too

    wait

    x.lint

  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Lint the entire project.
x.lint() {
  { { {
    x.lint.sql &
    x.lint.go &
    wait
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Format Go files.
x.lint.go() {
  { { {
    files=$(find . \
      -not -path "**/$PROTO_DIR/*" \
      -not -path "**/"$PG_REPO_GEN"/*" \
      -not -path "**/testdata/*" \
      -not -path "**/*.cache/*" \
      -not -path "**/*.gen.*" \
      -name "*.go")

    goimports -w $files
    gofumpt -w $files
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Format SQL files.
x.lint.sql() {
  { { {
    SQL_DIRS=(
      "$PG_REPO/queries"
      "db"
    )
    for slq_dir in ${SQL_DIRS[@]}; do
      pg_format \
        --spaces 2 \
        --wrap-limit 130 \
        --function-case 2 \
        --keyword-case 1 \
        --placeholder "sqlc\\.(arg|narg)\\(:?[^)]*\\)" \
        --inplace \
        --nogrouping \
        --keep-newline \
        --comma-start \
        $(find "$slq_dir" -maxdepth 3 -name '*.*sql')
    done
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Run required backend pre-test setup: services, database cleanup, codegen...
# Can be called independently, e.g. before running tests through an IDE.
x.test.backend-setup() {
  { { {
    # NOTE: tests run independently in Go so we can't have a function be called and run
    # only once before any test starts
    run_shared_services up -d --remove-orphans
    x.gen
    drop_and_recreate_db "postgres_test"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Test the entire project. Accepts `go test` parameters.
# Args: [...]
x.test() {
  { { {
    x.test.backend-setup
    APP_ENV="$env" go test "$@" ./...
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Test and build the entire project.
x.build() {
  { { {
    x.lint
    x.test ""
    go build -o rest-server "$PWD"/cmd/rest-server
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Run backend with hot-reloading.
x.run.backend-hr() {
  run_shared_services up -d --remove-orphans
  # TODO new include_files flag in fork, e.g. for openapi.yaml.
  # else generated .yaml files trigger rebuild.
  # --build.include_files "openapi.yaml" \
  # NOTE: building binary very unreliable, leads to bin not found.
  air \
    --build.pre_build_cmd "bin/project gen" \
    --build.cmd "" \
    --build.bin "go run ./cmd/rest-server/ -env=.env.$env" \
    --build.include_ext "go" \
    --build.exclude_regex ".gen.go,_test.go" \
    --build.exclude_dir ".git,tmp,$PROTO_DIR,"$PG_REPO_GEN",**/testdata,frontend,*.cache" \
    --build.stop_watch "internal/rest/,internal/services/" \
    --build.delay 1000 \
    --build.exclude_unchanged "true" |
    sed -e "s/^/${BLUE}[Air]${OFF} /"
}

# Run frontend with hot-reloading.
x.run.frontend() {
  set -a
  # export to replace config with envvars
  source ".env.$env"
  set +a

  cd frontend
  pnpm run generate

  pnpm run dev |
    sed -e "s/^/${GREEN}[Vite]${OFF} /"
}

# Run all project services with hot reload enabled in dev mode.
x.run-dev() {
  env="dev"

  run_hot_reload

  next_allowed_run=$(date +%s)
  latency=3
  # close_write event, else duplicated, tripl. events -> race condition
  while true; do
    inotifywait \
      --monitor "$SPEC" \
      --event=close_write \
      --format='%T %f' \
      --timefmt='%s' |
      while read -r event_time event_file 2>/dev/null || sleep $latency; do
        if [[ $event_time -ge $next_allowed_run ]]; then
          next_allowed_run=$(date --date="${latency}sec" +%s)

          for pid in "${pids[@]}"; do
            # air and vite spawn processes as well, need to kill those (whose parent is pid), kill $pid will not kill children. pkill -P would also work
            echo "$(list_descendants $pid)"
            kill "$(list_descendants $pid)"
          done
          pids=()

          run_hot_reload
        fi
      done
  done
}

# Run project in production mode.
x.run-prod() {
  env="prod"

  docker network create traefik-net 2>/dev/null || true
  run_shared_services up -d --remove-orphans
  x.db.recreate

  cd frontend && pnpm run generate && cd - >/dev/null
  x.gen

  DOCKER_BUILDKIT=1 BUILDKIT_PROGRESS=plain docker-compose \
    --project-name "$PROJECT_PREFIX"_"$env" \
    -f docker-compose."$env".yml \
    --env-file ".env.$env" \
    up -d --build 2>&1 # https://github.com/docker/compose/issues/7346

  # TODO handle rollback and restore up to X revision (from restored db)
  # see dump_schema_v
  echo "Migrations:"
  x.migrate up
}

# Remove running project containers, including shared ones between environments.
x.stop-project() {
  { { {
    run_shared_services down --remove-orphans
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Checks before release:
# - Magic keyword "STOPSHIP" not found in tracked files.
x.release() {
  { { {
    x.search-stopship "STOPSHIP" &
    go mod verify &

    wait
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Block build if magic keyword is found in any file
# Args: keyword
x.search-stopship() {
  { { {
    stopship_keyword="$1"
    local matches
    matches=$(find "$(git rev-parse --show-toplevel)" \
      -type f \
      -not -path '**/.git/*' \
      -not -path '**/.venv/*' \
      -not -path '**/node_modules/*' \
      -not -path '**/build/*' \
      -not -path '**/*.pyc' \
      -not -path "$0" \
      -not -exec git check-ignore -q --no-index {} \; \
      -exec grep -i --files-with-matches --regexp="$stopship_keyword" {} \;)
    if [[ -n $matches ]]; then
      echo "${RED}'$stopship_keyword'${OFF} found in tracked files."
      echo "Please fix all related issues in the following files:"
      printf "\t %s\n" $matches
      exit 1
    fi
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

########################## migrations ##########################

# Wrapper for golang-migrate with predefined configuration.
x.migrate() {
  { { {
    migrate \
      -path $MIGRATIONS_DIR/ \
      -database "postgres://$POSTGRES_USER:$POSTGRES_PASSWORD@localhost:$DB_PORT/$POSTGRES_DB?sslmode=disable" \
      "$@" 2>&1 # migrate outputs everything to stderr

    if [[ "${*:1}" =~ up* ]]; then
      echo "Running post-migration scripts"
      for file in $(find db/post-migration -maxdepth 1 -name '*.sql' | sort); do
        dockerdb_psql -U postgres -d "$POSTGRES_DB" <$file
      done
    fi
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Create a new migration file with the given `name`.
# Args: name
x.migrate.create() {
  { { {
    tmp="$*"
    tmp="${tmp// /_}"
    name="${tmp,,}"
    [[ -z $name ]] && err "Please provide a migration name"
    x.migrate create -ext sql -dir $MIGRATIONS_DIR/ -seq -digits 7 "$name"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

########################## db ##########################

# Show active and max number of connections for the current environment.
x.db.conns() {
  { { {
    current_conns=$(dockerdb_psql -d $POSTGRES_DB -c "SELECT count(*) FROM pg_stat_activity WHERE datname = '$POSTGRES_DB';")
    max_conns=$(dockerdb_psql -d $POSTGRES_DB -c "SHOW max_connections;")
    echo "$current_conns/$max_conns active connections in '$POSTGRES_DB'"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Create a new database in the current environment if it doesn't exist
# and stops its running processes if any.
x.db.recreate() {
  { { {
    create_db_if_not_exists "postgres_$env"
    stop_db_processes "postgres_$env"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Drop and recreate the database in the current environment.
x.db.drop() {
  [[ $env = "prod" ]] && confirm "This will drop all database data. Continue?"
  { { {
    drop_and_recreate_db "$POSTGRES_DB"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Drop and recreate the database used for code generation up to N, N-1 revisions or none.
# Useful to get correct analyses on migration scripts.
# Args: {up|up-1|drop}
x.db.gen() {
  { { {
    latest_rev=$(find $MIGRATIONS_DIR/*.sql -maxdepth 0 | wc -l)
    second_latest_rev=$((latest_rev - 2)) # up+down
    POSTGRES_DB=$GEN_POSTGRES_DB x.db.drop
    # TODO should be able to autocomplete x function nested cases
    # if it's called as $CMD
    case $1 in
    up)
      POSTGRES_DB=$GEN_POSTGRES_DB x.migrate up
      ;;
    up-1)
      POSTGRES_DB=$GEN_POSTGRES_DB x.migrate up $second_latest_rev
      ;;
    drop) ;;
    *)
      err "Valid options {up|up-1|drop}, got: $1"
      ;;
    esac
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Seed gen database.
x.db.gen.initial-data() {
  { { {
    POSTGRES_DB=$GEN_POSTGRES_DB x.db.initial-data
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Seed gen database.
x.db.initial-data() {
  # xlog eats up read -p (prompt)
  { { {
    x.db.drop
    x.migrate up
    echo "Loading initial data to $POSTGRES_DB"
    dockerdb_psql -d $POSTGRES_DB <"./db/initial_data_$env.pgsql"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Backup the database for the current environment.
x.db.dump() {
  { { {
    running_dumps=$(dockerdb_psql -P pager=off -U postgres -d "postgres_$env" \
      -c "SELECT pid FROM pg_stat_activity WHERE application_name = 'pg_dump';")
    if [[ "$running_dumps" != "" ]]; then
      err "pg_dump is already running, aborting new dump"
    fi

    mkdir -p "$DUMPS_FOLDER"
    schema_v=$(dockerdb_psql -P pager=off -U postgres -d "postgres_$env" \
      -c "SELECT version FROM schema_migrations;")
    dump_file="${dump_prefix}$(date +%Y-%m-%dT%H-%M-%S)_version${schema_v}.gz"

    echo "Dumping database to $dump_file"
    dockerdb pg_dump -U postgres -d "postgres_$env" |
      gzip >"$DUMPS_FOLDER/$dump_file"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Restore the database with the latest dump or `file` for the current environment.
# Args: [file]
x.db.restore() {
  # xlog eats up read -p (prompt)
  dump_file="$1"
  if [[ -n $dump_file ]]; then
    [[ ! -f $dump_file ]] && err "$dump_file does not exist"
    [[ "$dump_file" != *"$dump_prefix"* ]] && confirm "${RED}Dump doesn't match prefix '$dump_prefix'. Continue?${OFF}"
  else
    mkdir -p "$DUMPS_FOLDER"
    latest_dump_file=$(find "$DUMPS_FOLDER"/ -name "$dump_prefix*.gz" | sort -r | head -n 1)
    if [[ -z "$latest_dump_file" ]]; then
      err "No $dump_prefix* file found in $DUMPS_FOLDER"
    fi
    dump_file="$latest_dump_file"
  fi

  confirm "Do you want to restore ${YELLOW}$dump_file${OFF} in the ${RED}$env${OFF} environment?"

  x.db.drop
  gunzip -c "$dump_file" | dockerdb_psql -U postgres -d "postgres_$env"
  # sanity check, but probably better to do it before restoring...
  dump_schema_v=$(dockerdb_psql -P pager=off -U postgres -d "postgres_$env" -c "SELECT version FROM schema_migrations;")
  file_schema_v=$(echo "$dump_file" | sed -E 's/.*_version([0-9]+)\..*/\1/')
  echo "Migration revision: $dump_schema_v"
  if [[ "$dump_schema_v" != "$file_schema_v" ]]; then
    err "Schema version mismatch: dump $dump_schema_v != file $file_schema_v"
  fi
}

########################## e2e ##########################

# Setup E2E Python environment.
x.e2e.sync-deps() {
  { { {
    cd e2e
    python -m venv .venv
    source .venv/bin/activate
    pip install pip-tools
    pip-compile requirements.in
    pip-compile requirements-dev.in
    pip-sync requirements-dev.txt requirements.txt
    cd - >/dev/null
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Run E2E tests. Accepts `pytest` parameters.
# Args: [...]
x.e2e.run() {
  { { {
    name="$PROJECT_PREFIX-e2e"
    cd e2e
    DOCKER_BUILDKIT=1 BUILDKIT_PROGRESS=plain \
      docker build -t "$name" .
    docker run -it --rm --ipc=host -v "$(pwd):/src" "$name" bash -c "pytest $*"
    cd - >/dev/null
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

########################## openapi ##########################

# Run a diff against the previous OpenAPI spec in the main branch.
# Can also be used to generate changelogs when upgrading major versions.
x.diff-oas() {
  { { {
    base_spec="/tmp/openapi.yaml"
    git show "main:$SPEC" >"$base_spec"

    tmp="$(yq .info.version "$base_spec")"
    base_v="${tmp%%.*}"
    tmp=$(yq .info.version "$SPEC")
    rev_v="${tmp%%.*}"
    ((rev_v != base_v)) &&
      echo "${YELLOW}Revision mismatch $rev_v and $base_v, skipping diff.${OFF}" && return

    args="-format text -breaking-only -fail-on-diff -exclude-description -exclude-examples"
    if oasdiff $args -base "$base_spec" -revision $SPEC; then
      echo "${GREEN}No breaking changes found in $SPEC${OFF}"
    else
      echo "${RED}Breaking changes found in $SPEC${OFF}"
      return 1
    fi
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

########################### helpers ###########################

# IMPORTANT: bug in declare -F returns line number of last nested function, if any.
# extracting function here instead...
run_hot_reload() {
  x.run.backend-hr &
  pids+=("$!")
  x.run.frontend &
  pids+=("$!")
}

run_shared_services() {
  cd docker
  docker-compose \
    -p "$PROJECT_PREFIX" \
    -f docker-compose.shared.yml \
    --env-file ../.env."$env" \
    "$@" 2>&1 # https://github.com/docker/compose/issues/7346
  cd - >/dev/null
}

# Automatically generate CRUD and By queries based on existing indexes from a Postgres schema.
x.gen.xo() {
  # TODO from hn
  # I've used https://github.com/xo/xo, extended it with some custom functions for templating, extended the templates themselves, and can now generate CRUD for anything in the database, functions for common select queries based on the indices that exist in the database, field filtering and scanning, updates for subsets of fields including some atomic operations, etc. The sky is the limit honestly. It has allowed me to start with something approximating a statically generated ORM and extend it with any features I want as time goes on. I also write .extra.go files along side the generated .xo.go files to extend the structs that are generated with custom logic and methods to convert data into response formats.
  # TODO joins? e.g. users join user_organizations (user_id)
  #
  { { {
    echo "Running generation"
    # xo schema --help

    mkdir -p "$PG_REPO_GEN"/db
    xo_schema -o "$PG_REPO_GEN"/db \
      --ignore "*.created_at" \
      --ignore "*.updated_at" || err "Failed xo public schema generation" &

    mkdir -p "$PG_REPO_GEN"/db/v
    xo_schema -o "$PG_REPO_GEN"/db/v \
      --schema v \
      --main-schema-pkg $GOMOD_PKG/"$PG_REPO_GEN"/db ||
      err "Failed xo v schema generation" &

    mkdir -p "$PG_REPO_GEN"/db/cache
    xo_schema -o "$PG_REPO_GEN"/db/cache \
      --schema cache \
      --main-schema-pkg $GOMOD_PKG/"$PG_REPO_GEN"/db ||
      err "Failed xo cache schema generation" &

    wait
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

xo_schema() {
  xo schema "postgres://$POSTGRES_USER:$POSTGRES_PASSWORD@localhost:$DB_PORT/$GEN_POSTGRES_DB?sslmode=disable" \
    --src "$PG_REPO"/xo-templates \
    "$@"
}

usage() {
  command_comments_parser() {
    head -$((${lns[$i]} - 1)) $0 |
      tac |
      sed -n '/#/!q;p' |
      tac |
      awk '{$1=$1;print}'
  }

  command_options_comments_parser() {
    tail -n +$((${lns[$i]} + 1)) $0 |
      sed -n '/^[[:blank:]]*#/!q;p' |
      awk '{$1=$1;print}'
  }

  construct_column() {
    comment_parser="$1"
    for i in ${!lns[@]}; do
      comment_paragraph="$($comment_parser)"
      ROWS["${rows[$i]}"]="$comment_paragraph"
      mapfile -t comments <<<"${ROWS[${rows[$i]}]}"
      for comment in "${comments[@]}"; do
        comment="$(clean_comment "$comment")"
        args="-"
        if [[ ${comment,,} == args:* ]]; then
          args=$(clean_args "$comment")
        fi
        ROW_ARGS[${rows[$i]}]="$args"
      done
    done

    for i in "${!rows[@]}"; do
      mapfile -t comments <<<"${ROWS[${rows[$i]}]}"
      for j in "${!comments[@]}"; do
        comment="$(clean_comment "${comments[$j]}")"
        if [[ ${comment,,} == args:* ]]; then
          continue
        fi

        if [[ $j = 0 ]]; then
          docs+=("$(
            printf -- "%s\t%s\t%s" \
              "${rows[$i]}" \
              "${ROW_ARGS[${rows[$i]}]}" \
              "$comment"
          )")
          continue
        fi

        docs+=("$(
          printf -- "%s\t%s\t%s" \
            "" \
            "" \
            "$comment"
        )")
      done
    done

    column -t \
      --separator $'\t' \
      --output-width 150 \
      --table-noextreme C2 \
      --table-noheadings \
      --table-wrap C3 \
      --table-columns C1,C2,C3 < <(printf "    %s\n" "${docs[@]}")
  }

  declare -A ROWS ROW_ARGS
  declare docs rows X_OPTIONS

  for c in "${COMMANDS[@]}"; do
    shopt -s extdebug
    lns+=("$(declare -F x.$c | awk '{print $2}')")
    rows+=("${c}")
    shopt -u extdebug
  done

  x_functions="$(construct_column command_comments_parser)"

  lns=()
  rows=()
  docs=()

  parse_x_options X_OPTIONS

  for c in "${X_OPTIONS[@]}"; do
    lns+=("${c##*)}")
    rows+=("${c%%)*}")
  done

  x_options="$(construct_column command_options_comments_parser)"

  cat <<EOF

$BOLD$UNDERSCORE$(basename $0)$OFF centralizes all relevant project commands.

${BOLD}USAGE:
    $RED$(basename $0) x.function [--x-option ...] args [optional args]$OFF

${BOLD}x.functions:$OFF
$(echo "${x_functions}" |
    sed -E 's/    ([[:alnum:][:punct:]]*)(.*)/    '"$BLUE$BOLD"'\1'"$OFF"'\2''/')

${BOLD}--x-options:$OFF
$(echo "${x_options}" |
      sed -E 's/    ([[:alnum:][:punct:]]*)(.*)/    '"$GREEN$BOLD"'\1'"$OFF"'\2''/')
EOF

}

# gets all --x-options values
parse_x_options() {
  declare -n list="$1" # pass ref by name
  while IFS= read -r line; do
    list+=("$(awk '{$1=$1;print $1 $NF}' <<<"$line")")
  done < <(sed -nr '/.*(--x-[=*[:alnum:]_-]+[)]+).*/{p;=}' $0 | sed '{N;s/\n/ /}')
  mapfile -t list < \
    <(LC_COLLATE=C sort < <(printf "%s\n" "${list[@]}"))
}

clean_comment() {
  tmp="$1"
  tmp="${tmp//\#/}"
  comment="${tmp#* }"
  [[ -z $comment ]] && comment="·"
  # TODO split in % MAX_COMMENT_LEN lines instead, on spaces only.
  ((${#comment} > MAX_COMMENT_LEN)) && comment="${comment:0:MAX_COMMENT_LEN}..."
  echo "$comment"
}

clean_args() {
  tmp="$1"
  tmp="${tmp,,##*args\:}"
  args="${tmp#* }"
  echo "$args"
}

# --------------------- completion and delegation --------------------
#      `complete -C foo foo` > `source <(foo bloated_completion)`

while IFS= read -r line; do
  [[ $line =~ ^declare\ -f\ x\. ]] || continue
  COMMANDS+=("${line##declare -f x.}")
done < <(declare -F)
# sort the array. Mimic file input to sort
mapfile -t COMMANDS < \
  <(LC_COLLATE=C sort < <(printf "%s\n" "${COMMANDS[@]}"))

MAX_XFN_LEN=0 # for logging purposes
for c in "${COMMANDS[@]}"; do
  len=${#c}
  ((len > MAX_XFN_LEN)) && MAX_XFN_LEN=$((len - 1)) # remove "x." but account for extra last space appended.
done

if [[ -n $COMP_LINE ]]; then
  pre="${COMP_LINE##* }" # the part after the last space in the current command
  cur_commands=(${COMP_LINE%"$pre"})

  for c in "${COMMANDS[@]}"; do
    if [[ " ${cur_commands[*]} " =~ " ${c} " ]]; then
      xfn_specified=true
      break
    fi
  done

  for c in "${COMMANDS[@]}"; do
    test -z "${xfn_specified}" || break
    test -z "${pre}" -o "${c}" != "${c#"${pre}"}" -a "${pre}" != "${c}" && echo "${c}"
  done

  test -z "${xfn_specified}" && exit

  declare __x_options x_options_lines

  parse_x_options x_options_lines

  for c in "${x_options_lines[@]}"; do
    tmp="${c%%)*}"
    xopt="${tmp//\*/}"
    __x_options+=("$xopt")
  done

  declare -A __x_opts_seen
  for cmd in "${cur_commands[@]}"; do
    for opt in ${__x_options[@]}; do
      if [[ "$cmd" == *"$opt"* ]]; then
        __x_opts_seen[$opt]=true
        break
      fi
    done
  done

  for opt in ${__x_options[@]}; do
    [[ -n "${__x_opts_seen[$opt]}" ]] && continue
    # TODO prevent opts that accept values having whitespace added
    [[ ${opt:0:${#pre}} == "${pre,,}" ]] && echo "${opt}"
  done

  exit
fi

declare CMD="$1"

# First comment lines automatically added to usage docs.
set +e
while [[ "$#" -gt 0 ]]; do
  case $1 in
  --x-help)
    # Show help for a particular x function.
    COMMANDS=("$CMD")
    usage
    exit
    ;;
  --x-force-regen) # this comment should be ignored
    # Removes generation cache, forcing a new run.
    force_regen=1
    ;;
  --x-no-confirmation)
    # Bypasses confirmation messages.
    export NO_CONFIRMATION=1
    ;;
  --x-env=*)
    # Environment to run commands in. Defaults to "dev".
    # Args: env
    env="${1#--x-env=}"
    valid_envs="dev staging prod ci"
    if [[ ! " ${valid_envs[*]} " =~ " $env " ]]; then
      err "Valid environments: $valid_envs"
    fi
    ;;
  *)
    # will set everything else back
    args+=("$1")
    ;;
  esac
  shift
done
set -e
for arg in ${args[@]}; do
  set -- "$@" "$arg"
done
# applicable to any command
ensure_envvars_set ".env.template" ".env.${env}"
source ".env.$env"

# handle executing x functions
if [[ -n "$1" ]]; then
  shift
  for c in "${COMMANDS[@]}"; do
    declare cmd=$(command -v "x.$c")
    if [[ $c == "$CMD" && -n "$cmd" ]]; then
      "x.$CMD" "$@"
      exit $?
    fi
  done
fi

# default to show usage if its a noop
usage
