#!/usr/bin/env bash
# shellcheck disable=1091,2155,2068,2086,2031,2048,2178,2120
#
#
# Main s̶c̶r̶i̶p̶t program to manage the entire project stack.
#
#

source ".helpers.sh"

starting_cwd=$(pwd)
ensure_pwd_is_top_level

declare X_FORCE_REGEN X_NO_CONFIRMATION X_NO_GEN X_NO_BACKUP X_ENV X_DEBUG X_NO_EXIT_CLEANUP
declare BUILD_VERSION

readonly CURRENT_BRANCH=$(git rev-parse --abbrev-ref HEAD)

readonly SPEC_PATH="openapi.yaml"

readonly BUILD_DIR="bin/build"
readonly TOOLS_DIR="bin/tools"
readonly PROTO_DIR="internal/pb"
readonly MIGRATIONS_DIR="db/migrations"
readonly MIGRATIONS_TABLE="schema_migrations"
readonly POST_MIGRATIONS_DIR="db/post-migrations"
readonly POST_MIGRATIONS_TABLE="schema_post_migrations"
readonly CERTIFICATES_DIR="certificates"
readonly GOWRAP_TEMPLATES_DIR="internal/gowrap-templates"
readonly REPOS_DIR="internal/repos"
readonly PG_REPO_DIR="$REPOS_DIR/postgresql"
readonly XO_TEMPLATES_DIR="$PG_REPO_DIR/xo-templates"
readonly BIN_TEMPLATES_DIR="bin/templates"

readonly POSTGRES_TEST_DB="postgres_test"
readonly DUMPS_DIR="$HOME/openapi_go_gin_postgres_dumps"
pkg="$(head -1 go.mod)"
readonly GOMOD_PKG="${pkg#module *}"
# can only run with count=1 at most
readonly XO_TESTS_PKG="$GOMOD_PKG/internal/repos/postgresql/xo-templates/tests"
readonly OPID_AUTH_PATH="operationAuth.gen.json"
readonly REST_MODELS="internal/rest/models.spec.go"
readonly PG_REPO_GEN="$PG_REPO_DIR/gen"
readonly RUN_CACHE=".run.cache"
readonly GEN_CACHE=".generate.cache"
readonly GEN_CACHE_BACKUP="$GEN_CACHE-backup"
readonly SWAGGER_UI_DIR="internal/static/swagger-ui"
readonly FRONTEND_GEN="frontend/src/gen"
readonly MAX_FNAME_LOG_LEN=13
readonly GEN_SCHEMA_PATH="db/schema.sql"

readonly TMP="${TMPDIR:-"/tmp"}"

GEN_POSTGRES_DB="gen_db"

failed_tool_build_marker="$TMP/failed_tool_build_marker"
rm -f "$failed_tool_build_marker"

go_test_flags=()
# shuffle disables go test caching. Definitely don't want in dev.
test -n "$CI" && go_test_flags+=("-shuffle=on")

# determines whether gen cache should be restored at program exit, i.e. failed commands.
# cache folder must be cleaned at exit if true.
# only restores if true.
declare need_backup_restore
# stores the first executing function of xsetup.backup to track if caching gen is already running,
# to allow for nested xsetup.backup and cache-cleanup inside multiple functions.
xsetup_backup_gen_caller=""

# stores the first executing function to determine if a migration
# is needed when running gen* functions which call each other
xsetup_gen_migrated=""

# stores the first executing function to determine if tools have been built
xsetup_tools_built=""

# log for any function output.
xlog() {
  local fname="${FUNCNAME[1]#*.}"
  local color="$BLUE"
  local max_len=$MAX_FNAME_LOG_LEN

  [[ "$CMD" = "$fname" ]] && cat && return

  if [[ "${FUNCNAME[1]%%.*}" != "x" ]]; then
    fname="${FUNCNAME[1]}" # show non-x funcs
    color="$MAGENTA"
  fi

  if [[ "${FUNCNAME[1]}" =~ ^.*(check\.bin|install\.bin).* ]]; then
    max_len=100
  fi

  if [[ ${#fname} -gt $max_len ]]; then
    fname="${fname:0:$max_len}…"
  fi

  local _=$(printf "%*s |\n" $((max_len + 1)) "$fname")
  sed -ue "s/^/${color}$fname >${OFF} /"
}

# log stderr for any function output.
# sed is buffering by default (without -u) so streams dont preserve order
# > >(one) 2> >(two) are background processes so it will break our parallel code.
xerr() {
  local fname="${FUNCNAME[1]#*.}"
  local max_len=$MAX_FNAME_LOG_LEN

  [[ "$CMD" = "$fname" ]] && cat && return
  if [[ ${#fname} -gt $max_len ]]; then
    fname="${fname:0:$max_len}…"
  fi

  local _=$(printf "%*s |\n" $((max_len + 1)) "$fname")
  sed -ue "s/^/${RED}$fname >${OFF} /" >&2
}

kill_descendants() {
  # air and vite spawn processes as well, need to kill those (whose parent is pid), kill $pid will not kill children. pkill -P would also work
  kill $pids || true
  kill "$(list_descendants $pids)" || true
  pids=""
}

######################### x-functions setup #########################

xsetup.build-tools() {
  test -n "$xsetup_tools_built" && return

  xsetup_tools_built="${FUNCNAME[1]}"

  x.gen.build-tools || err Could not rebuild gen tools
}

backup_branch="backup-gen-$(uuidgen)"

# TODO: when running gen and then stashing changes, or switching branches,
# cache should be removed.
gen-cache.backup() {
  rm -rf "$GEN_CACHE_BACKUP" || true
  cp -r "$GEN_CACHE" "$GEN_CACHE_BACKUP" || true
}

gen-cache.restore() {
  rm -rf "$GEN_CACHE"
  mv "$GEN_CACHE_BACKUP" "$GEN_CACHE" || true
}

# Create a backup stash with current changes.
# Uncommitted changes are restored on error unless --x-no-backup flag is passed.
xsetup.backup() {
  test -n "$xsetup_backup_gen_caller" && return

  xsetup_backup_gen_caller="${FUNCNAME[1]}"

  mkdir -p "$GEN_CACHE"

  backup_stash_name="backup-stash-$backup_branch"

  echo "$backup_branch" >backup-gen-stash-dummy.txt # make sure something unique and not gitignored is in the current branch
  gen-cache.backup
  git stash push -m "$backup_stash_name" --include-untracked || err "Could not backup untracked changes before codegen"
  git checkout -b "$backup_branch" &>/dev/null
  git stash apply "stash^{/$backup_stash_name}" &>/dev/null

  need_backup_restore=true # unless
}

xsetup.backup.cleanup() {
  # only accept gen if the main function that backed it up in the first place
  # finishes successfully, i.e. this very function was called
  if [[ "$xsetup_backup_gen_caller" = "${FUNCNAME[1]}" ]]; then
    need_backup_restore=false
  fi
}

xsetup.backup.restore() {
  echo "
Backup branch: $backup_branch

${RED}Restoring previous uncommitted changes to current branch (${YELLOW}${CURRENT_BRANCH}${RED})${OFF}"
  wait # for any pending job

  git reset --hard &>/dev/null && git clean -df &>/dev/null
  # if not removing the whole cache folder we get `already exists, no checkout` upon stash apply since we have just reset gitignore
  # IMPORTANT: we do want to delete regardless since we are restoring the cache folder on stash apply so we
  # don't need complex cache invalidation based on what's been run
  rm -rf "$GEN_CACHE"
  git stash apply "stash^{/$backup_stash_name}" &>/dev/null
}

xsetup.drop-and-migrate-gen-db() {
  test -n "$xsetup_gen_migrated" && return
  xsetup_gen_migrated=1

  { { {
    cache_all "$GEN_CACHE/db.md5" db/ --exclude "db/schema.sql" && return 0

    POSTGRES_DB="$GEN_POSTGRES_DB"

    x.db.drop
    x.migrate up
    export_gen_db_schema

  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

######################### x-functions #########################

# Check build dependencies are met and prompt to install if missing.
x.check-build-deps() {
  { { {
    mkdir -p $TOOLS_DIR

    while IFS= read -r line; do
      [[ $line =~ ^declare\ -f\ check\.bin\. ]] && BIN_CHECKS+=("${line##declare -f check.bin.}")
      [[ $line =~ ^declare\ -f\ install\.bin\. ]] && BIN_INSTALLS+=("${line##declare -f install.bin.}")
    done < <(declare -F)

    echo "Checking dependency minimum versions..."
    for bin in "${BIN_CHECKS[@]}"; do
      # local r
      # r="$(...)" # redirect to var while also streaming unbuffered output with | tee /dev/tty
      if "check.bin.$bin"; then
        continue
      fi

      if ! element_in_array "$bin" "${BIN_INSTALLS[@]}"; then
        echo "No automatic installation available. Please install $bin manually and retry"
        exit 1
      fi

      with_tty confirm "Do you want to install $bin now?" || exit 1

      echo "Installing $bin..."
      if ! "install.bin.$bin"; then
        err "$bin installation failed"
      fi

      if ! "check.bin.$bin"; then
        err "$bin check failed after installation"
      fi

      echo "Installed $bin..."
    done
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Check dependencies and fetch required tools.
x.bootstrap() {
  { { {
    git submodule update --init --recursive # sync later on with git `git submodule update --force --recursive --remote`

    x.check-build-deps
    x.backend.sync-deps
    x.install-tools
    x.setup.swagger-ui
    x.gen.build-tools

    cd frontend
    pnpm i --frozen-lockfile
    cd -

    cd e2e
    pnpm i --frozen-lockfile
    cd -

    traefik_dir="$HOME/traefik-bootstrap"
    with_tty confirm "Do you want to setup and run traefik (install dir: $traefik_dir)?" && x.setup.traefik "$traefik_dir"
    echo "${RED}Make sure to add \`complete -o nospace -C project project\` to your ~/.bashrc for completion.${OFF}"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Install miscellaneous tool binaries locally.
x.install-tools() {
  { { {
    declare -A jobs
    local pids=() failed_jobs=()

    # install node libs with --prefix $TOOLS_DIR, if any
    # ...
    # TODO: abstract to function that just accepts commands to run
    # as it is we might just use parallel but may use own functions later.
    commands=(
      "go install -tags 'postgres' github.com/golang-migrate/migrate/v4/cmd/migrate@v4.15.2"
      "go install github.com/sqlc-dev/sqlc/cmd/sqlc@v1.24.0"
      "go install github.com/golangci/golangci-lint/cmd/golangci-lint@v1.55.2"
      "go install github.com/tufin/oasdiff@v1.10.9"
      "go install golang.org/x/tools/cmd/goimports@latest"
      "go install mvdan.cc/gofumpt@latest"
      "go install mvdan.cc/sh/v3/cmd/shfmt@latest"
      "go install github.com/air-verse/air@latest"
      "go install github.com/danicc097/xo/v5@v5.6.0"
      "go install github.com/mikefarah/yq/v4@v4.34.2"
      "go install github.com/hexdigest/gowrap/cmd/gowrap@v1.4.0"
      "go install golang.org/x/tools/cmd/stringer@latest"
      "go install github.com/maxbrunsfeld/counterfeiter/v6@latest"
      "go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.28.1"
      "go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.2"
      # vacuum lint openapi.exploded.yaml -ed
      "go install github.com/daveshanley/vacuum@latest"
    )
    for command in "${commands[@]}"; do
      $command &
      pids+=($!)
      jobs[$!]="$command"
    done

    for pid in "${pids[@]}"; do
      wait -fn "$pid" || failed_jobs+=("${jobs[$pid]}")
    done

    # For failing installs for no apparent reason, try running 'go clean -cache' and retry. Else install with ' -mod=readonly' flag.
    if [[ ${#failed_jobs[@]} -gt 0 ]]; then
      err "Could not install all tools. Failed jobs:
$(join_by $'\n' "${failed_jobs[@]}")"
    fi
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Fetch latest Swagger UI bundle.
x.setup.swagger-ui() {
  { { {
    local name

    name="$(curl --retry 10 --user-agent "$GOMOD_PKG" -fsSL "https://api.github.com/repos/swagger-api/swagger-ui/releases/latest" | jq -r ".. .tag_name? // empty")"
    curl --retry 10 -fsSL "github.com/swagger-api/swagger-ui/archive/refs/tags/$name.tar.gz" -o swagger-ui.tar.gz
    tar xf swagger-ui.tar.gz swagger-ui-"${name#*v}"/dist --one-top-level=swagger-ui --strip-components=2
    rm swagger-ui.tar.gz
    mkdir -p $SWAGGER_UI_DIR
    mv swagger-ui/* $SWAGGER_UI_DIR
    rm -r swagger-ui
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Run pre-generation scripts in the internal package.
x.gen.pregen() {
  xsetup.backup
  xsetup.drop-and-migrate-gen-db
  xsetup.build-tools
  { { {
    POSTGRES_DB="$GEN_POSTGRES_DB"

    echo "Running generation"

    # NOTE: swaggest won't generate for arrays of structs. use a struct with array fields instead.
    generate_structs_map # spec structs or their content in rest package might have changed

    go build -o $BUILD_DIR/codegen cmd/codegen/main.go || mark_failed_tool_build

    local rest_structs=() refs=() spec_schemas=()

    generate_missing_spec_schemas

    codegen validate-spec -env=".env.$X_ENV"
    # useless after merging models and db
    # ast-parser verify-no-import --imports "$GOMOD_PKG/internal/repos/postgresql/gen/models" "$REST_MODELS" || err Please use generated rest package OpenAPI types instead.

    sync_spec_with_db

    ######## Ensure consistent style for future codegen

    pascal_case_spec_operation_ids

    update_roles_and_scopes

    validate_spec
    codegen pre -env=".env.$X_ENV" -op-id-auth="$OPID_AUTH_PATH"
    update_spec_with_structs
    remove_schemas_marked_for_deletion
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  xsetup.backup.cleanup
}

pascal_case_spec_operation_ids() {
  echo "Applying PascalCase to operation IDs"

  # outputs safe double-quoted paths for yq
  # https://github.com/mikefarah/yq/issues/1295
  mapfile -t opid_paths < <(yq e '
      .paths[][].operationId
      | path
      | with(.[] | select(contains(".") or contains("/") or contains("{")); . = "\"" + . + "\"")
      | join(".")
    ' $SPEC_PATH)
  mapfile -t opids < <(yq e ".paths[][].operationId" $SPEC_PATH)

  local ops=""
  for i in ${!opids[@]}; do
    local new_opid=""
    to_pascal new_opid "${opids[$i]}"
    ops+=".${opid_paths[$i]}=\"${new_opid}\" |" # cant have space
  done

  yq_op="${ops%"|"}"

  yq e "$yq_op" -i "$SPEC_PATH"
}

# ext vars: spec_schemas, rest_structs
generate_missing_spec_schemas() {
  declare -A rest_schema_lookup spec_schema_lookup

  go-utils.find_structs rest_structs "$REST_MODELS"
  for schema in "${rest_structs[@]}"; do # only interested in generating rest models.spec.go schemas. let the rest fail normally.
    rest_schema_lookup["$schema"]=1
  done

  mapfile -t spec_schemas < <(yq eval '.components.schemas[] | key' "$SPEC_PATH")
  for schema in "${spec_schemas[@]}"; do # only interested in generating rest models.spec.go schemas. let the rest fail.
    spec_schema_lookup["$schema"]=1
  done

  local missing_rest_models=()
  mapfile -t refs < <(sed -nr "s/[^#]?.*\\$ref: '#\/components\/schemas\/(.*)'/\1/p" $SPEC_PATH | sort | uniq)
  for ref in "${refs[@]}"; do
    if [[ -n "${rest_schema_lookup["$ref"]}" && -z "${spec_schema_lookup["$ref"]}" ]]; then
      missing_rest_models+=("$ref")
    fi
  done

  if [[ ${#missing_rest_models[@]} -gt 0 ]]; then
    echo "${YELLOW}Generating missing rest package \$ref's:"
    printf "${YELLOW}  - %s\n" "${missing_rest_models[@]}"
    echo "${OFF}"

    local new_schemas=""
    for ref in "${missing_rest_models[@]}"; do
      new_schemas+="\"$ref\": {
            \"x-gen-struct\": \"$ref\",
            \"x-is-generated\": true
          },"
    done

    yq eval-all ".components.schemas += {
        ${new_schemas%,}
      }" -i "$SPEC_PATH"
  fi
}

validate_spec() {
  echo "Validating spec $SPEC_PATH"

  mapfile -t invalid_schemas < <(yq e '.components.schemas[] | select(has("x-gen-struct") and (has("x-is-generated") | not)) | key' $SPEC_PATH)
  [[ ${#invalid_schemas[@]} -gt 0 ]] && err "x-gen-struct can only be used in generated schemas (x-is-generated). Found in:
$(join_by $'\n' "${invalid_schemas[@]}")"

  mapfile -t invalid_schemas < <(yq e '.components.schemas[] | select(has("x-gen-struct") and key != .x-gen-struct) | key' $SPEC_PATH)
  [[ ${#invalid_schemas[@]} -gt 0 ]] && err "x-gen-struct must match schema name. Please rename:
$(join_by $'\n' "${invalid_schemas[@]}")"

  local clashes=() services_prefix_clashes=() db_prefix_clashes=()
  for custom_schema in $(yq ".components.schemas[] | select((.x-is-generated == true) | not) | key" $SPEC_PATH); do
    if [[ " ${rest_structs[*]} " =~ " ${custom_schema} " ]]; then
      line_number=$(yq ".components.schemas.$custom_schema | line" $SPEC_PATH)
      clashes+=("	$custom_schema (${SPEC_PATH}:${line_number})")
    fi
    if [[ $custom_schema =~ ^Services[A-Z].*$ ]]; then
      services_prefix_clashes+=("	$custom_schema (${SPEC_PATH}:${line_number})")
    fi
    if [[ $custom_schema =~ ^Models[A-Z].*$ ]]; then
      db_prefix_clashes+=("	$custom_schema (${SPEC_PATH}:${line_number})")
    fi
  done

  if [[ ${#clashes[@]} -gt 0 ]]; then
    err "The following non-generated schemas would have been overridden by internal/rest/models.spec.go structs. Please rename or delete:
$(join_by $'\n' "${clashes[@]}")"
  fi
  if [[ ${#services_prefix_clashes[@]} -gt 0 ]]; then
    err "Services prefix is restricted to generated structs. Please rename or delete:
$(join_by $'\n' "${services_prefix_clashes[@]}")"
  fi
  if [[ ${#db_prefix_clashes[@]} -gt 0 ]]; then
    err "Models prefix is restricted to generated structs. Please rename or delete:
  $(join_by $'\n' "${db_prefix_clashes[@]}")"
  fi
}

update_roles_and_scopes() {
  echo "Updating roles and scopes"
  ######## Sync spec enums with external policy sources and validate existing schema enums.
  ######## External json files are the source of truth, indexed by enum name
  # arrays can't be nested in bash
  declare -A enum_src_files=(
    [Scope]="$SCOPE_POLICY_PATH"
    [Role]="$ROLE_POLICY_PATH"
  )
  declare -A enum_vext=(
    [Scope]="x-required-scopes"
    [Role]="x-required-role"
  )
  declare -A enum_values=()
  for enum in ${!enum_src_files[@]}; do
    [[ $(yq e ".components.schemas | has(\"$enum\")" $SPEC_PATH) = "false" ]] &&
      yq e ".components.schemas.$enum.type = \"string\"" -i $SPEC_PATH

    local src_file="${enum_src_files[$enum]}"
    vendor_ext="${enum_vext[$enum]}"

    enums=$(yq -P --output-format=yaml '.[] | key' $src_file)
    mapfile -t enums <<<$enums

    src_comment="$src_file keys"
    replace_enum_in_spec "$enum" enums "$src_comment"

    mapfile spec_enums < <(yq e ".paths[][].$vendor_ext | select(length > 0)" $SPEC_PATH)
    spec_enums=("${spec_enums[*]//- /}")
    mapfile -t spec_enums < <(printf "\"%s\"\n" ${spec_enums[*]})
    mapfile -t clean_enums < <(printf "\"%s\"\n" ${enums[*]})
    # ensure only existing enums from src_file are used
    for spec_enum in "${spec_enums[@]}"; do
      [[ ! " ${clean_enums[*]} " =~ " ${spec_enum} " ]] && err "$spec_enum is not a valid '$enum'"
    done

    enum_list=$(printf ",\"%s\"" "${enums[@]}")
    enum_list="[${enum_list:1}]"
    enum_values[$enum]=$enum_list
  done

  ######## IDE intellisense/validation
  yq -e ".definitions.Operation.properties +=
        {
          \"${enum_vext[Role]}\": {
            \"type\": \"string\",
            \"enum\": ${enum_values[Role]}
          },
          \"${enum_vext[Scope]}\": {
            \"type\": \"array\",
            \"items\": {\"enum\": ${enum_values[Scope]}}
          }
        }" -i -oj .vscode/openapi-schema.json

  ######## Generate shared policies once the spec has been validated
  echo "Writing shared auth policies"

  yq -o=json e "
    .paths[][]
    | explode(.)
    | {
      .operationId: {
        \"scopes\": .x-required-scopes,
        \"role\": .x-required-role,
        \"requiresAuthentication\": has(\"security\")
        }
      }
    | select(.[]) as \$i ireduce ({}; . + \$i)
  " $SPEC_PATH >$OPID_AUTH_PATH
}

sync_spec_with_db() {
  cache_all "$GEN_CACHE/models.md5" ${base_cache_deps[@]} db/ $SPEC_PATH && return 0

  sync_db_enums_with_spec

  # TODO: repo specific changes should all be abstracted to their own standalone functions
  # to be easily replaced in other repos
  ######## Sync projects and related project info

  mapfile -t db_projects < <(docker.postgres.psql -d $POSTGRES_DB -c "select name from projects;" 2>/dev/null)

  [[ ${#db_projects[@]} -gt 0 ]] || err "No projects found in database $POSTGRES_DB"
  replace_enum_in_spec "ProjectName" db_projects "projects table"

  for project in ${db_projects[@]}; do
    ### kanban steps
    mapfile -t kanban_steps < <(docker.postgres.psql -d $POSTGRES_DB -c "
          select name from kanban_steps where project_id = (
            select project_id from projects where name = '$project'
            );" 2>/dev/null)
    [[ ${#kanban_steps[@]} -gt 0 ]] || {
      echo "${YELLOW}[WARNING] No kanban steps found for project '$project' in database $POSTGRES_DB${OFF}" && continue
    }

    local pascal_project=""
    to_pascal pascal_project "$project"
    schema_name="${pascal_project}KanbanSteps"
    replace_enum_in_spec "$schema_name" kanban_steps "kanban_steps table"

    ### work item types
    mapfile -t work_item_types < <(docker.postgres.psql -d $POSTGRES_DB -c "
          select name from work_item_types where project_id = (
            select project_id from projects where name = '$project'
            );" 2>/dev/null)
    [[ ${#work_item_types[@]} -gt 0 ]] || {
      echo "${YELLOW}[WARNING] No work item types found for project '$project' in database $POSTGRES_DB${OFF}" && continue
    }
    schema_name="${pascal_project}WorkItemTypes"
    replace_enum_in_spec "$schema_name" work_item_types "work_item_types table"
  done

  generate_models_mappings
}

generate_repo_constructor() {
  local out_path="internal/repos/repos.gen.go"

  local repos="$REPOS_DIR/repos.go"
  local repo_interfaces=()
  go-utils.find_interfaces repo_interfaces $repos

  for iface in "${repo_interfaces[@]}"; do
    local struct_fields+=("${iface} ${iface}")
  done

  cat <<EOF >$out_path
// Code generated by project. DO NOT EDIT.

package repos

type Repos struct {
    $(join_by $'\n' "${struct_fields[@]}")
}
EOF

  gofumpt -w $out_path
}

# for manually inserted elements via migrations, e.g. projects, kanban_steps, work_item_type,
# generate 2-way maps id<- ->name to save up useless db calls and make logic switching
# and repos usage much easier
generate_models_mappings() {
  local model_mappings_path="internal/models_mappings.gen.go"
  cat <<EOF >$model_mappings_path
// Code generated by project. DO NOT EDIT.

package internal

import (
  "$GOMOD_PKG/$PG_REPO_GEN/models"
)
EOF

  mapfile -t projects_rows < <(docker.postgres.psql -d $POSTGRES_DB -c "select project_id,name from projects;" 2>/dev/null)
  generate_model_mappings_dicts "Project" models.ProjectID projects_rows "Name"

  for project in ${db_projects[@]}; do
    local pascal_project=""
    to_pascal pascal_project "$project"
    mapfile -t kanban_steps_rows < <(docker.postgres.psql -d $POSTGRES_DB -c "
          select kanban_step_id,name from kanban_steps where project_id = (
            select project_id from projects where name = '$project'
            );" 2>/dev/null)
    [[ ${#kanban_steps_rows[@]} -gt 0 ]] || continue
    prefix="${pascal_project}KanbanSteps"
    generate_model_mappings_dicts $prefix models.KanbanStepID kanban_steps_rows

    {
      kanban_steps_order_rows=()
      mapfile -t kanban_steps_order_rows < <(docker.postgres.psql -d $POSTGRES_DB -c "
          select kanban_step_id,step_order from kanban_steps where project_id = (
            select project_id from projects where name = '$project'
            );" 2>/dev/null)
      [[ ${#kanban_steps_order_rows[@]} -gt 0 ]] || continue
      echo "var (
    ${prefix}StepOrderByID = map[models.KanbanStepID]int{
  "
      for row in "${kanban_steps_order_rows[@]}"; do
        first=$(cut_first "$row" "|") # always safe
        mapfile -t arr <<<"${first}"
        local id="${arr[0]}"
        local kanban_step="${arr[1]}"
        echo "${id}: ${kanban_step},"
      done
      echo "})"
    } >>$model_mappings_path

    mapfile -t work_item_types_rows < <(docker.postgres.psql -d $POSTGRES_DB -c "
          select work_item_type_id,name from work_item_types where project_id = (
            select project_id from projects where name = '$project'
            );" 2>/dev/null)
    [[ ${#work_item_types_rows[@]} -gt 0 ]] || continue
    prefix="${pascal_project}WorkItemTypes"
    generate_model_mappings_dicts $prefix models.WorkItemTypeID work_item_types_rows
  done

  gofumpt -w $model_mappings_path
}

# generates dictionaries for existing database elements, meant for those
# inserted exclusively via migrations
generate_model_mappings_dicts() {
  local prefix="$1"
  local id_type="$2"
  local -n __arr="$3" # db rows
  local prefix_suffix="$4"

  {
    echo "var ("
    echo "${prefix}NameByID = map[${id_type}]models.${prefix}${prefix_suffix}{"

    for row in "${__arr[@]}"; do
      first=$(cut_first "$row" "|") # always safe
      mapfile -t arr <<<"${first}"
      local id="${arr[0]}" name="${arr[1]}" pascal_name=""
      to_pascal pascal_name "$name"
      echo "${id}: models.${prefix}${prefix_suffix}${pascal_name},"
    done

    echo "}"
    echo "${prefix}IDByName = map[models.${prefix}${prefix_suffix}]${id_type}{"

    for row in "${__arr[@]}"; do
      first=$(cut_first "$row" "|") # always safe
      mapfile -t arr <<<"${first}"
      local id="${arr[0]}" name="${arr[1]}" pascal_name=""
      to_pascal pascal_name "$name"
      echo "models.${prefix}${prefix_suffix}${pascal_name}: ${id},"
    done

    echo "})"
  } >>"$model_mappings_path"
}

clean_yq_array() {
  local -n __arr="$1"
  __arr=("${__arr[*]//- /}")
  mapfile -t __arr < <(printf "\"%s\"\n" ${__arr[*]})
  echo ${__arr[@]}
}

go_test() {
  local cache_opt="-count=1" exit_code=1
  cache_all "$GEN_CACHE/go-test.md5" .env.$X_ENV db/ >/dev/null && cache_opt=""

  set -x
  APP_ENV="$X_ENV" go test ${go_test_flags[@]} $cache_opt $@
  exit_code=$?
  set +x

  return $exit_code
}

# Run post-generation scripts in the internal package.
x.gen.postgen() {
  xsetup.backup
  xsetup.drop-and-migrate-gen-db
  xsetup.build-tools
  { { {
    POSTGRES_DB="$GEN_POSTGRES_DB"

    echo "Running generation"

  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  xsetup.backup.cleanup
}

# Generate type-safe Go code from SQL.
x.gen.sqlc() {
  xsetup.backup
  xsetup.drop-and-migrate-gen-db
  { { {
    echo "Running generation"
    rm -f "$PG_REPO_GEN"/models/*.sqlc.go
    x.lint.sql
    sqlc generate --experimental -f "$PG_REPO_DIR"/sqlc.yaml || err "Failed sqlc generation"
    rm -f "$PG_REPO_GEN"/models/models.go                            # sqlc enums
    sed -i 's/\bdb DBTX\b/d DBTX/g' "$PG_REPO_GEN"/models/querier.go # consistent with repo gowrap gen so we can use reposwrappers
    gowrap gen \
      -g \
      -p "$GOMOD_PKG/$PG_REPO_GEN/models" \
      -i Querier \
      -t "$GOWRAP_TEMPLATES_DIR/sqlc.tmpl" \
      -o "$PG_REPO_DIR/sqlc_querier_wrapper.gen.go"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  xsetup.backup.cleanup
}

# Automatically generate CRUD and index queries with joins based on existing indexes from a Postgres schema.
x.gen.xo() {
  xsetup.backup
  xsetup.drop-and-migrate-gen-db
  { { {
    cache_all "$GEN_CACHE/xo.md5" ${base_cache_deps[@]} go.mod db/ $XO_TEMPLATES_DIR/ \
      --exclude "$XO_TEMPLATES_DIR/tests/*" && return 0

    echo "Running generation"

    rm -rf "$PG_REPO_GEN"/models/*.xo.go

    mkdir -p "$PG_REPO_GEN"/models
    xo_schema -o "$PG_REPO_GEN"/models --debug \
      --schema public \
      --ignore "*.created_at" \
      --ignore "*.updated_at" || err "Failed xo public schema generation" &

    xo_schema -o "$PG_REPO_GEN"/models --debug \
      --schema extra_schema \
      --ignore "*.created_at" \
      --ignore "*.updated_at" ||
      err "Failed xo extra_schema schema generation" &

    mkdir -p "$PG_REPO_GEN"/models
    xo_schema -o "$PG_REPO_GEN"/models \
      --schema cache ||
      err "Failed xo cache schema generation" &

    wait_without_error

    files=$(find "$PG_REPO_GEN/models" \
      -name "*.go")
    goimports -w $files

    xo_generate_spec_helpers
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  xsetup.backup.cleanup
}

# Generate a type-safe SQL builder.
x.gen.jet() {
  xsetup.backup
  xsetup.drop-and-migrate-gen-db
  xsetup.build-tools
  { { {
    # results may be combined with xo's *Public structs and not reinvent the wheel for jet.
    # should not be hard to generate all adapters at once jet->xo *Public in a new file alongside jet gen.
    # in the end fields are the same name if goName conventions are followed (configurable via custom jet cmd)
    # if it gives problems for some fields (ID, API and the like)
    echo "Running generation"

    local gen_path="$PG_REPO_GEN/jet"
    local schema=public
    rm -rf "$gen_path"
    {
      jet -dbname="$GEN_POSTGRES_DB" --env=.env."$X_ENV" --out=./"$gen_path" \
        --schema="$schema" \
        --ignore-tables="$MIGRATIONS_TABLE,$POST_MIGRATIONS_TABLE"
      mv "./$gen_path"/$GEN_POSTGRES_DB/* "$gen_path"
      rm -r "./$gen_path/$GEN_POSTGRES_DB/"
    }
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  xsetup.backup.cleanup
}

# Generate repo interface wrappers with common logic: tracing, timeout...
# Args: [name]
x.gen.gowrap() {
  xsetup.backup
  { { {
    local name="$1" # optional to force regen on an interface
    local repos="$REPOS_DIR/repos.go"

    echo "Running generation"

    local cache="$GEN_CACHE/gowrap"
    local suffixes=(
      "retry-repo:with_retry"
      "timeout:with_timeout"
      "otel:with_otel"
      "prometheus:with_prometheus" # TODO: https://last9.io/blog/native-support-for-opentelemetry-metrics-in-prometheus/ https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/pkg/translator/prometheus
    )

    local repo_interfaces=()
    go-utils.find_interfaces repo_interfaces $repos

    mkdir -p "$cache"

    if test -n "$name"; then
      repo_interfaces=("$name")
    fi

    local updated_ifaces=()
    for iface in "${repo_interfaces[@]}"; do
      iface_content="$(go-utils.get_interface_methods $iface $repos)"
      if diff "$cache/$iface" <(echo "$iface_content") &>/dev/null && [[ "$iface" != "$name" ]]; then
        [[ $X_FORCE_REGEN -eq 0 ]] && continue
      fi

      for suffix in ${suffixes[@]}; do
        {
          IFS=":" read -r -a arr <<<${suffix}
          local tmpl="${arr[0]}"
          local suffix="${arr[1]}"
          gowrap gen \
            -g \
            -p $GOMOD_PKG/$REPOS_DIR \
            -i $iface \
            -t "$GOWRAP_TEMPLATES_DIR/$tmpl.tmpl" \
            -o "$REPOS_DIR/reposwrappers/${iface,,}_$suffix.gen.go"
        } &
      done

      echo "$iface_content" >"$cache/$iface"
      updated_ifaces+=("$iface")
    done

    wait_without_error || err Failed jobs

    if [[ ${#updated_ifaces[@]} -gt 0 ]]; then
      echo "Updated repo interfaces: ${updated_ifaces[*]}"
    fi
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  xsetup.backup.cleanup
}

# Generate Go client and server from spec.
x.gen.client-server() {
  xsetup.build-tools
  xsetup.backup
  { { {
    echo "Running generation"
    paths_file=".openapi.paths.yaml"

    local all_types=()
    # mapfile -t all_types < <(ast-parser find-types --exclude-generics --public-only "$REST_MODELS")
    go-utils.find_all_types all_types "$REST_MODELS"
    rest_types_list=$(join_by "," ${all_types[*]})

    mapfile -t spec_rest_types < <(yq e '.components.schemas[] | select(.x-spec-rest-type == true) | key' $SPEC_PATH)
    spec_rest_types_list=$(join_by "," ${spec_rest_types[*]})

    # hack to get separate types generation
    sed "s/\$ref: '\#\//\$ref: '$SPEC_PATH\#\//g" $SPEC_PATH >"$paths_file"

    # yq e 'del(.components)' -i "$paths_file" # dont delete since recent oapi-codegen does some checks even if we are not generating types
    go build -o $BUILD_DIR/oapi-codegen cmd/oapi-codegen/main.go || mark_failed_tool_build # templates are embedded, required rebuild

    mapfile -t exclude_schemas_arr < <(yq e '.components.schemas[] | select(.x-oapi-ignore == true) | key' $SPEC_PATH)
    exclude_schemas=$(printf ",\"%s\"" "${exclude_schemas_arr[@]}")
    exclude_schemas="[${exclude_schemas:1}]"

    local models_config=$(
      cat <<EOF
package: models
generate:
  models: true
  embedded-spec: true
mode: models
output: internal/repos/postgresql/gen/models/openapi_types.gen.go
compatibility:
  always-prefix-enum-values: true
output-options:
  skip-prune: true
  exclude-schemas: $exclude_schemas
EOF
    )
    local server_config=$(
      cat <<EOF
package: rest
generate:
  gin-server: true
  strict-server: true
  embedded-spec: true
output: internal/rest/openapi_server.gen.go
mode: server
exclude-rest-types: true
skip-discriminator-utils: true
is-rest-server-gen: true
import-mapping:
  "openapi.yaml": github.com/danicc097/openapi-go-gin-postgres-sqlc/internal/repos/postgresql/gen/models
# additional-imports:
#   - alias: .
#     package: github.com/danicc097/openapi-go-gin-postgres-sqlc/internal/models
output-options:
  skip-prune: true
compatibility:
  always-prefix-enum-values: true
EOF
    )
    local test_client_config=$(
      cat <<EOF
package: resttesting
generate:
  client: true
output: internal/rest/resttesting/openapi_client_gen.go
test-client: true
additional-imports:
  - alias: .
    package: github.com/danicc097/openapi-go-gin-postgres-sqlc/internal/repos/postgresql/gen/models
  - alias: models
    package: github.com/danicc097/openapi-go-gin-postgres-sqlc/internal/repos/postgresql/gen/models
EOF
    )
    # IMPORTANT: additional-imports packages affect x-go extensions
    # see https://github.com/oapi-codegen/oapi-codegen?tab=readme-ov-file#openapi-extensions

    # TODO: we already have structs.gen.go indexed by "Models(models struct)" or plain rest type...
    # however this might be faster if the build is cached (templates unchanged). could construct internally as map set anyway
    oapi-codegen --config <(echo "$models_config") --types "$rest_types_list" "$SPEC_PATH" || err "Failed types generation" &
    oapi-codegen --config <(echo "$server_config") --spec-rest-types "$spec_rest_types_list" --types "$rest_types_list" "$paths_file" || err "Failed server generation" &
    oapi-codegen --config <(echo "$test_client_config") --types "$rest_types_list" --spec-rest-types "$spec_rest_types_list" "$SPEC_PATH" || err "Failed client generation" &
    # not used now. may be useful for a cli at some point
    # oapi-codegen --config internal/client/oapi-codegen-client.yaml "$SPEC_PATH" || err "Failed client generation" &

    wait_without_error

    # TODO: remove once x-go-type in use for all db types.
    # models gen cannot be done if we have x-go-type...
    # sed -i 's/ \*Models\([A-Z]\)/ \*db\.\1/g; s/\[\]Models\([A-Z]\)/\[\]db\.\1/g; s/ Models\([A-Z]\)/ db\.\1/g; s/ \(\(externalRef0\.\)\?Models\)\([A-Z]\)/ db\.\3/g' internal/rest/openapi_server.gen.go
    # sed -i 's/ \*Models\([A-Z]\)/ \*db\.\1/g; s/\[\]Models\([A-Z]\)/\[\]db\.\1/g; s/ Models\([A-Z]\)/ db\.\1/g; s/ \(\(externalRef0\.\)\?Models\)\([A-Z]\)/ db\.\3/g' internal/rest/resttesting/openapi_client_gen.go
    # db already imports models pkg
    # sed -i 's/ \*Models\([A-Z]\)/ \*db\.\1/g; s/\[\]Models\([A-Z]\)/\[\]db\.\1/g; s/ Models\([A-Z]\)/ db\.\1/g; s/ \(\(externalRef0\.\)\?Models\)\([A-Z]\)/ db\.\3/g' internal/repos/postgresql/gen/models/openapi_types.gen.go

    # ast-parser find-redeclared --delete "$REST_MODELS" # for duplicates in rest. depends on oapi-codegen output
    codegen implement-server # depends on oapi-codegen output

    files=$(find "internal/rest/" -name "api_*[^_test].go" -o -name "openapi_*.go")
    goimports -w $files
    gofumpt -w $files
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  xsetup.backup.cleanup
}
# Generate mocks for specified interfaces.
x.gen.counterfeiter() {
  # This shouldn't pose any problems, the interface is the only input to counterfeiter.
  { { {
    tfidfpb_dir="internal/pb/python-ml-app-protos/tfidf/v1"
    envvar="internal/envvar/envvar.go"
    repos="$REPOS_DIR/repos.go"
    tfidfpb="$tfidfpb_dir/service_grpc.pb.go"

    declare -A ifaces
    ifaces=(
      ["Provider-$envvar"]="internal/envvar/envvartesting/provider.gen.go"
      ["User-$repos"]="$REPOS_DIR/repostesting/user.gen.go"
      ["Notification-$repos"]="$REPOS_DIR/repostesting/notification.gen.go"
      ["Project-$repos"]="$REPOS_DIR/repostesting/project.gen.go"
      ["Team-$repos"]="$REPOS_DIR/repostesting/team.gen.go"
      ["MovieGenreClient-$tfidfpb"]="$tfidfpb_dir/v1testing/movie_genre_client.gen.go"
      ["MovieGenreServer-$tfidfpb"]="$tfidfpb_dir/v1testing/movie_genre_server.gen.go"
    )

    local cache="$GEN_CACHE/counterfeiter"
    local updated_ifaces=()

    mkdir -p "$cache"

    for key in ${!ifaces[@]}; do
      input_path="${key#*-}"
      iface="${key%%-*}"
      iface_content="$(go-utils.get_interface_methods $iface $input_path)"

      cache_entry_dir="$cache/$(echo $input_path | base64)" # so we can rm -rf $GEN_CACHE/*/$interface only when invalidating on failed gen
      mkdir -p "$cache_entry_dir"
      if diff "$cache_entry_dir/$iface" <(echo "$iface_content") &>/dev/null; then
        [[ $X_FORCE_REGEN -eq 0 ]] && continue
      fi

      counterfeiter -o "${ifaces[$key]}" "$input_path" "$iface" 2>&1 &
      echo "$iface_content" >"$cache_entry_dir/$iface"
      updated_ifaces+=("$key")
    done

    wait_without_error || err Failed jobs

    # counterfeiter is unaware of grpc's obscure mustEmbedUnimplemented***() for forward server compatibility
    if ! grep -q 'v1\.UnimplementedMovieGenreServer' $tfidfpb_dir/v1testing/movie_genre_server.gen.go; then
      sed -i '/type FakeMovieGenreServer struct {/a v1\.UnimplementedMovieGenreServer' $tfidfpb_dir/v1testing/movie_genre_server.gen.go
    fi

    if [[ ${#updated_ifaces[@]} -gt 0 ]]; then
      echo "Updated repo interfaces: ${updated_ifaces[*]}"
    fi

  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Generate the required servers/clients for relevant services.
x.gen.proto() {
  { { {
    local import_path="python-ml-app-protos/tfidf/v1"
    local filename="internal/python-ml-app-protos/tfidf/v1/service.proto"

    mkdir -p internal/pb
    # Plugins are no longer supported by protoc-gen-go.
    # Instead protoc-gen-go-grpc and the go package (in proto or via M flag) are required
    echo "Running generation"
    protoc \
      --go-grpc_out=internal/pb/. \
      --go_out=internal/pb/. \
      --go-grpc_opt=M${filename}=${import_path},paths=import \
      --go_opt=M${filename}=${import_path},paths=import \
      internal/python-ml-app-protos/tfidf/v1/service.proto || err "Failed proto generation"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Run frontend code generation.
x.gen.frontend() {
  xsetup.backup
  { { {
    export PATH=frontend/node_modules/.bin:$PATH

    local exploded_spec=openapi.exploded.yaml

    SCHEMA_OUT="frontend/src/types/schema.d.ts"
    local orval_config="frontend/orval.config.ts"
    deps=(
      "$orval_config"
      "frontend/src/react-query.ts"
      "frontend/src/react-query.default.ts"
      "frontend/scripts/"
      "frontend/package.json"
    )

    cache_all "$GEN_CACHE/frontend.md5" ${base_cache_deps[@]} $SPEC_PATH ${deps[@]} && return 0

    config_template_setup frontend # no need to run if cached .env

    mkdir -p frontend/src/types
    rm -rf $FRONTEND_GEN

    yq 'explode(.)' $SPEC_PATH >$exploded_spec # js-yaml wont support explicit tags - used by most gen libs

    {
      node frontend/scripts/generate-client-validator.js
      # client-validator will be useless with orval
      rm -f frontend/src/client-validator/gen/validate.ts
      find frontend/src/client-validator/gen/ -type f -exec \
        sed -i "s/from '.\/validate'/from '..\/validate'/g" {} \;
    } & # nothing depends on this yet

    {
      v="$(openapi-typescript --version)"
      openapi-typescript $exploded_spec --output "$SCHEMA_OUT" --path-params-as-types --prettier-config .prettierrc
      echo "/* Generated by openapi-typescript $v */
/* eslint-disable @typescript-eslint/ban-ts-comment */
/* eslint-disable */
// @ts-nocheck
export type schemas = components['schemas']
" | cat - "$SCHEMA_OUT" >/tmp/out && mv /tmp/out "$SCHEMA_OUT"
    } &

    {
      orval --config $orval_config
      orval_frontend_branded_types
    } &

    wait_without_error || err Failed jobs
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  xsetup.backup.cleanup
}

# Run e2e code generation.
x.gen.e2e() {
  xsetup.backup
  { { {
    source .env.e2e

    export PATH=e2e/node_modules/.bin:$PATH

    orval_config="e2e/orval.config.ts"

    cache_all "$GEN_CACHE/e2e.md5" ${base_cache_deps[@]} $SPEC_PATH $orval_config e2e/package.json && return 0

    config_template_setup e2e # no need to run if cached .env

    rm -rf e2e/client/gen

    orval --config $orval_config
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  xsetup.backup.cleanup
}

# Run all codegen and postgen commands for the project.
x.gen() {
  [[ -n $X_NO_GEN ]] && return
  xsetup.backup # Modification of vars inside would be local to subshell (caused by pipeline)
  xsetup.drop-and-migrate-gen-db
  xsetup.build-tools
  { { {
    echo "Running code generation"

    x.lint.sql

    go generate ./...

    x.gen.gowrap &
    x.gen.proto &
    x.gen.xo &
    x.gen.sqlc &
    x.gen.jet >/dev/null &
    generate_repo_constructor &

    wait_without_error || err Failed jobs

    {
      x.gen.pregen
      x.gen.client-server
    } &
    x.gen.counterfeiter & # delay since it depends on generated output (xo, proto...)

    wait_without_error || err Failed jobs

    x.gen.postgen

    # restart is not robust
    # vscode will randomly lose connection when restarting
    # for pid in $(pidof gopls); do
    #   restart_pid $pid &
    # done

    x.gen.frontend &
    x.gen.e2e &

    wait_without_error || err Failed jobs
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  xsetup.backup.cleanup
}

# Build code generation custom tools.
x.gen.build-tools() {
  { { {
    # openapi-go via codegen cli requires structs already compiled, but we will rebuild
    # right before codegen gen-schema since PublicStructs is not used anywhere else inside codegen
    # generate_structs_map

    out_dir=$BUILD_DIR

    mkdir -p $out_dir
    for cmd in jet oapi-codegen codegen; do
      echo "Building $cmd..."
      { go build -o $out_dir/$cmd cmd/$cmd/main.go || mark_failed_tool_build; } &
    done

    wait_without_error --no-kill || {
      mark_failed_tool_build
    }
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Lint the entire project.
x.lint() {
  { { {
    x.lint.sql &
    x.lint.go &
    x.lint.frontend &
    x.lint.shell &
    wait # wait_without_error -  don't care about errors yet
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Format Go files.
x.lint.go() {
  echo "Linting go"
  files=$(find . \
    -not -path "**/$PROTO_DIR/*" \
    -not -path "**/$PG_REPO_GEN/*" \
    -not -path "**/testdata/*" \
    -not -path "**/node_modules/*" \
    -not -path "**/.venv/*" \
    -not -path "**/*.cache/*" \
    -not -path "**/vendor/*" \
    -not -path "**/*.gen.*" \
    -not -path "**/*.xo.go" \
    -name "*.go")
  goimports -w $files || err "Linting failed"
  gofumpt -w $files || err "Linting failed"
  # recent releases possibly broke generated code detection
  # golangci-lint run --config=.golangci.yml --allow-parallel-runners --fast --fix &>/dev/null || true
}

exhaustruct_lint() {
  local lint_config=$(
    cat <<EOF
linters:
  disable-all: true
  enable:
    - exhaustruct
linters-settings:
  exhaustruct:
    # lint struct usage in all packages that use them
    include:
      - 'github\.com/danicc097/openapi-go-gin-postgres-sqlc/internal/rest\.DemoWorkItems$'
      # internal/rest may need to run this, alongside all services structs
      - 'github\.com/danicc097/openapi-go-gin-postgres-sqlc/internal/repos/postgresql/gen/models\..*'
      # these are all adapters from service to db, so make sure they're always updated
      - 'github\.com/danicc097/openapi-go-gin-postgres-sqlc/internal/repos\..*$'
    exclude:
      - 'github\.com/danicc097/openapi-go-gin-postgres-sqlc/internal/repos/postgresql/gen/models\..*(UpdateParams|Joins)$'
      - 'github\.com/danicc097/openapi-go-gin-postgres-sqlc/internal/repos/postgresql/gen/models\.(HTTPError|ValidationError)$'
# in case we want to run a linter on test files only
# issues:
#   exclude-rules:
#     - path-except: _test\.go
#       linters:
#         - exhaustruct
EOF
  )

  # postgresqlrandom: ideally tests in rest, service and repo  use Random<...>CreateParams directly or indirectly
  #                     via fixture factories, so we always want these up to date when a new field is added.
  # services: may construct db CreateParams from specific service models
  # repos: ensure all repos initialized (TODO: same for services)
  # TODO: rest should force exhaust on services, rest and db structs.
  local packages=("
  internal/services
  internal/repos/postgresql/postgresqlrandom
  internal/rest
  internal/repos
  ")

  for package in ${packages[@]}; do
    LOG_LEVEL=error golangci-lint run --allow-parallel-runners --config <(echo "$lint_config") $package &
  done

  lint_config=$(
    cat <<EOF
linters:
  disable-all: true
  enable:
    - exhaustruct
linters-settings:
  exhaustruct:
    include:
      - 'github\.com/danicc097/openapi-go-gin-postgres-sqlc/internal/repos\.Repos$'
      - 'github\.com/danicc097/openapi-go-gin-postgres-sqlc/internal/services\.Services$'
EOF
  )
  LOG_LEVEL=error golangci-lint run --config <(echo "$lint_config") & # ensure repo initialization is complete on all pkgs

  wait_without_error
}

# Format frontend files.
x.lint.frontend() {
  echo "Linting frontend"
  cd frontend
  pnpm run lint:fix
  echo "Success"
}

# Format shell files.
x.lint.shell() {
  shfmt -l -i 2 -w bin/*
}

# Format SQL files.
x.lint.sql() {
  echo "Linting SQL"
  SQL_DIRS=(
    "$REPOS_DIR"
    "db"
  )
  for slq_dir in ${SQL_DIRS[@]}; do
    pg_format --config .pg_format $(find "$slq_dir" -name '*.*sql' -not -path "db/schema.sql") &
  done

  wait_without_error
}

# Run required backend pre-test setup: services, database cleanup, codegen...
# Can be called independently, e.g. before running tests through an IDE.
x.test.backend.setup() {
  xsetup.backup # Modification of vars inside would be local to subshell (caused by pipeline)
  { { {
    # NOTE: tests run independently in Go so we can't have a function be called and run
    # only once before any test starts
    run_shared_services up -d --build --remove-orphans --force-recreate --wait
    # no need to migrate, done on every test run internally
    docker.postgres.drop_and_recreate_db $POSTGRES_TEST_DB
    x.gen
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  xsetup.backup.cleanup
}

# Test backend. Accepts `go test` parameters.
# Args: [...]
x.test.backend() {
  xsetup.backup
  { { {
    exhaustruct_lint

    x.test.xo

    # done per test suite database automatically
    # yes y 2>/dev/null | POSTGRES_DB=$POSTGRES_TEST_DB x.migrate down || true # nochange exits

    go_test -tags 'skip_xo' "$@" ./...
    # go_test -tags 'skip_xo,!skip_countone' "$@" -count=1 ./...
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  xsetup.backup.cleanup
}

# Build frontend.
# Args: [...]
x.build.frontend() {
  config_template_setup frontend

  cd frontend
  pnpm run build
}

# Test frontend. Accepts `vitest` parameters.
# Args: [...]
x.test.frontend() {
  # TODO accept vitest args
  config_template_setup frontend

  cd frontend
  pnpm run test:no-watch "$@"
  # pnpm run test-types:no-watch
}

# Test frontend on file changes. Accepts `vitest` parameters.
# Args: [...]
x.test.frontend.watch() {
  config_template_setup frontend

  cd frontend
  pnpm run test "$@"
}

# Run custom xo generation script tests. Accepts `go test` parameters.
# To fix tests, run with --x-no-backup.
# Args: [...]
x.test.xo() {
  xsetup.backup
  { { {
    POSTGRES_DB="xo_tests_db"
    GEN_POSTGRES_DB="xo_tests_db"

    cache_all "$GEN_CACHE/test-xo.md5" ${base_cache_deps[@]} bin/.helpers.sh $XO_TEMPLATES_DIR/ \
      --exclude "$XO_TEMPLATES_DIR/tests/got/*" \
      --exclude "$XO_TEMPLATES_DIR/tests/snapshot/*" && return 0

    docker.postgres.drop_and_recreate_db $POSTGRES_DB

    echo "Running xo template tests"

    test_dir="$XO_TEMPLATES_DIR/tests"
    docker.postgres.psql -d $POSTGRES_DB <"$test_dir/xotests_schema.sql" # need correct schema for xo gen
    echo "Schema loaded"

    rm -rf "$test_dir/got"
    mkdir -p "$test_dir/got"

    echo Generating xo code...

    {
      xo_schema -o "$test_dir/got" --debug \
        --ignore "*.created_at" \
        --ignore "*.updated_at" || err "Failed xo xo_tests schema generation" &

      xo_schema -o "$test_dir/got" --debug \
        --schema xo_tests \
        --ignore "*.created_at" \
        --ignore "*.updated_at" || err "Failed xo xo_tests schema generation" &

      xo_schema -o "$test_dir/got" --debug \
        --schema xo_tests_cache \
        --ignore "*.created_at" \
        --ignore "*.updated_at" || err "Failed xo xo_tests schema generation" &
    }

    wait_without_error

    # workaround after merging of models and db types so we can independently test.
    # shellcheck disable=SC2016
    echo '
package got

type Direction string
const (
	DirectionAsc  Direction = "asc"
	DirectionDesc Direction = "desc"
)

type PaginationCursor struct {
	Column    string    `json:"column"`
	Direction Direction `json:"direction"`
	Value *interface{} `json:"value"`
}
' >"$test_dir/got/models.gen.go"

    files=$(find "$test_dir/got" \
      -name "*.go")
    goimports -w $files
    gofumpt -w $files

    go_test -count=1 "$XO_TESTS_PKG" "$@" ||
      err "xo tests failed"

    if ! $test_dir/diff_check; then
      with_tty confirm "Do you want to update test snapshot with current changes?" && { # must redirect inside xlog pipeline
        rm -rf "$test_dir/snapshot"
        cp -r "$test_dir/got" "$test_dir/snapshot"
      }
    fi
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  xsetup.backup.cleanup
}

test_backend_watch() {
  clear

  local tags='skip_xo,skip_countone'
  echo "Waiting for changes..."
  echo "Default test tags: $tags"
  echo "Test args: $*"

  trap 'exit' SIGINT # overrides exit-cleanup

  while true; do
    inotifywait \
      --recursive \
      -qq \
      --event=close_write \
      --format='%T %f' \
      --timefmt='%s' \
      . && clear &&
      { go_test -tags $tags "$@" && echo "${GREEN}✓ All tests passing${OFF}"; } || echo "${RED}X Tests failed${OFF}"
  done
}

# Test backend on file changes. Accepts `go test` parameters.
# Args: [...]
x.test.backend.watch() {
  test_backend_watch "$@" ./...
}

# Watch specific functions in a given dir. Accepts `go test` parameters.
# Example: internal/repos/postgresql/ TestUser_Create TestUser_Update -count=5
# Example: internal/repos/postgresql/ "TestUser_.*"
# Args: dir [fn ...]
x.test.backend.watch-for() {
  local dir="$1"
  local tests=()
  local gotest_args=()

  for arg in "${@:2}"; do
    if [[ $arg == Test* ]]; then
      tests+=("$arg")
    else
      gotest_args+=("$arg")
    fi
  done

  local fns=$(
    IFS='|'
    echo "${tests[*]}"
  )

  test_backend_watch -run "^($fns)$" "$GOMOD_PKG/$dir" "${gotest_args[@]}"
}

pre_build_cmd="project setup-swagger-ui"

# Run backend.
x.run.backend() {
  $pre_build_cmd
  go run ./cmd/rest-server/main.go -env=.env.$X_ENV
}

# Run backend with hot-reloading.
x.run.backend-hr() {
  # TODO replace healthcheck with adhoc calls and bring services up in btaches
  # to prevent either bombarding with req or having to wait too long at startup.
  # see https://github.com/moby/moby/issues/33410

  # https://github.com/air-verse/air/blob/master/air_example.toml
  # NOTE: building binary unreliable sometimes, leads to bin not found.
  local build_cmd="go build -o /mnt/ramdisk/rest-server ./cmd/rest-server/main.go"
  local bin_cmd="/mnt/ramdisk/rest-server -env=.env.$X_ENV"
  if ! test -d /mnt/ramdisk; then # setup with sudo mkdir -p /mnt/ramdisk; sudo mount -t tmpfs -o size=100M tmpfs /mnt/ramdisk
    build_cmd=""
    bin_cmd="go run ./cmd/rest-server/main.go -env=.env.$X_ENV"
  fi
  echo "Running $bin_cmd"

  air \
    --build.pre_cmd "$pre_build_cmd" \
    --build.cmd "$build_cmd" \
    --build.bin "$bin_cmd" \
    --build.include_ext "go,work,mod" \
    --build.include_file "openapi.yaml" \
    --build.exclude_regex ".gen.go,_test.go" \
    --build.exclude_dir ".git,tmp,$PROTO_DIR,$PG_REPO_GEN,**/testdata,vendor,frontend,external,*.cache,$GEN_CACHE,$TOOLS_DIR,internal/static/swagger-ui" \
    --build.delay 1000 \
    --build.exclude_unchanged "true" |
    sed -e "s/^/${BLUE}[Air]${OFF} /"
}

# Run frontend with hot-reloading.
x.run.frontend() {
  config_template_setup frontend
  cd frontend
  pnpm run dev |
    sed -e "s/^/${GREEN}[Vite]${OFF} /"
}

# Run all project services with hot reload enabled in dev mode.
x.run.all() {
  run_hot_reload

  while true; do
    sleep 5
  done

  # TODO fix won't kill children
  # next_allowed_run=$(date +%s)
  # latency=3
  # close_write event, else duplicated, tripl. events -> race condition
  # while true; do
  #   inotifywait \
  #     --monitor $SPEC_PATH \
  #     --event=close_write \
  #     --format='%T %f' \
  #     --timefmt='%s' |
  #     while read -r event_time event_file 2>/dev/null || sleep $latency; do
  #       if [[ $event_time -ge $next_allowed_run ]]; then
  #         next_allowed_run=$(date --date="${latency}sec" +%s)

  #         kill_descendants || true

  #         run_hot_reload
  #       fi
  #     done
  # done
}

# Syncs backend dependencies.
x.backend.sync-deps() {
  GOWORK=off go mod tidy
  GOWORK=off go mod vendor
}

# Run project in production mode, i.e. dockerized and bundled.
x.run-dockerized() {
  # project run-dockerized --x-env=prod --x-no-gen
  run_shared_services up -d --build --wait
  x.db.recreate

  # TODO: migrations:
  # 1. Prevent insert/update/delete
  # 2. Backup
  # 3. Run up migrations
  # 5. Allow insert/update/delete again

  # x.gen

  x.setup-swagger-ui

  DOCKER_BUILDKIT=1 BUILDKIT_PROGRESS=plain docker compose \
    --project-name "$PROJECT_PREFIX"_"$X_ENV" \
    -f docker-compose.yml \
    --env-file ".env.$X_ENV" \
    up -d --build --wait --force-recreate 2>&1 # https://github.com/docker/compose/issues/7346

  x.migrate up
}

# Remove running project containers, including shared ones between environments.
x.stop-project() {
  { { {
    run_shared_services down --remove-orphans
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Recreates docker volumes for Postgres, Redis, etc. Unsaved data will be lost.
x.recreate-shared-services() {
  run_shared_services up -d --build --force-recreate --wait
}

# Checks before release:
# - Magic keyword "STOPSHIP" not found in tracked files.
x.release() {
  { { {
    search_stopship "STOPSHIP" &
    GOWORK=off go mod verify & # (https://github.com/golang/go/issues/54372)

    wait_without_error || err Failed jobs
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Runs the go type checker.
x.backend.typecheck() {
  golangci-lint run --no-config --disable-all --enable=typecheck --allow-parallel-runners --timeout=10m
}

# depends on gen.xo output
# still no branded types in orval: https://github.com/orval-labs/orval/issues/1222
orval_frontend_branded_types() {
  local import_string="import type * as EntityIDs from 'src/gen/entity-ids'"
  local escaped_import_string=$(escape_sed "$import_string")

  local import_command=("-e" "/$escaped_import_string/!{1s/^/$escaped_import_string\n/}")
  local replace_commands=()

  local int_ids=()
  go-utils.find_db_ids_int int_ids "$PG_REPO_GEN/models"
  for entity in "${int_ids[@]}"; do
    local camel="" pascal=""
    to_camel camel $entity
    to_pascal pascal $entity
    replace_commands+=(
      "-e" "s/${camel}?: number/${camel}?: EntityIDs.${pascal}/g"
      "-e" "s/${camel}: number/${camel}: EntityIDs.${pascal}/g"
      "-e" "s/${camel}: faker.number.int({min: undefined, max: undefined})/${camel}: faker.number.int({min: undefined, max: undefined}) as EntityIDs.${pascal}/g"
      "-e" "s/${camel}: faker.number.int({min: undefined, max: undefined})/${camel}: faker.number.int({min: undefined, max: undefined}) as EntityIDs.${pascal}/g"
      "-e" "s/${camel}:[[:space:]]*faker\.helpers\.arrayElement(\[faker\.number\.int({min:[[:space:]]*undefined,[[:space:]]*max:[[:space:]]*undefined}),[[:space:]]*null\])/${camel}: faker.helpers.arrayElement([faker.number.int({min: undefined, max: undefined}), null]) as EntityIDs.${pascal} | null/g"
    )

  done

  local uuid_ids=()
  go-utils.find_db_ids_uuid uuid_ids "$PG_REPO_GEN/models"
  for entity in "${uuid_ids[@]}"; do
    local camel="" pascal=""
    to_camel camel $entity
    to_pascal pascal $entity
    replace_commands+=(
      "-e" "s/${camel}?: Models${pascal}/${camel}?: EntityIDs.${pascal}/g"
      "-e" "s/${camel}: Models${pascal}/${camel}: EntityIDs.${pascal}/g"
      "-e" "s/${camel}: faker.word.sample()/${camel}: faker.string.uuid() as EntityIDs.${pascal}/g"
      "-e" "s/${camel}: faker.string.uuid()/${camel}: faker.string.uuid() as EntityIDs.${pascal}/g"
    )
  done

  local sed_commands=("${import_command[@]}" "${replace_commands[@]}")

  find $FRONTEND_GEN -type f -name '*.ts' -exec sed -i "${sed_commands[@]}" {} +

  local exports=()
  for entity in "${int_ids[@]}"; do
    local camel="" pascal=""
    to_camel camel $entity
    to_pascal pascal $entity
    exports+=("export type ${pascal} = Branded<number, '${camel}'>") # swaggest requires pointer to struct
  done
  for entity in "${uuid_ids[@]}"; do
    local camel="" pascal=""
    to_camel camel $entity
    to_pascal pascal $entity
    exports+=("export type ${pascal} = Branded<string, '${camel}'>") # swaggest requires pointer to struct
  done
  out="$FRONTEND_GEN/entity-ids.ts"
  cat <<EOF >$out
import { Branded } from 'src/types/utils'

$(printf "%s\n" "${exports[@]}")
EOF

  # NOTE: disabled prettier and mock gen, not worth the time it takes.
  # cheapest way to update both new and previous files that need formatting
  # ignore errors since files may have been deleted
  # { git diff --name-only $FRONTEND_GEN | xargs --no-run-if-empty prettier --write -- &>/dev/null; } || true
}

# Creates a generic CRUD endpoint in the OpenAPI spec and generates related files for a
# given db table by struct name.
# Accepts flags:
# --project - Generate as entity under a parameterized project.
# Args: name
x.dev-utils.create-crud() {
  xsetup.backup
  { { {
    # TODO: restore endpoint if has_deleted_at

    load_exported_crud_gen_vars ${@}

    for arg in "${@:2}"; do
      case $arg in
      --testing)
        export testing=1
        ;;
      *) ;;
      esac
    done

    # missing schemas will be autogenerated since they exist in rest/models.spec.go
    local new_crud_schema="$(bash "$BIN_TEMPLATES_DIR"/crud-openapi.yaml.tmpl.bash)"
    yq eval-all "(
        select(fi == 1).paths
        ) as \$paths
        | (
          select(fi == 1).components.schemas
        ) as \$schemas
        | select(fi == 0)
        | .paths += \$paths
        " "$SPEC_PATH" <(echo "$new_crud_schema") | sponge "$SPEC_PATH"

    echo "
type ${pascal_name}Response struct {
	models.${pascal_name}
}

type Create${pascal_name}Request struct {
	models.${pascal_name}CreateParams
}

type Update${pascal_name}Request struct {
	models.${pascal_name}UpdateParams
}
" >>"$REST_MODELS"

    echo "package rest" >"internal/rest/api_${snake_name}.go" # required to prevent codegen failing. Will be replaced afterwards

    yq eval-all \
      ". += {
    \"${kebab_name}:create\": {
      \"description\": \"Can create ${sentence_name}s.\"
    },
    \"${kebab_name}:edit\": {
      \"description\": \"Can edit ${sentence_name}s.\"
    },
    \"${kebab_name}:delete\": {
      \"description\": \"Can delete ${sentence_name}s.\"
    }
  }" -i -o=json "$SCOPE_POLICY_PATH"

    update_roles_and_scopes

    local file="$PG_REPO_DIR/repo_${snake_name}.go"
    test -e "$file" && err "File $file exists. Please rename to avoid overwrite"
    bash "$BIN_TEMPLATES_DIR"/crud-repo.go.tmpl.bash >"$file" && gofumpt -w $file
    file="internal/services/service_${snake_name}.go"
    test -e "$file" && err "File $file exists. Please rename to avoid overwrite"
    bash "$BIN_TEMPLATES_DIR"/crud-service.go.tmpl.bash >"$file" && gofumpt -w $file
    file="$PG_REPO_DIR/postgresqlrandom/${snake_name}.go"
    test -e "$file" && err "File $file exists. Please rename to avoid overwrite"
    bash "$BIN_TEMPLATES_DIR"/crud-postgresqlrandom.go.tmpl.bash >"$file" && gofumpt -w $file
    file="internal/services/servicetestutil/${snake_name}.go"
    test -e "$file" && err "File $file exists. Please rename to avoid overwrite"
    bash "$BIN_TEMPLATES_DIR"/crud-servicetestutil.go.tmpl.bash >"$file" && gofumpt -w $file

    echo "
// ${pascal_name} defines the datastore/repository handling persisting ${sentence_name} records.
type ${pascal_name} interface {
	ByID(ctx context.Context, d models.DBTX, id models.${pascal_name}ID, opts ...models.${pascal_name}SelectConfigOption) (*models.${pascal_name}, error)
	Create(ctx context.Context, d models.DBTX, params *models.${pascal_name}CreateParams) (*models.${pascal_name}, error)
	Update(ctx context.Context, d models.DBTX, id models.${pascal_name}ID, params *models.${pascal_name}UpdateParams) (*models.${pascal_name}, error)
	Delete(ctx context.Context, d models.DBTX, id models.${pascal_name}ID) (*models.${pascal_name}, error)
}
" >>"$REPOS_DIR/repos.go"

    local services_path="internal/services/services.go"
    local line=$(awk '/return &Services/{print NR}' "$services_path")
    test -z line && err "No Services line found"
    sed -i "$((line - 2))a\\
${lower_name}svc := New${pascal_name}(logger, repos)" "$services_path"
    # 1 new line was just prepended
    sed -i "$((line + 1))a\\
${pascal_name}: ${lower_name}svc," "$services_path"

    local line=$(awk '/type Services struct {/{print NR}' "$services_path")
    test -z line && err "No Services line found"
    sed -i "$((line))a\\
${pascal_name} *${pascal_name}" "$services_path"

    local service_repos_path="internal/services/repos.go"
    local line=$(awk '/return &repos.Repos/{print NR}' "$service_repos_path")
    test -z line && err "No repos.Repos line found"
    local string="${lower_name}repo := reposwrappers.New${pascal_name}WithTracing(
		reposwrappers.New${pascal_name}WithTimeout(
			postgresql.New${pascal_name}(),
			reposwrappers.${pascal_name}WithTimeoutConfig{},
		),
		postgresql.OtelName,
		nil,
	)"
    modified_string="${string//$'\n'/}"
    newline_count=$((${#string} - ${#modified_string}))
    formatted_string="${string//$'\n'/\\n}" # for sed

    sed -i "$((line - 2))a\\
$formatted_string" "$service_repos_path"
    # 1 new line was just prepended
    sed -i "$((line + newline_count + 1))a\\
${pascal_name}: ${lower_name}repo," "$service_repos_path"

    generate_repo_constructor
    x.gen.gowrap $pascal_name

    test -n "$testing" && return 0 # will gen and run tests afterwards

    if gofumpt -w "$services_path" "$service_repos_path" && x.backend.typecheck; then
      x.gen
      x.dev-utils.create-crud-after-gen ${@}
    else
      err "Typecheck failed after CRUD gen. To debug:
      1. Commit current changes
      2. Run $CMD again with --x-no-backup flag
      3. Fix typecheck errors manually
      4. Run 'project gen', fix errors if any
      5. Run 'project dev-utils.create-crud-after-gen $*'"
    fi

  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  xsetup.backup.cleanup
}

load_exported_crud_gen_vars() {
  local name="$1"
  test -z "$name" && err "A db table struct name is required"

  for arg in "${@:2}"; do
    case $arg in
    --project)
      # For entities specific to a project, ie with project_id FK referencing projects.project_id
      # Could autogenerate directly without arg passed, but just be explicit
      export with_project=1
      ;;
    *) ;;
    esac
  done

  camel_name="" pascal_name=""
  to_camel camel_name $name
  to_pascal pascal_name $name
  export camel_name pascal_name

  export kebab_name="$(to_kebab $name)"
  export sentence_name="$(to_lower_sentence $name)"
  export snake_name="$(to_snake $name)"
  export lower_name="$(to_lower $name)"
  _db_struct_fields=()
  go-utils.struct_fields "$pascal_name" "$PG_REPO_GEN/models/$lower_name.xo.go" _db_struct_fields
  if [[ " ${_db_struct_fields[*]} " =~ " DeletedAt " ]]; then
    export has_deleted_at=1
  fi
  db_struct_fields="${_db_struct_fields[*]}"
  export db_struct_fields
  _db_create_params_struct_fields=()
  go-utils.struct_fields "${pascal_name}CreateParams" "$PG_REPO_GEN/models/$lower_name.xo.go" _db_create_params_struct_fields
  db_create_params_struct_fields="${_db_create_params_struct_fields[*]}"
  export db_create_params_struct_fields
  _db_update_params_struct_fields=()
  go-utils.struct_fields "${pascal_name}UpdateParams" "$PG_REPO_GEN/models/$lower_name.xo.go" _db_update_params_struct_fields
  db_update_params_struct_fields="${_db_update_params_struct_fields[*]}"
  export db_update_params_struct_fields
}

dummy() {
  xsetup.backup
  { { {
    echo Dummy function with xsetup branch backup
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  xsetup.backup.cleanup
}

# Test CRUD+tests generator. Accepts `go test` parameters.
# Args: [...]
x.test.create-crud-gen() {
  xsetup.backup
  { { {
    # IMPORTANT: we rely on cleanup based on xsetup forced exit, so do not call this
    # function along any others, only as a standalone command `project test.create-crud-gen`
    local migration_name="$(uuidgen)_create_crud_gen"
    x.migrate.create "$migration_name"
    local migration_file=$(find $MIGRATIONS_DIR -name "*${migration_name}.up.sql")
    test -e $migration_file
    cp $BIN_TEMPLATES_DIR/testdata/crud-schema.sql $migration_file

    x.gen.xo

    x.dev-utils.create-crud TestCrudGenBase --testing
    x.dev-utils.create-crud TestCrudGenDeletedAt --testing
    x.dev-utils.create-crud TestCrudGenDeletedAtProject --project --testing

    x.gen

    x.dev-utils.create-crud-after-gen TestCrudGenBase
    x.dev-utils.create-crud-after-gen TestCrudGenDeletedAt
    x.dev-utils.create-crud-after-gen TestCrudGenDeletedAtProject --project

    local names=("TestCrudGenBase" "TestCrudGenDeletedAt" "TestCrudGenDeletedAtProject")
    for name in ${names[@]}; do
      local snake_name="$(to_snake $name)"
      local pg_random_file="$PG_REPO_DIR/postgresqlrandom/${snake_name}.go"
      local line=$(awk "/return &models.${name}CreateParams{/{print NR}" "$pg_random_file")
      test -z line && err "No line found"
      sed -i "$((line))a\\
    Message: \"Some message\"," $pg_random_file
    done

    x.test.backend -count=1 "$@"

    if [[ $X_ENV != "ci" ]]; then # always restore branch if we're running locally
      echo "${GREEN}✅ Tests passing. Forcing exit 1 to restore previous branch.${OFF}"
      exit 1
    fi
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  xsetup.backup.cleanup
}

# Create CRUD gen tests manually. Use only when prompted to do so.
x.dev-utils.create-crud-after-gen() {
  { { {
    load_exported_crud_gen_vars ${@}

    local file="$PG_REPO_DIR/new_random_test.go"
    nr_create_params=""
    nr_create_args=""

    if [[ -n "$with_project" ]]; then
      nr_create_params=", projectID models.ProjectID"
      nr_create_args="projectID"
    fi

    echo "
func newRandom${pascal_name}(t *testing.T, d models.DBTX $nr_create_params) *models.${pascal_name} {
	t.Helper()

	${camel_name}Repo := reposwrappers.New${pascal_name}WithRetry(postgresql.New${pascal_name}(), testutil.NewLogger(t), 3, 200*time.Millisecond)

	cp := postgresqlrandom.${pascal_name}CreateParams($nr_create_args)

	${camel_name}, err := ${camel_name}Repo.Create(context.Background(), d, cp)
	require.NoError(t, err, \"failed to create random entity\") // IMPORTANT: must fail. If testing actual failures use random create params instead

	return ${camel_name}
}
    " >>"$file" && gofumpt -w $file

    local file="$PG_REPO_DIR/repo_${snake_name}_test.go"
    test -e "$file" && err "File $file exists. Please rename to avoid overwrite"
    bash "$BIN_TEMPLATES_DIR"/crud-repo-tests.go.tmpl.bash >"$file" && gofumpt -w $file
    local file="internal/services/service_${snake_name}_test.go"
    test -e "$file" && err "File $file exists. Please rename to avoid overwrite"
    bash "$BIN_TEMPLATES_DIR"/crud-service-tests.go.tmpl.bash >"$file" && gofumpt -w $file
    local file="internal/rest/api_${snake_name}_test.go"
    test -e "$file" && err "File $file exists. Please rename to avoid overwrite"
    bash "$BIN_TEMPLATES_DIR"/crud-api-tests.go.tmpl.bash >"$file" && gofumpt -w $file

    # now that generated api models exist at this point (backend.typecheck would have failed)
    file="internal/rest/api_${snake_name}.go"
    rm -f "$file"
    test -e "$file" && err "File $file exists. Please rename to avoid overwrite"
    bash "$BIN_TEMPLATES_DIR"/crud-api.go.tmpl.bash >"$file" && gofumpt -w $file

  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Shows existing user api keys.
x.dev-utils.api-keys() {
  docker.postgres psql --no-psqlrc -d $POSTGRES_DB -c "select email, api_key from user_api_keys left join users using (user_id);"
}

# Shows current database column comments used in xo codegen
# to ease further updates to comments in migrations.
x.dev-utils.show-column-comments() {
  local query="SELECT DISTINCT
  c.relname || '.' || a.attname AS column,
  COALESCE(col_description(format('%s.%s', n.nspname, c.relname)::regclass::oid, isc.ordinal_position), '') as column_comment
FROM pg_attribute a
  JOIN ONLY pg_class c ON c.oid = a.attrelid
  JOIN ONLY pg_namespace n ON n.oid = c.relnamespace
  INNER JOIN information_schema.columns as isc on c.relname = isc.table_name and isc.column_name = a.attname
  LEFT JOIN pg_constraint ct ON ct.conrelid = c.oid
    AND a.attnum = ANY(ct.conkey)
    AND ct.contype = 'p'
  LEFT JOIN pg_attrdef ad ON ad.adrelid = c.oid
    AND ad.adnum = a.attnum
WHERE a.attisdropped = false
  AND n.nspname = 'public'
  AND (true OR a.attnum > 0)
  AND col_description(format('%s.%s', n.nspname, c.relname)::regclass::oid, isc.ordinal_position) is not null;"

  docker.postgres psql --no-psqlrc -d $GEN_POSTGRES_DB -c "$query" 2>/dev/null
}

# Shows current database table comments used in xo codegen
# to ease further updates to comments in migrations.
x.dev-utils.show-table-comments() {
  local query="SELECT
  c.relname AS table,
  COALESCE(obj_description(c.oid, 'pg_class'), '') AS table_comment
FROM pg_class c
  JOIN pg_namespace n ON n.oid = c.relnamespace
WHERE c.relkind = 'r'
  AND n.nspname = 'public'
  AND obj_description(c.oid, 'pg_class') IS NOT NULL;"

  docker.postgres psql --no-psqlrc -d $GEN_POSTGRES_DB -c "$query" 2>/dev/null
}

# Setups a traefik container with predefined configuration in `install-dir`.
# Args: install-dir
x.setup.traefik() {
  { { {
    test -z "$1" && err "installation directory is required"

    x.setup.mkcert

    git clone --depth=1 https://github.com/danicc097/traefik-bootstrap.git "$1" || confirm "$1 already exists. Continue?"
    docker network create traefik-net || true
    mkdir -p "$1"/traefik/certificates
    cp $CERTIFICATES_DIR/* "$1"/traefik/certificates
    cd "$1" || exit
    cp traefik/dynamic_conf.yaml.example traefik/dynamic_conf.yaml
    echo "Adding $PWD/certificates/"
    yq e ".tls.certificates += [{
    \"certFile\": \"$PWD/$CERTIFICATES_DIR/localhost.pem\",
    \"keyFile\": \"$PWD/$CERTIFICATES_DIR/localhost-key.pem\"
  }]" -i traefik/dynamic_conf.yaml

    ./compose-up
    cd - >/dev/null || exit
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Installs mkcert local development certificates.
x.setup.mkcert() {
  { { {
    cd "$CERTIFICATES_DIR" || exit
    echo "Setting up local certificates"
    mkcert --cert-file localhost.pem --key-file localhost-key.pem "localhost" "*.e2e.localhost" "*.local.localhost" "*.dev.localhost" "*.ci.localhost" "*.prod.localhost" "127.0.0.1" "::1" "host.docker.internal" 2>&1
    cd -
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

########################## migrations ##########################

migrate_post() {
  migrate \
    -path $POST_MIGRATIONS_DIR/ \
    -database "postgres://$POSTGRES_USER:$POSTGRES_PASSWORD@localhost:$EXPOSED_POSTGRES_PORT/$POSTGRES_DB?sslmode=disable&x-migrations-table=$POST_MIGRATIONS_TABLE" $@
}

# Wrapper for golang-migrate with predefined configuration.
x.migrate() {
  { { {
    migrate \
      -path $MIGRATIONS_DIR/ \
      -database "postgres://$POSTGRES_USER:$POSTGRES_PASSWORD@localhost:$EXPOSED_POSTGRES_PORT/$POSTGRES_DB?sslmode=disable" \
      "$@" 2>&1

    if [[ "${*:1}" =~ (up|down)+ ]]; then # don't want to pass over create, etc.
      echo "Running post-migrations"
      if [[ "${*:1}" =~ (down)+ ]]; then
        migrate_post force 1 2>&1 # no down revisions. Post migrations should be idempotent
      else
        migrate_post $@ 2>&1
      fi
    fi
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Create a new migration file with the given `name`.
# Args: name
x.migrate.create() {
  { { {
    tmp="$*"
    tmp="${tmp// /_}"
    name="${tmp,,}"
    test -z $name && err "Please provide a migration name"
    x.migrate create -ext sql -dir $MIGRATIONS_DIR/ -seq -digits 7 "$name"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

########################## db ##########################

# Connects to the database container.
x.db.bash() {
  docker exec -it postgres_db_"$PROJECT_PREFIX" bash
}

# psql session for the current environment.
x.db.psql() {
  x.db.psql-with-db $POSTGRES_DB
}

# psql session for `database`.
# Args: db
x.db.psql-with-db() {
  docker exec -it postgres_db_"$PROJECT_PREFIX" psql -d $1
}

# Show active and max number of connections for the current environment.
x.db.conns() {
  x.db.conns-db $POSTGRES_DB
}

# Poll db connections in database.
# Args: db
x.db.conns.watch-db() {
  trap 'exit' SIGINT # overrides exit-cleanup

  while true; do
    out=$(x.db.conns-db $1 || echo "${RED}Connection error ($1)${OFF}")
    clear
    echo $out
    sleep 0.2
  done
}

# Show active and max number of connections for `database`.
# Args: db
x.db.conns-db() {
  { { {
    current_conns=$(docker.postgres.psql -d $1 -c "SELECT count(*) FROM pg_stat_activity WHERE datname = '$1';")
    max_conns=$(docker.postgres.psql -d $1 -c "SHOW max_connections;")
    echo "$current_conns/$max_conns active connections in '$1'"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}
# Create a new database in the current environment if it doesn't exist
# and stops its running processes if any.
x.db.recreate() {
  { { {
    docker.postgres.create_db $POSTGRES_DB
    docker.postgres.stop_db_processes $POSTGRES_DB
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Drop and recreate the database in the current environment.
x.db.drop() {
  [[ $X_ENV = "prod" && "$POSTGRES_DB" != "$GEN_POSTGRES_DB" ]] && with_tty confirm "This will drop production database data. Continue?"
  { { {
    docker.postgres.drop_and_recreate_db "$POSTGRES_DB"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

export_gen_db_schema() {
  docker.postgres bash -c "pg_dump $GEN_POSTGRES_DB --column-inserts --exclude-table $MIGRATIONS_TABLE --exclude-table $POST_MIGRATIONS_TABLE" | sed -e '/^--/d' >"$GEN_SCHEMA_PATH"
}

# Drop and recreate the database used for code generation up to N, N-1 revisions or none.
# Args: mode
x.db.gen() {
  { { {
    latest_rev=$(find $MIGRATIONS_DIR/*.sql -maxdepth 0 | wc -l)
    second_latest_rev=$(((latest_rev - 2) / 2)) # up+down
    POSTGRES_DB=$GEN_POSTGRES_DB x.db.drop
    case $1 in
    up)
      POSTGRES_DB=$GEN_POSTGRES_DB x.migrate up
      ;;
    up-1)
      echo "Migrating up to penultimate revision..."
      POSTGRES_DB=$GEN_POSTGRES_DB x.migrate up $second_latest_rev
      ;;
    drop) ;;
    *)
      err "Valid options: \"$(join_by , ${X_FUNC_DB_GEN__COMP})\" but got: \"$1\""
      ;;
    esac
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Seed gen database.
x.db.gen.initial-data() {
  { { {
    POSTGRES_DB=$GEN_POSTGRES_DB x.db.initial-data
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Seed database.
x.db.initial-data() {
  { { {
    x.db.drop
    x.migrate up
    echo "Loading initial data to $POSTGRES_DB"
    # docker.postgres.psql -d $POSTGRES_DB <"./db/initial_data_$X_ENV.pgsql"
    go run cmd/initial-data/main.go -env .env.$X_ENV
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Backup the database for the current environment.
x.db.dump() {
  { { {
    local dump_prefix="dump_${X_ENV}_"
    running_dumps=$(docker.postgres.psql -P pager=off -U postgres -d "postgres_$X_ENV" \
      -c "SELECT pid FROM pg_stat_activity WHERE application_name = 'pg_dump';")
    if [[ "$running_dumps" != "" ]]; then
      err "pg_dump is already running, aborting new dump"
    fi

    mkdir -p "$DUMPS_DIR"
    schema_v=$(docker.postgres.psql -P pager=off -U postgres -d "postgres_$X_ENV" \
      -c "SELECT version FROM $MIGRATIONS_TABLE;")
    dump_file="${dump_prefix}$(date +%Y-%m-%dT%H-%M-%S)_version${schema_v}.gz"

    echo "Dumping database to $dump_file"
    docker.postgres pg_dump -U postgres -d "postgres_$X_ENV" |
      gzip >"$DUMPS_DIR/$dump_file"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Restore the database with the latest dump or `file` for the current environment.
# Args: [file]
x.db.restore() {
  dump_file="$1"
  test -z $dump_file && err "A dump file path is required"

  local dump_prefix="dump_${X_ENV}_"
  if [[ -n $dump_file ]]; then
    [[ ! -f $dump_file ]] && err "$dump_file does not exist"
    [[ "$dump_file" != *"$dump_prefix"* ]] && confirm "${RED}Dump doesn't match prefix '$dump_prefix'. Continue?${OFF}"
  else
    mkdir -p "$DUMPS_DIR"
    latest_dump_file=$(find "$DUMPS_DIR"/ -name "$dump_prefix*.gz" | sort -r | head -n 1)
    if [[ -z "$latest_dump_file" ]]; then
      err "No $dump_prefix* file found in $DUMPS_DIR"
    fi
    dump_file="$latest_dump_file"
  fi

  confirm "Do you want to restore ${YELLOW}$dump_file${OFF} in the ${RED}$X_ENV${OFF} environment?"

  x.db.drop
  gunzip -c "$dump_file" | docker.postgres.psql -U postgres -d "postgres_$X_ENV"
  # sanity check, but probably better to do it before restoring...
  dump_schema_v=$(docker.postgres.psql -P pager=off -U postgres -d "postgres_$X_ENV" -c "SELECT version FROM $MIGRATIONS_TABLE;")
  file_schema_v=$(echo "$dump_file" | sed -E 's/.*_version([0-9]+)\..*/\1/')
  echo "Migration revision: $dump_schema_v"
  if [[ "$dump_schema_v" != "$file_schema_v" ]]; then
    err "Schema version mismatch: $dump_schema_v (dump) != $file_schema_v (file). Dump has probably been renamed."
  fi
}

########################## e2e ##########################

# Run E2E tests.
x.e2e.run() {
  { { {
    source .env.e2e

    x.gen.e2e

    name="$PROJECT_PREFIX-e2e"
    cd e2e
    DOCKER_BUILDKIT=1 BUILDKIT_PROGRESS=plain docker build -t "$name" .
    cd - >/dev/null

    # need symlink resolution for data

    test -t 0 && opts="-t"
    docker run -i $opts --rm \
      --ipc=host \
      --network host \
      -v "$(pwd)/cmd/oidc-server/data/:/cmd/oidc-server/data/" \
      -v "$(pwd)/e2e:/e2e/" \
      "$name" \
      bash -c "playwright test"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

########################## openapi ##########################

# Run a diff against the previous OpenAPI spec in the main branch.
# Can also be used to generate changelogs when upgrading major versions.
x.diff-oas() {
  { { {
    base_spec="/tmp/openapi.yaml"
    git show "main:$SPEC_PATH" >"$base_spec"

    tmp="$(yq .info.version "$base_spec")"
    base_v="${tmp%%.*}"
    tmp=$(yq .info.version "$SPEC_PATH")
    rev_v="${tmp%%.*}"
    ((rev_v != base_v)) &&
      echo "${YELLOW}Revision mismatch $rev_v and $base_v, skipping diff.${OFF}" && return

    args="-format text -breaking-only -fail-on-diff -exclude-description -exclude-examples"
    if oasdiff $args -base "$base_spec" -revision $SPEC_PATH; then
      echo "${GREEN}No breaking changes found in $SPEC_PATH${OFF}"
    else
      err "${RED}Breaking changes found in $SPEC_PATH${OFF}"
    fi
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

########################### helpers ###########################

# IMPORTANT: bug in declare -F returns line number of last nested function, if any.
# extracting function here instead...
run_hot_reload() {
  x.run.backend-hr &
  pids="$pids $!"
  x.run.frontend &
  pids="$pids $!"
}

run_shared_services() {
  docker network create traefik-net 2>/dev/null || true

  local extra_services
  if [[ $X_ENV != "prod" ]]; then
    extra_services="-f docker-compose.oidc.yml"
  fi
  cd docker
  DOCKER_BUILDKIT=1 BUILDKIT_PROGRESS=plain docker compose \
    -p "$PROJECT_PREFIX" \
    -f docker-compose.shared.yml \
    $extra_services \
    --env-file ../.env."$X_ENV" \
    "$@" 2>&1 # https://github.com/docker/compose/issues/7346
  cd - >/dev/null
}

xo_generate_spec_helpers() {
  local int_ids=()
  local anchors=()
  go-utils.find_db_ids_int int_ids "$PG_REPO_GEN/models"
  for entity in ${int_ids[@]}; do
    local camel_name="" pascal_name=""
    to_camel camel_name "$entity"
    to_pascal pascal_name "$entity"
    anchors+=("x-${camel_name}Parameter: &x-${camel_name}Parameter
  # Generated. DO NOT EDIT.
  !!merge <<: *x-serialIDParameterBase
  schema:
    type: integer
    x-go-type: models.${pascal_name}
    x-go-type-import:
      name: db
      path: $GOMOD_PKG/internal/repos/postgresql/gen/models
  name: ${camel_name}
")
  done
  local uuid_ids=()
  go-utils.find_db_ids_uuid uuid_ids "$PG_REPO_GEN/models"
  for entity in "${uuid_ids[@]}"; do
    local camel_name="" pascal_name=""
    to_camel camel_name "$entity"
    to_pascal pascal_name "$entity"
    anchors+=("x-${camel_name}Parameter: &x-${camel_name}Parameter
  # Generated. DO NOT EDIT.
  !!merge <<: *x-uuidIDParameterBase
  schema:
    type: integer
    x-go-type: models.${pascal_name}
    x-go-type-import:
      name: db
      path: $GOMOD_PKG/internal/repos/postgresql/gen/models
  name: ${camel_name}
")
  done

  local start_pattern="# <<<--- GENERATED MODELS ANCHORS. DO NOT EDIT. --->>>"
  local end_pattern="# <<<--- END OF ANCHORS MODELS GENERATION. DO NOT EDIT. --->>>"
  local generated_anchors="$start_pattern
$(join_by "" "${anchors[@]}")
$end_pattern"

  formatted_string="${generated_anchors//$'\n'/\\n}" # for sed
  sed -i -e "/$start_pattern/,/$end_pattern/c\\
$formatted_string
" "$SPEC_PATH"
}

mark_failed_tool_build() {
  touch "$failed_tool_build_marker"
}

# generate db structs for use with swaggest/openapi-go.
generate_structs_map() {
  # cached most of the time. building here since it must be called before tools build
  # go build -o $BUILD_DIR/ast-parser cmd/ast-parser/main.go

  local path="$PG_REPO_GEN/models"
  local structs=()
  go-utils.find_structs structs "$path"
  for struct in "${structs[@]}"; do
    map_fields+=("\"Models$struct\": new(models.$struct),") # swaggest requires pointer to struct
  done
  map_fields+=("
  //
  ")

  local path="$REST_MODELS"
  local structs=()
  go-utils.find_structs structs "$path"
  for struct in "${structs[@]}"; do
    # rest won't have prefix anymore. it is the default schema to use...
    map_fields+=("\"$struct\": new(rest.$struct),")
  done
  map_fields+=("
  //
  ")

  # workaround for reflection type name with generics in swaggest
  local reflect_type_map_entries=()
  local generic_structs=()
  go-utils.find_generic_structs generic_structs "$path"

  for generic_struct in "${generic_structs[@]}"; do
    reflect_type_map_entries+=("$(find "$path" -exec awk "$AWK_REMOVE_GO_COMMENTS" {} \; |
      sed -n -e "s/^type \(.*\) = ${generic_struct}\[\(.*\)\].*/${generic_struct},$(escape_sed $GOMOD_PKG)\/internal\/rest.\2,\1/p")")
  done

  echo '{}' | jq --argjson array "$(
    printf '%s\n' "${reflect_type_map_entries[@]}" | jq -R -s '
      split("\n")[:-1]
      | map(split(","))
      | map({key: .[0], path: .[1], value: .[2]})'
  )" '.rest = ($array | reduce .[] as $item ({}; . + { ($item.key + "[" + $item.path + "]"): $item.value }))' >"internal/codegen/reflectTypeMap.gen.json"

  # ast-parser find-structs --create-generics-map --public-only "$path" >"internal/codegen/reflectTypeMap.gen.json" & # 1.5s * 2 on each run

  out="internal/codegen/structs.gen.go"
  cat <<EOF >$out
// Code generated by project. DO NOT EDIT.

package codegen

import (
  "$GOMOD_PKG/$PG_REPO_GEN/models"
	rest "$GOMOD_PKG/internal/rest"
  )
    var PublicStructs = map[string]any{
$(printf "%s\n" "${map_fields[@]}")
  }
EOF

  gofumpt -w $out

  wait_without_error
}

# Converts db enums to OpenAPI schemas.
sync_db_enums_with_spec() {
  db_search_paths=("public" "other_schema")

  search_path_str=$(printf "'%s'," "${db_search_paths[@]}")
  search_path_str=${search_path_str%,}
  enum_query=$(printf "
    SELECT t.typname AS enum_name
    FROM pg_type t
    INNER JOIN pg_namespace n ON n.oid = t.typnamespace
    WHERE t.typtype = 'e' AND n.nspname IN (%s);" "$search_path_str")

  mapfile -t db_enum_names < <(docker.postgres.psql -d $POSTGRES_DB -c "$enum_query" 2>/dev/null)
  for db_enum_name in "${db_enum_names[@]}"; do
    local schema_name=""
    to_pascal schema_name "$db_enum_name"
    local enum_values=()
    mapfile -t enum_values < <(docker.postgres.psql -d $POSTGRES_DB -c "SELECT unnest(enum_range(NULL::\"$db_enum_name\"));" 2>/dev/null)

    local schema_path=".components.schemas.$schema_name"
    if yq -e "$schema_path" $SPEC_PATH &>/dev/null; then # -e exits 1 if no match
      if ! yq -e "$schema_path | has(\"x-is-generated\")" $SPEC_PATH &>/dev/null; then
        err "Clashing schema name '$schema_name'. Please remove it before continuing."
      fi
    fi

    src_comment="database enum '$db_enum_name'"
    replace_enum_in_spec "$schema_name" enum_values "$src_comment" "1"
  done
}

x.setup-swagger-ui() {
  go run cmd/swagger-ui-setup/main.go -env=".env.$X_ENV" -swagger-ui-dir=$SWAGGER_UI_DIR

  local bundle_spec="$SWAGGER_UI_DIR/openapi.yaml"
  cp $SPEC_PATH $bundle_spec
  yq 'explode(.)' -i $bundle_spec # derefence yaml
  sed -i 's/!!merge //' $bundle_spec
}

# Replaces database.
# Parameters:
#   Schema name
#   Enum values (nameref)
#   Enum source comment
replace_enum_in_spec() {
  local enum="$1"
  local -n __arr="$2"
  local src_comment="$3"
  local ignore=$4

  local schema_path=".components.schemas.$enum"

  local __enums
  __enums=$(printf ",\"%s\"" "${__arr[@]}")
  __enums="[${__enums:1}]"

  echo "Replacing '$enum' enum in $SPEC_PATH with values from $src_comment"
  __enums=$__enums yq e "
    $schema_path.type = \"string\" |
    $schema_path.enum = env(__enums) |
    $(if test -n "$ignore"; then echo "$schema_path.x-oapi-ignore = true |"; fi)
    $schema_path.x-is-generated = true |
    $schema_path.description = \"is generated from $src_comment.\" |
    ($schema_path | key) line_comment=\"Generated from $src_comment. DO NOT EDIT.\"" -i $SPEC_PATH

}

find_deleted_pkg_schemas() {
  local -n __all_types="$1"
  local pkg="$2"
  local pkg_prefix=""
  to_pascal pkg_prefix "$pkg"

  echo "Finding deleted structs or enums from package '$pkg_prefix'..."
  mapfile -t spec_schemas < <(yq eval '.components.schemas[] | key' "$SPEC_PATH" | grep -E "^${pkg_prefix}" || true)

  local found=0
  for spec_schema in ${spec_schemas[*]}; do
    for type in ${__all_types[@]}; do
      if [[ $spec_schema == "${pkg_prefix}$type" ]]; then
        found=1
      fi
    done
    ((found == 0)) && echo "${YELLOW}[WARNING] $SPEC_PATH schema $spec_schema no longer exists in package '$pkg'. Remove if necessary.${OFF}"
    found=0
  done
}

update_spec_with_structs() {
  echo "Updating spec with rest package structs"

  mapfile -t structs_to_generate < <(yq e '.components.schemas[] | select(has("x-gen-struct")).x-gen-struct' $SPEC_PATH)

  local rest_structs=()
  # should use ast-parser find-structs, but go/types loading is too slow all
  # over gen functions, we might be better off with simple regexes
  go-utils.find_structs rest_structs "$REST_MODELS"
  structs_to_generate+=("${rest_structs[@]}") # always generate all rest/models.spec.go to further prevent user adding clashing names

  # NOTE: https://github.com/pkujhd/goloader allows reloading packages at runtime
  # NOTE: yaegi extraction doesn't work with generics and is slow. See failed-yaegi* branches
  # otherwise maybe yaegi can be used for openapi-go without much fuzz
  # Only implement the above if there are cases where we cant have compilable state when
  # building gen-schema at this step, or the workaround is too tedious
  local existing_structs=()
  go-utils.find_structs existing_structs "$PG_REPO_GEN/models" # we know type () construct is not used in generated code
  mapfile -t existing_structs < <(array.add_prefix "Models" "${existing_structs[@]}")
  if grep -EHn '^type \($' "$REST_MODELS"; then
    err "Type grouping is not supported to allow for faster gen. Fix matches above."
  fi
  go-utils.find_structs existing_structs "$REST_MODELS"

  # TODO: delete old db models as well
  for struct in "${structs_to_generate[@]}"; do
    if ! [[ " ${existing_structs[*]} " =~ " $struct " ]]; then
      echo "Schema with 'x-gen-struct: $struct' no longer exists. Deleting..."
      yq e "del(.components.schemas.$struct)" -i "$SPEC_PATH"
      # structs_to_generate=("${structs_to_generate[@]/$struct/}")  # this removes prefixes from all items instead...
      array.remove_element structs_to_generate "$struct"
    fi
  done

  local int_ids=()
  go-utils.find_db_ids_int int_ids "$PG_REPO_GEN/models"
  local uuid_ids=()
  go-utils.find_db_ids_uuid uuid_ids "$PG_REPO_GEN/models"
  local db_ids=("${int_ids[@]}" "${uuid_ids[@]}")

  struct_names_arg=$(join_by "," ${structs_to_generate[*]})
  existing_structs_arg=$(join_by "," ${existing_structs[*]})
  db_ids_arg=$(join_by "," ${db_ids[*]})

  local gen_schema_spec_path="/tmp/openapi.yaml"
  # always compile and run since we need new PublicStructs that were just changed
  codegen gen-schema --struct-names $struct_names_arg --db-ids $db_ids_arg --existing-structs $existing_structs_arg | yq '
    with_entries(select(.key == "components"))' \
    >$gen_schema_spec_path || mark_failed_tool_build

  schema_keys=$(yq eval '.components.schemas | keys | .[]' "$gen_schema_spec_path")

  # see gen_schema.go for extensions to append
  # TODO: accumulate yq ops instead
  for schema_key in $schema_keys; do
    if [[ $schema_key =~ ^Models[A-Z].*$ ]]; then
      go_type="${schema_key#Models}"

      # db models are ignored in rest gen
      # "x-go-type-import": {"name": "models", "path": "github.com/danicc097/openapi-go-gin-postgres-sqlc/internal/repos/postgresql/gen/models"}
      # "x-go-type": "'$go_type'",
      yq eval -i \
        '.components.schemas."'$schema_key'" += {
          "x-go-type":"'$go_type'"
          }
        ' \
        "$gen_schema_spec_path"
    fi
  done

  # only keep schemas not outputted by gen. User should have been warned on clashes.
  # NOTE: must use shallow merge to replace every single schema, else indirect generations are out of date.
  yq eval-all "(
        select(fi == 0).components.schemas
        ) as \$schemas
        | (
          select(fi == 1).components.schemas
        ) as \$generated_schemas
        | (\$generated_schemas.* | key) line_comment=\"Generated schema. DO NOT EDIT.\"
        | \$generated_schemas.* |= . + {\"x-is-generated\": true}
        | select(fi == 0)
        | .components.schemas += \$generated_schemas
        " "$SPEC_PATH" "$gen_schema_spec_path" | sponge "$SPEC_PATH"
}

remove_schemas_marked_for_deletion() {
  echo "Removing schemas marked for deletion"

  local paths_arr paths
  paths_arr=$(yq e '..
      | select(has("x-TO-BE-DELETED"))
      | path
      | with(.[] | select(contains(".") or contains("/") or contains("{")); . = "\"" + . + "\"")
      | join(".")
      | . = "." + .
    ' $SPEC_PATH)

  test -z "$paths_arr" && return

  paths=$(join_by "," "${paths_arr[@]}")
  yq e "del($paths)" -i "$SPEC_PATH"
}

xo_schema() {
  # xo cannot use db files as input, needs an up-to-date schema
  # not recreating db on every gen can lead to plain wrong generation based on an old dev schema.
  # Also use a unique db to prevent cosmic accidents
  xo schema "postgres://$POSTGRES_USER:$POSTGRES_PASSWORD@localhost:$EXPOSED_POSTGRES_PORT/$GEN_POSTGRES_DB?sslmode=disable" \
    --src "$XO_TEMPLATES_DIR" \
    --exclude $MIGRATIONS_TABLE \
    --exclude $POST_MIGRATIONS_TABLE \
    "$@"
}

# updates dynamic config.json with env vars for the given directory.
# Requires an existing config.template.json
config_template_setup() {
  local dir="$1"
  export ENV_REPLACE_GLOB=$dir/config.json
  # ensure config has all k:v as "<KEY>": "$<KEY>"
  # this has to always be run at startup
  jq \
    'to_entries | map_values({ (.key) : ("$" + .key) }) | reduce .[] as $item ({}; . + $item)' \
    $dir/config.template.json >"$ENV_REPLACE_GLOB"
  envvars=$(printenv | awk -F= '{print $1}' | sed 's/^/\$/g' | paste -sd,)
  frontend/nginx/replace-envvars.sh "$envvars"
}

ensure_tools_up_to_date() {
  mkdir -p "$RUN_CACHE/xfunctions"
  body="$RUN_CACHE/xfunctions/install-tools"
  declare -f x.install-tools >"$body" # ignores comments

  if ! cache_all "$RUN_CACHE/install-tools.md5" "$body" --no-regen >/dev/null; then
    x.install-tools
  fi
}

######################## ENTRYPOINT ########################

export PROC=$$

export POSTGRES_TRACE=${POSTGRES_TRACE:-false}
export GIN_MODE=${GIN_MODE:-release}

source ".project.dependencies.sh"
source ".project.usage.sh"

set -Eeo pipefail
set -o errtrace

X_ENV="dev" # default if none set
test -n "$CI" && X_ENV="ci"
# prod detection not necessary, calls without --x-env=prod will no affect prod data
# unless .env.dev is filled and reuse db/credentials which is far-fetched.

# --------------------- completion and delegation --------------------
#      `complete -o nospace -C foo foo` > `source <(foo bloated_completion)`

declare -A X_CUSTOM_COMPLETION

readonly X_OPT_ENV__COMP="dev prod ci e2e"
readonly X_FUNC_DB_GEN__COMP="up up-1 drop"

_completion_x_option__env() {
  local pre="$1"
  local items=${X_OPT_ENV__COMP[@]}
  for i in ${items[@]}; do
    [[ ${i,,} == ${pre}* || " ${__x_options[*]} " =~ " ${pre} " ]] && echo "$i"
  done
}

_completion_x_function__db.gen() {
  local pre="$1"

  local items=${X_FUNC_DB_GEN__COMP[@]}
  for i in ${items[@]}; do
    [[ ${i,,} == ${pre}* ]] && echo "$i"
  done
}

_completion_x_function__test.backend.watch-for() {
  local pre="$1"
  local dirs=()
  local -n __previous_x_fn_items="$2"
  # local -n __previous_x_opt_items="$3" # not worth it, we wont have multiple completion for options, too convoluted. create multiple x options instead if at all

  dir=${__previous_x_fn_items[0]}
  seenfns=("${__previous_x_fn_items[@]:1}")
  if [[ "${dir}" =~ "${dir%/}/" ]]; then
    declare -a fns
    go-utils.find_test_functions fns "$dir" 2>/dev/null || true

    for i in "${fns[@]}"; do
      [[ ${i,,} == ${pre}* && ! " ${seenfns[*]} " =~ " ${i} " ]] && echo "$i"
    done

    return
  fi

  # TODO abstract away for all completion functions to use
  function find_dirs() {
    while IFS= read -r -d '' dir; do
      str="${dir%/}/"
      str="${dir##\./}/"
      dirs+=("$str")
    done < <(find "$1" -mindepth 1 -maxdepth 1 -type d -print0)
  }

  find_dirs .

  for dir in "${dirs[@]}"; do
    if [[ "${pre}" == "${dir%/}"/* ]]; then
      find_dirs "${pre%/*}"
      break
    fi
  done

  for dir in "${dirs[@]}"; do
    [[ ${dir,,} == ${pre}* ]] && echo "$dir"
  done
}

# should echo an array of possible options.
# args received by completion functions:
# 1. pre string
# 2. current completion items array
X_CUSTOM_COMPLETION=(
  ["db.gen"]="_completion_x_function__db.gen"
  ["test.backend.watch-for"]="_completion_x_function__test.backend.watch-for"
  ["--x-env="]="_completion_x_option__env"
)

while IFS= read -r line; do
  [[ $line =~ ^declare\ -f\ x\. ]] || continue
  COMMANDS+=("${line##declare -f x.}")
done < <(declare -F)
# sort the array. Mimic file input to sort
mapfile -t COMMANDS < \
  <(LC_COLLATE=C sort < <(printf "%s\n" "${COMMANDS[@]}"))

MAX_XFN_LEN=0 # for logging purposes
for c in "${COMMANDS[@]}"; do
  len=${#c}
  ((len > MAX_XFN_LEN)) && MAX_XFN_LEN=$((len - 1)) # remove "x." but account for extra last space appended.
done

if [[ -n $COMP_LINE ]]; then
  pre="${COMP_LINE##* }" # the part after the last space in the current command
  pre="${pre,,}"         # autocomplete regardless of user input case
  cur_commands=(${COMP_LINE%"$pre"})
  # x option that accepts a single completion string needs special handling like --x-env=
  if [[ "$COMP_LINE" =~ =$ ]]; then # ends with =
    cur_commands=(${COMP_LINE})
  elif [[ "$COMP_LINE" =~ =([^[:space:]]+)$ ]]; then # started typing option
    pre=${BASH_REMATCH[1]}
    cur_commands=("${COMP_LINE%%=*}=")
  fi

  complete_with_fn() {
    local fn=${X_CUSTOM_COMPLETION["$1"]}
    local items=$($fn "$pre" previous_x_fn_items 2>/dev/null || true)
    test -z "$items" && return
    for i in ${items[@]}; do
      echo "$i"
    done
    exit
  }

  for c in "${COMMANDS[@]}"; do
    if [[ " ${cur_commands[*]} " =~ " ${c} " ]]; then
      xfn_specified=$c
      break
    fi
  done

  for c in "${COMMANDS[@]}"; do
    test -z "${xfn_specified}" || break
    test -z "${pre}" -o "${c}" != "${c#"${pre}"}" -a "${pre}" != "${c}" && echo "${c} "
  done

  declare __x_options x_options_lines

  parse_x_options x_options_lines

  for c in "${x_options_lines[@]}"; do
    tmp="${c%%)*}"
    xopt="${tmp//\*/}"
    __x_options+=("$xopt")
  done

  # will jump directly to x options completion if we start typing "-"
  declare -A __x_opts_seen
  for cmd in ${cur_commands[@]}; do
    for opt in ${__x_options[@]}; do
      if [[ "$cmd" == *"$opt"* ]]; then
        __x_opts_seen[$opt]=true
        __x_last_opt_seen=$opt
        break
      fi
    done
  done

  # completion for functions may be filepath completion, single or multiple selection from list of items...
  # theres no way to abstract to main autocomplete.
  declare -A __x_completion_x_options_items_seen
  declare -A __x_completion_x_function_items_seen

  previous_x_fn_items=()
  save_flag=false
  for i in "${!cur_commands[@]}"; do
    cmd="${cur_commands[$i]}"

    if [ "$cmd" == "$xfn_specified" ]; then
      save_flag=true
      continue # start matching rest of args passed
    fi

    if [ "$save_flag" == true ]; then
      previous_x_fn_items+=("$cmd")
    fi
  done

  # show completion for an x function
  if [[ -n "${xfn_specified}" && "${pre}" != -* && ${#__x_opts_seen[@]} -eq 0 ]]; then
    complete_with_fn $xfn_specified || true
  # show completion for last --x-(.*)= if it specifies a completion func
  elif [[ ${#__x_opts_seen[@]} -gt 0 ]] && [[ -n "$pre" ]]; then
    xopt="${pre//\*/}"
    ! [[ " ${__x_options[*]} " =~ " ${xopt} " ]] && xopt="$__x_last_opt_seen"
    complete_with_fn $xopt || true
  fi

  test -z "${xfn_specified}" && exit

  for opt in ${__x_options[@]}; do
    [[ -n "${__x_opts_seen[$opt]}" ]] && continue
    if [[ ${opt:0:${#pre}} == "${pre,,}" ]]; then
      [[ "$opt" == "${pre,,}" ]] && continue # will have to be removed for inner completion
      if [[ "${opt,,}" =~ ^.*= ]]; then
        echo "${opt}"
      else
        echo "${opt} "
      fi
    fi
  done

  exit
fi

declare CMD="$1"

# First comment lines automatically added to usage docs.
while [[ "$#" -gt 0 ]]; do
  case $1 in
  --x-help)
    # Show help for a particular x function.
    COMMANDS=("$CMD")
    usage
    exit
    ;;
  --x-force-regen)
    # Removes code generation cache, forcing a new run.
    export X_FORCE_REGEN=1
    ;;
  --x-no-confirmation)
    # Bypasses confirmation messages. (WIP: Use `yes` in the meantime)
    export X_NO_CONFIRMATION=1
    ;;
  --x-no-gen)
    # Skips code generation steps.
    export X_NO_GEN=1
    ;;
  --x-no-exit-cleanup)
    # Skip default cleanup on exit.
    export X_NO_EXIT_CLEANUP=1
    ;;
  --x-debug)
    # Debug bash script with -x shell option.
    export X_DEBUG=1
    ;;
  --x-no-backup)
    # Backup stash is not restored on failure.
    # Please ensure there are no important uncommitted changes in the current branch beforehand.
    export X_NO_BACKUP=1
    ;;
  --x-env=*)
    # Environment to run commands in. Defaults to "dev" locally.
    # Args: env
    export X_ENV="${1#--x-env=}"
    if [[ ! " ${X_OPT_ENV__COMP[*]} " =~ " $X_ENV " ]]; then
      err "Valid environments: $X_OPT_ENV__COMP"
    fi
    ;;
  *)
    # will set everything else back later
    args+=("$1")
    ;;
  esac
  shift
done

for arg in ${args[@]}; do
  set -- "$@" "$arg"
done

export BUILD_VERSION="$X_ENV-$(git rev-parse --verify HEAD)"

readonly X_FORCE_REGEN X_NO_CONFIRMATION X_NO_GEN X_NO_BACKUP X_ENV X_DEBUG X_NO_EXIT_CLEANUP
readonly base_cache_deps="bin/project bin/.helpers.sh .env.$X_ENV"

######################## INIT ########################

if test -n "$X_DEBUG"; then
  set -x
  N=$(date +%s%N)
  # NOTE: would be nice to process incremental times, but due to xlog and xerr, subshells and
  # background commands all over the place it's nearly impossible
  if test -z "${BASH_SOURCE[1]##*/}"; then
    export PS4='+[$((($(date +%s%N)-$N)/1000000))ms][${YELLOW}${BASH_SOURCE##*/}:${LINENO}${OFF}]: ${FUNCNAME[0]:+${FUNCNAME[0]}(): }'
  else
    export PS4='+[$((($(date +%s%N)-$N)/1000000))ms][${YELLOW}${BASH_SOURCE[1]##*/}:${BASH_LINENO[0]}${OFF}]: ${FUNCNAME[0]:+${FUNCNAME[0]}(): }'
  fi
fi

# applicable to any command
ensure_envvars_set ".env.template" ".env.${X_ENV}"

# export to all subsequent commands or scripts
set -a
# shellcheck source=SCRIPTDIR/../.env.dev
source ".env.$X_ENV"
set +a

trap 'show_tracebacks' ERR
trap killgroup EXIT HUP INT TERM
trap errtrap SIGUSR1
# may get called multiple times
trap 'exit-cleanup $LINENO' EXIT HUP INT TERM # EXIT (0) executed on exit from the shell
if test -n "$X_NO_EXIT_CLEANUP"; then
  trap ':' EXIT HUP INT TERM
fi
exit_cleanup_calls=0

# exit-cleanup prevents exiting while we check if we should run cleanup.
# For monitoring functions, a top level "trap 'exit' SIGINT"
# is necessary to allow the user to exit. Handle cleanup calls manually, if any.
# If the background process calls this script externally, --x-no-exit-cleanup is required
# in those calls, due to possible CURRENT_BRANCH changes in the meantime.
exit-cleanup() {
  trap 'echo ignoring SIGINT' SIGINT
  [[ "$exit_cleanup_calls" -gt 0 ]] && return
  ((exit_cleanup_calls++)) || true

  {
    if [[ $need_backup_restore = true ]]; then
      test -z "$X_NO_BACKUP" && xsetup.backup.restore
      # IMPORTANT: if any command failed always restore gen cache, regardless of flags passed.
      gen-cache.restore
    fi

    # will fail if not using --x-no-exit-cleanup when required
    git checkout "$CURRENT_BRANCH" &>/dev/null || err "[ERROR] Could not checkout $CURRENT_BRANCH branch" >&2
    rm -f backup-gen-stash-dummy.txt || true
    git branch -D "$backup_branch" &>/dev/null || true

    cd "$starting_cwd" || true
  } & # ensure cleanup is always run at exit
  wait
}

killgroup() {
  printf "Killing spawned processes...\n\n"
  # kill $(jobs -p) 2>/dev/null # doesn't really kill all children, just process group leaders
  kill_descendants 2>/dev/null || true
  pgrep -P $PROC | xargs kill || true
  exit 1
}

errtrap() {
  printf "Exiting due to propagated error...\n"
  killgroup
}

source .envrc

pids=""

################ handle executing x functions ################

if [[ -n "$1" ]]; then
  shift
  for c in "${COMMANDS[@]}"; do
    declare cmd=$(command -v "x.$c")
    if [[ $c == "$CMD" && -n "$cmd" ]]; then
      ensure_tools_up_to_date

      "x.$CMD" "$@"
      err_code=$?

      if test -f "$failed_tool_build_marker"; then
        err "At least one gen tool build failed. A gen rerun may be required"
      fi

      exit $err_code # do not quote
    fi
  done
fi

# default to show usage if its a noop or didn't match a command
usage
