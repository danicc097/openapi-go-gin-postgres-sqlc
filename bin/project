#!/bin/bash
# shellcheck disable=1091,2155,2068,2086,2031,2048,2178
#
#
# Main s̶c̶r̶i̶p̶t program to manage the entire project stack.
#
#

export PROC=$$

source ".helpers.sh"
source ".project.dependencies.sh"
source ".project.usage.sh"

starting_cwd=$(pwd)
ensure_pwd_is_top_level

# log for any function output.
xlog() {
  local fname="${FUNCNAME[1]#*.}"
  local color="$BLUE"
  local max_len=$MAX_FNAME_LOG_LEN

  [[ "$CMD" = "$fname" ]] && cat && return

  if [[ "${FUNCNAME[1]%%.*}" != "x" ]]; then
    fname="${FUNCNAME[1]}" # show non-x funcs
    color="$MAGENTA"
  fi

  if [[ "${FUNCNAME[1]}" =~ ^.*(check\.bin|install\.bin).* ]]; then
    max_len=100
  fi

  if [[ ${#fname} -gt $max_len ]]; then
    fname="${fname:0:$max_len}…"
  fi

  local _=$(printf "%*s |\n" $((max_len + 1)) "$fname")
  sed -ue "s/^/${color}$fname >${OFF} /"
}

# log stderr for any function output.
# sed is buffering by default (without -u) so streams dont preserve order
# > >(one) 2> >(two) are background processes so it will break our parallel code.
xerr() {
  local fname="${FUNCNAME[1]#*.}"
  local max_len=$MAX_FNAME_LOG_LEN

  [[ "$CMD" = "$fname" ]] && cat && return
  if [[ ${#fname} -gt $max_len ]]; then
    fname="${fname:0:$max_len}…"
  fi

  local _=$(printf "%*s |\n" $((max_len + 1)) "$fname")
  sed -ue "s/^/${RED}$fname >${OFF} /" >&2
}

kill_descendants() {
  # air and vite spawn processes as well, need to kill those (whose parent is pid), kill $pid will not kill children. pkill -P would also work
  kill $pids || true
  kill "$(list_descendants $pids)" || true
  pids=""
}

######################### x-functions #########################

# Check build dependencies are met and prompt to install if missing.
x.check-build-deps() {
  { { {
    mkdir -p ./bin/tools/

    while IFS= read -r line; do
      [[ $line =~ ^declare\ -f\ check\.bin\. ]] && BIN_CHECKS+=("${line##declare -f check.bin.}")
      [[ $line =~ ^declare\ -f\ install\.bin\. ]] && BIN_INSTALLS+=("${line##declare -f install.bin.}")
    done < <(declare -F)

    echo "Checking dependencies..."
    for bin in "${BIN_CHECKS[@]}"; do
      # local r
      # r="$(...)" # redirect to var while also streaming unbuffered output with | tee /dev/tty
      if "check.bin.$bin"; then
        continue
      fi

      if ! element_in_array "$bin" "${BIN_INSTALLS[@]}"; then
        echo "No automatic installation available. Please install $bin manually and retry"
        exit 1
      fi

      confirm "Do you want to install $bin now?" || exit 1

      # TODO test -n "$NO_CONFIRMATION" && { yes y | install...}
      echo "Installing $bin..."
      if ! "install.bin.$bin"; then
        err "$bin installation failed"
      fi
      echo "Installed $bin..."
    done
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Check dependencies and fetch required tools.
x.bootstrap() {
  { { {
    git submodule update --init --recursive
    x.check-build-deps
    x.install-tools
    x.setup.swagger-ui
    x.gen.build-tools
    traefik_dir="$HOME/traefik-bootstrap"
    confirm "Do you want to setup and run traefik (install dir: $traefik_dir)?" && x.setup.traefik "$traefik_dir"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Install go libraries as runnable programs.
x.install-tools() {
  { { {
    echo "Installing go tooling..."

    go install -tags 'postgres' github.com/golang-migrate/migrate/v4/cmd/migrate@v4.15.2 &
    # go install github.com/kyleconroy/sqlc/cmd/sqlc@v1.16.0 &
    go install github.com/danicc097/sqlc/cmd/sqlc@custom &
    go install github.com/golangci/golangci-lint/cmd/golangci-lint@v1.47.2 &
    go install github.com/joho/godotenv/cmd/godotenv@latest &
    go install github.com/tufin/oasdiff@latest &
    go install golang.org/x/tools/cmd/goimports@latest &
    go install mvdan.cc/gofumpt@latest &
    go install github.com/danicc097/air@latest &
    go install github.com/danicc097/xo@v1.0.0 &
    go install github.com/mikefarah/yq/v4@v4.27.2 &
    go install github.com/hexdigest/gowrap/cmd/gowrap@latest &

    go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.28.1 &
    go install github.com/planetscale/vtprotobuf/cmd/protoc-gen-go-vtproto@v0.2.0 &
    go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.2 &

    GO111MODULE=off go get -u github.com/maxbrunsfeld/counterfeiter &

    wait
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Fetch latest Swagger UI bundle.
x.setup.swagger-ui() {
  { { {
    local name="$(curl --silent "https://api.github.com/repos/swagger-api/swagger-ui/releases/latest" | jq -r ".. .tag_name? // empty")"
    curl -fsSL "github.com/swagger-api/swagger-ui/archive/refs/tags/$name.tar.gz" -o swagger-ui.tar.gz
    tar xf swagger-ui.tar.gz swagger-ui-"${name#*v}"/dist --one-top-level=swagger-ui --strip-components=2
    rm swagger-ui.tar.gz
    mkdir -p internal/static/swagger-ui
    mv swagger-ui/* internal/static/swagger-ui/
    rm -r swagger-ui
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Run pre-generation scripts in the internal package.
x.gen.pregen() {
  backup-gen
  drop-and-migrate-gen-db
  { { {
    POSTGRES_DB="$GEN_POSTGRES_DB"

    echo "Running generation"

    missing_opids="$(yq e "(.paths[][] | select(has(\"operationId\") | not) | path) | join(\".\")" $SPEC)"
    [[ -n "$missing_opids" ]] && err "Paths missing operationId: $(printf "\t\n%s" ${missing_opids[*]})"

    # spec must always be valid before generating. Schemas to generate must already exist with the appropiate vendor extension
    pregen -env=".env.$env" --validate-spec-only

    sync_db_enums_with_spec

    ######## Sync projects and related project info

    mapfile -t db_projects < <(dockerdb_psql -d $POSTGRES_DB -c "select name from projects;" 2>/dev/null)

    [[ ${#db_projects[@]} -gt 0 ]] || err "No projects found in database $POSTGRES_DB"

    src_comment="projects table"
    replace_enum_in_spec "Project" db_projects "$src_comment"

    for project in ${db_projects[@]}; do
      ### kanban steps
      mapfile -t kanban_steps < <(dockerdb_psql -d $POSTGRES_DB -c "
          select name from kanban_steps where project_id = (
            select project_id from projects where name = '$project'
            );" 2>/dev/null)
      [[ ${#kanban_steps[@]} -gt 0 ]] || {
        echo "${YELLOW}[WARNING] No kanban steps found for project '$project' in database $POSTGRES_DB${OFF}" && continue
      }
      schema_name="$(to_pascal $project)KanbanSteps"
      src_comment="kanban_steps table"
      replace_enum_in_spec "$schema_name" kanban_steps "$src_comment"

      ### work item types
      mapfile -t work_item_types < <(dockerdb_psql -d $POSTGRES_DB -c "
          select name from work_item_types where project_id = (
            select project_id from projects where name = '$project'
            );" 2>/dev/null)
      [[ ${#work_item_types[@]} -gt 0 ]] || {
        echo "${YELLOW}[WARNING] No work item types found for project '$project' in database $POSTGRES_DB${OFF}" && continue
      }
      schema_name="$(to_pascal $project)WorkItemTypes"
      src_comment="work_item_types table"
      replace_enum_in_spec "$schema_name" work_item_types "$src_comment"
    done

    generate_models_mappings

    ######## Ensure consistent style for future codegen

    echo "Applying PascalCase to operation IDs in $SPEC"
    for opid in $(yq e ".. | select(has(\"operationId\")).operationId" $SPEC); do
      local new_opid="$(to_pascal $opid)"
      local paths=$(opid=$opid yq e ".. | select(has(\"operationId\") and .operationId == env(opid)) | (path)" $SPEC)
      mapfile -t paths <<<$paths
      local escaped_path="$(join_yq_paths paths)"
      new_opid=$new_opid yq e ".$escaped_path.operationId = env(new_opid)" -i $SPEC
    done

    ######## Sync enums with external sources and validate
    ######## external json files are the source of truth
    # frontend scopes: 1) replace all "-_" by " " 2) Sort 3) range over and split(":") -> map[left] = append(right) and show aggregated
    # arrays can't be nested
    declare -A enum_src_files=(
      [Scope]="scopes.json"
      [Role]="roles.json"
    )
    declare -A enum_vext=(
      [Scope]="x-required-scopes"
      [Role]="x-required-role"
    )
    for enum in ${!enum_src_files[@]}; do
      [[ $(yq e ".components.schemas | has(\"$enum\")" $SPEC) = "false" ]] &&
        yq e ".components.schemas.$enum.type = \"string\"" -i $SPEC

      local src_file="${enum_src_files[$enum]}"
      vendor_ext="${enum_vext[$enum]}"

      enums=$(yq -P '.[] | key' $src_file)
      mapfile -t enums <<<$enums

      src_comment="$src_file keys"
      replace_enum_in_spec "$enum" enums "$src_comment"

      mapfile spec_enums < <(yq e ".paths[][].$vendor_ext | select(length > 0)" $SPEC)
      spec_enums=("${spec_enums[*]//- /}")
      mapfile -t spec_enums < <(printf "\"%s\"\n" ${spec_enums[*]})
      mapfile -t clean_enums < <(printf "\"%s\"\n" ${enums[*]})
      # ensure only existing enums from src_file are used
      for spec_enum in "${spec_enums[@]}"; do
        [[ ! " ${clean_enums[*]} " =~ " ${spec_enum} " ]] && err "$spec_enum is not a valid '$enum'"
      done
    done

    ######## Generate shared policies once the spec has been validated
    echo "Writing shared auth policies"
    yq -o=json e "
    .paths[][]
    | {.operationId: { \"scopes\": .x-required-scopes, \"role\": .x-required-role, \"requiresAuthentication\": has(\"security\")}}
    | select(.[]) as \$i ireduce ({}; . + \$i)
  " openapi.yaml >$OPID_AUTH

    yq --prettyPrint '.. style="single"' -i $SPEC
    # https://mikefarah.gitbook.io/yq/operators/anchor-and-alias-operators
    # but some tooling is not aware of this and raises errors, e.g. openapi-typescript-codegen, vscode yaml
    sed -i 's/!!merge //' $SPEC

    pregen -env=".env.$env" -op-id-auth="$OPID_AUTH"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  backup-gen.cleanup
}

# for manually inserted elements via migrations, e.g. projects, kanban_steps, work_item_type,
# generate 2-way maps id<- ->name to save up useless db calls and make logic switching
# and repos usage much easier
generate_models_mappings() {
  local model_mappings_path="internal/models_mappings.gen.go"
  cat <<EOF >$model_mappings_path
// Code generated by 'project gen.pregen'. DO NOT EDIT.

package internal

import "$GOMOD_PKG/internal/models"

EOF

  mapfile -t projects_rows < <(dockerdb_psql -d $POSTGRES_DB -c "select project_id,name from projects;" 2>/dev/null)
  generate_model_mappings_dicts Project projects_rows

  for project in ${db_projects[@]}; do
    mapfile -t kanban_steps_rows < <(dockerdb_psql -d $POSTGRES_DB -c "
          select kanban_step_id,name from kanban_steps where project_id = (
            select project_id from projects where name = '$project'
            );" 2>/dev/null)
    [[ ${#kanban_steps_rows[@]} -gt 0 ]] || continue
    prefix="$(to_pascal "$project")KanbanSteps"
    generate_model_mappings_dicts $prefix kanban_steps_rows

    kanban_steps_rows=()
    mapfile -t kanban_steps_rows < <(dockerdb_psql -d $POSTGRES_DB -c "
          select kanban_step_id,step_order from kanban_steps where project_id = (
            select project_id from projects where name = '$project'
            );" 2>/dev/null)
    [[ ${#kanban_steps_rows[@]} -gt 0 ]] || continue
    prefix="$(to_pascal "$project")KanbanSteps"
    echo "var (
    ${prefix}StepOrderByID = map[int]int{
  " >>$model_mappings_path
    for row in "${kanban_steps_rows[@]}"; do
      first=$(cut_first "$row" "|") # always safe
      mapfile -t arr <<<"${first}"
      local id="${arr[0]}"
      local kanban_step="${arr[1]}"
      echo "${id}: ${kanban_step}," >>$model_mappings_path
    done
    echo "})" >>$model_mappings_path
  done

  for project in ${db_projects[@]}; do
    mapfile -t work_item_types_rows < <(dockerdb_psql -d $POSTGRES_DB -c "
          select work_item_type_id,name from work_item_types where project_id = (
            select project_id from projects where name = '$project'
            );" 2>/dev/null)
    [[ ${#work_item_types_rows[@]} -gt 0 ]] || continue
    prefix="$(to_pascal "$project")WorkItemTypes"
    generate_model_mappings_dicts $prefix work_item_types_rows
  done

  gofumpt -w $model_mappings_path
}

# generates dictionaries for existing database elements, meant for those
# inserted exclusively via migrations
generate_model_mappings_dicts() {
  local prefix="$1"
  local -n __arr="$2" # db rows
  echo "var (
	${prefix}NameByID = map[int]models.${prefix}{
  " >>$model_mappings_path
  for row in "${__arr[@]}"; do
    first=$(cut_first "$row" "|") # always safe
    mapfile -t arr <<<"${first}"
    local id="${arr[0]}"
    local name="${arr[1]}"
    echo "${id}: models.${prefix}$(to_pascal "$name")," >>$model_mappings_path
  done
  echo "}
	${prefix}IDByName = map[models.${prefix}]int{
  " >>$model_mappings_path
  for row in "${__arr[@]}"; do
    first=$(cut_first "$row" "|") # always safe
    mapfile -t arr <<<"${first}"
    local id="${arr[0]}"
    local name="${arr[1]}"
    echo "models.${prefix}$(to_pascal "$name"): ${id}," >>$model_mappings_path
  done
  echo "})
  " >>$model_mappings_path
}

# outputs safe double-quoted paths for yq and
# fixes paths where a dot is present
join_yq_paths() {
  local -n __arr="$1"
  __arr=("${__arr[*]//- /}")
  mapfile -t __arr < <(printf "\"%s\"\n" ${__arr[*]})
  join_by "." ${__arr[*]}
}

clean_yq_array() {
  local -n __arr="$1"
  __arr=("${__arr[*]//- /}")
  mapfile -t __arr < <(printf "\"%s\"\n" ${__arr[*]})
  echo ${__arr[@]}
}

# Run post-generation scripts in the internal package.
x.gen.postgen() {
  backup-gen
  drop-and-migrate-gen-db
  { { {
    POSTGRES_DB="$GEN_POSTGRES_DB"

    echo "Running generation"
    generate_structs_map

    update_spec_with_structs

    delete_duplicate_schemas

    postgen -env=".env.$env"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  backup-gen.cleanup
}

# Generate type-safe Go code from SQL.
x.gen.sqlc() {
  backup-gen
  drop-and-migrate-gen-db
  { { {
    echo "Running generation"
    rm -f "$PG_REPO_GEN"/db/*.sqlc.go
    sqlc generate --experimental -f "$PG_REPO_DIR"/sqlc.yaml || err "Failed sqlc generation"
    rm -f "$PG_REPO_GEN"/db/models.go # sqlc enums
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  backup-gen.cleanup
}

# Automatically generate CRUD and index queries with joins based on existing indexes from a Postgres schema.
x.gen.xo() {
  backup-gen
  drop-and-migrate-gen-db
  { { {
    if md5sum -c "$CACHE/xo.md5" >/dev/null && [[ $FORCE_REGEN -eq 0 ]]; then
      echo "Skipping generation (cached). Force regen with --x-force-regen"
      exit 0
    fi
    md5_all "$CACHE/xo.md5" ${CACHE_DEP_FILES[@]} go.mod db/ $XO_TEMPLATES_DIR/ &

    echo "Running generation"

    rm -rf "$PG_REPO_GEN"/db/*.xo.go

    mkdir -p "$PG_REPO_GEN"/db
    xo_schema -o "$PG_REPO_GEN"/db --debug \
      --schema public \
      --ignore "*.created_at" \
      --ignore "*.updated_at" || err "Failed xo public schema generation" &

    mkdir -p "$PG_REPO_GEN"/db/v
    xo_schema -o "$PG_REPO_GEN"/db/v \
      --schema v \
      --main-schema-pkg $GOMOD_PKG/"$PG_REPO_GEN"/db ||
      err "Failed xo v schema generation" &

    mkdir -p "$PG_REPO_GEN"/db/cache
    xo_schema -o "$PG_REPO_GEN"/db/cache \
      --schema cache \
      --main-schema-pkg $GOMOD_PKG/"$PG_REPO_GEN"/db ||
      err "Failed xo cache schema generation" &

    wait

    files=$(find "$PG_REPO_GEN/db" \
      -name "*.go")
    goimports -w $files
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  backup-gen.cleanup
}

# Shows current database column comments used in xo codegen
# to ease further updates to comments in migrations.
x.gen.show-column-comments() {
  local query="SELECT DISTINCT
  c.relname as table,
  a.attname::varchar AS column,
  COALESCE(col_description(format('%s.%s', n.nspname, c.relname)::regclass::oid, isc.ordinal_position), '') as column_comment
FROM pg_attribute a
  JOIN ONLY pg_class c ON c.oid = a.attrelid
  JOIN ONLY pg_namespace n ON n.oid = c.relnamespace
  INNER JOIN information_schema.columns as isc on c.relname = isc.table_name and isc.column_name = a.attname
  LEFT JOIN pg_constraint ct ON ct.conrelid = c.oid
    AND a.attnum = ANY(ct.conkey)
    AND ct.contype = 'p'
  LEFT JOIN pg_attrdef ad ON ad.adrelid = c.oid
    AND ad.adnum = a.attnum
WHERE a.attisdropped = false
  AND n.nspname = 'public'
  AND (true OR a.attnum > 0)
  AND col_description(format('%s.%s', n.nspname, c.relname)::regclass::oid, isc.ordinal_position) is not null;"

  dockerdb psql --no-psqlrc -d $POSTGRES_DB -c "$query" 2>/dev/null
}

# Generate a type-safe SQL builder.
x.gen.jet() {
  backup-gen
  drop-and-migrate-gen-db
  { { {
    # results may be combined with xo's *Public structs and not reinvent the wheel for jet.
    # should not be hard to generate all adapters at once jet->xo *Public in a new file alongside jet gen.
    # in the end fields are the same name if goName conventions are followed (configurable via custom jet cmd)
    # if it gives problems for some fields (ID, API and the like)
    echo "Running generation"

    local gen_path="$PG_REPO_GEN/jet"
    local schema=public
    rm -rf "$gen_path"
    {
      jet -dbname="$GEN_POSTGRES_DB" -env=.env."$env" --out=./"$gen_path" --schema=$schema
      mv "./$gen_path"/$GEN_POSTGRES_DB/* "$gen_path"
      rm -r "./$gen_path/$GEN_POSTGRES_DB/"
    }
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  backup-gen.cleanup
}

# Generate interface wrappers with common logic: tracing, timeout...
x.gen.gowrap() {
  backup-gen
  { { {
    if md5sum -c "$CACHE/gowrap.md5" >/dev/null && [[ $FORCE_REGEN -eq 0 ]]; then
      echo "Skipping generation (cached). Force regen with --x-force-regen"
      exit 0
    fi
    md5_all "$CACHE/gowrap.md5" ${CACHE_DEP_FILES[@]} go.mod $repos $GOWRAP_TEMPLATES_DIR &

    echo "Running generation"

    local suffixes=(
      "retry:with_retry"
      "timeout:with_timeout"
      "otel:with_otel"
      "prometheus:with_prometheus"
    )
    local repos="$REPOS_DIR/repos.go"
    local repo_interfaces=()
    find_go_interfaces repo_interfaces $repos

    for iface in ${repo_interfaces[@]}; do
      for suffix in ${suffixes[@]}; do
        {
          IFS=":" read -r -a arr <<<${suffix}
          local tmpl="${arr[0]}"
          local suffix="${arr[1]}"
          gowrap gen \
            -g \
            -p $GOMOD_PKG/$REPOS_DIR \
            -i $iface \
            -t "$GOWRAP_TEMPLATES_DIR/$tmpl.tmpl" \
            -o "$REPOS_DIR/reposwrappers/${iface,,}_$suffix.gen.go"
        } &
      done
    done

    wait
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  backup-gen.cleanup
}

# Generate Go client and server.
x.gen.client-server() {
  { { {
    echo "Running generation"
    paths=".openapi.paths.yaml"

    # hack to get separate types generation
    sed "s/\$ref: '\#\//\$ref: '$SPEC\#\//g" $SPEC >"$paths"
    # yq e 'del(.components)' -i "$paths" # dont delete since recent oapi-codegen does some checks even if we are not generating types

    oapi-codegen --config internal/models/oapi-codegen-types.yaml "$SPEC" || err "Failed types generation"
    oapi-codegen --config internal/rest/oapi-codegen-server.yaml --models-pkg models "$paths" || err "Failed server generation"
    oapi-codegen --config internal/client/oapi-codegen-client.yaml "$SPEC" || err "Failed client generation"

  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Generate mocks for specified interfaces.
x.gen.counterfeiter() {
  # This shouldn't pose any problems, the interface is the only input to counterfeiter.
  { { {
    envvar="internal/envvar/envvar.go"
    repos="$REPOS_DIR/repos.go"
    tfidfpb="internal/pb/python-ml-app-protos/tfidf/v1/service_grpc.pb.go"

    if md5sum -c "$CACHE/counterfeiter.md5" >/dev/null && [[ $FORCE_REGEN -eq 0 ]]; then
      echo "Skipping generation (cached). Force regen with --x-force-regen"
      exit 0
    fi
    md5_all "$CACHE/counterfeiter.md5" ${CACHE_DEP_FILES[@]} go.mod $envvar $repos $tfidfpb &

    echo "Recreating mocks"
    {
      counterfeiter -o internal/envvar/envvartesting/provider.gen.go $envvar Provider &
      counterfeiter -o $REPOS_DIR/repostesting/user.gen.go $repos User &
      counterfeiter -o $REPOS_DIR/repostesting/notification.gen.go $repos Notification &
      counterfeiter -o $REPOS_DIR/repostesting/project.gen.go $repos Project &
      counterfeiter -o $REPOS_DIR/repostesting/team.gen.go $repos Team &
      counterfeiter -o internal/pb/python-ml-app-protos/tfidf/v1/v1testing/movie_genre_client.gen.go $tfidfpb MovieGenreClient &
      counterfeiter -o internal/pb/python-ml-app-protos/tfidf/v1/v1testing/movie_genre_server.gen.go $tfidfpb MovieGenreServer &
      wait
    } 2>&1 # outputs to stderr for some reason

    # counterfeiter is unaware of grpc's obscure mustEmbedUnimplemented***() for forward server compatibility
    sed -i '/type FakeMovieGenreServer struct {/a v1\.UnimplementedMovieGenreServer' \
      internal/pb/python-ml-app-protos/tfidf/v1/v1testing/movie_genre_server.gen.go
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Generate the required servers/clients for relevant services.
x.gen.proto() {
  { { {
    local import_path="python-ml-app-protos/tfidf/v1"
    local filename="internal/python-ml-app-protos/tfidf/v1/service.proto"

    mkdir -p internal/pb
    # Plugins are no longer supported by protoc-gen-go.
    # Instead protoc-gen-go-grpc and the go package (in proto or via M flag) are required
    echo "Running generation"
    protoc \
      --go-grpc_out=internal/pb/. \
      --go_out=internal/pb/. \
      --go-grpc_opt=M$filename=$import_path,paths=import \
      --go_opt=M$filename=$import_path,paths=import \
      internal/python-ml-app-protos/tfidf/v1/service.proto || err "Failed proto generation"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Run frontend code generation.
x.gen.frontend() {
  backup-gen
  { { {
    (
      frontend_config_setup

      SCHEMA_OUT="frontend/src/types/schema.d.ts"
      orval_config="frontend/orval.config.ts"

      if md5sum -c "$CACHE/frontend.md5" >/dev/null && [[ $FORCE_REGEN -eq 0 ]]; then
        echo "Skipping generation (cached). Force regen with --x-force-regen"
        exit 0
      fi
      md5_all "$CACHE/frontend.md5" ${CACHE_DEP_FILES[@]} $SPEC $orval_config frontend/package.json &

      mkdir -p frontend/src/types
      rm -rf frontend/src/gen

      {
        node frontend/generate-client-validator.js
        # TODO allow custom validate.ts baked into fork
        rm -f frontend/src/client-validator/gen/validate.ts
        find frontend/src/client-validator/gen/ -type f -exec \
          sed -i "s/from '.\/validate'/from '..\/validate'/g" {} \;
      } &
      {
        v="$(openapi-typescript --version)"
        openapi-typescript $SPEC --output "$SCHEMA_OUT" --path-params-as-types --prettier-config .prettierrc
        echo "/* Generated by openapi-typescript $v */
/* eslint-disable @typescript-eslint/ban-ts-comment */
/* eslint-disable */
// @ts-nocheck
export type schemas = components['schemas']
" | cat - "$SCHEMA_OUT" >/tmp/out && mv /tmp/out "$SCHEMA_OUT"
      } &
      orval --config $orval_config &

      wait_without_error
    )
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  backup-gen.cleanup
}

# Run all codegen and postgen commands for the project.
x.gen() {
  backup-gen # Modification of vars inside would be local to subshell (caused by pipeline)
  drop-and-migrate-gen-db
  { { {
    echo "Running code generation"

    x.gen.build-tools # || true (this should not fail compilation anymore, the whole app should be compilable at this point)

    # TODO try use gnu parallel and exit when anyone fails
    # (with current setup the whole x.gen is executed and only then
    # the trap on SIGUSR1 coming from `err` is run - now temporarily using kill 0 in `err` instead on sending SIGUSR1)
    # shopt -s inherit_errexit
    go generate ./... &
    x.gen.gowrap &
    x.gen.proto &
    x.gen.xo &
    x.gen.sqlc &
    x.gen.jet &

    wait_without_error

    x.gen.pregen

    x.gen.postgen &
    x.gen.counterfeiter & # delay since it depends on generated output too

    wait_without_error

    x.gen.client-server # depends on schemas generated in postgen

    # restart is not robust
    # vscode will randomly lose connection when restarting
    # for pid in $(pidof gopls); do
    #   restart_pid $pid &
    # done

    x.lint
    x.gen.frontend

  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  backup-gen.cleanup
}

# Build code generation custom tools.
x.gen.build-tools() {
  { { {
    out_dir=$BUILD_DIR

    mkdir -p $out_dir
    for cmd in pregen postgen gen-schema oapi-codegen jet; do
      echo "Building $cmd..."
      { go build -o $out_dir/$cmd cmd/$cmd/main.go ||
        echo "${YELLOW}[WARNING] Could not rebuild $cmd. If you made changes to it, build it with a compilable app state${OFF}" >&2; } &
    done

    wait
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Lint the entire project.
x.lint() {
  { { {
    x.lint.sql &
    x.lint.go &
    x.lint.frontend &
    wait
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Format Go files.
x.lint.go() {
  { { {
    files=$(find . \
      -not -path "**/$PROTO_DIR/*" \
      -not -path "**/$PG_REPO_GEN/*" \
      -not -path "**/testdata/*" \
      -not -path "**/node_modules/*" \
      -not -path "**/.venv/*" \
      -not -path "**/*.cache/*" \
      -not -path "**/vendor/*" \
      -not -path "**/*.gen.*" \
      -name "*.go")

    goimports -w $files || echo "Linting failed"
    gofumpt -w $files || echo "Linting failed"

    echo "Success"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Format frontend files.
x.lint.frontend() {
  { { {
    cd frontend
    pnpm run lint:fix
    echo "Success"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Format SQL files.
x.lint.sql() {
  { { {
    SQL_DIRS=(
      "$REPOS_DIR"
      "db"
    )
    for slq_dir in ${SQL_DIRS[@]}; do
      pg_format \
        --spaces 2 \
        --wrap-limit 130 \
        --function-case 2 \
        --keyword-case 1 \
        --placeholder "sqlc\\.(arg|narg)\\(:?[^)]*\\)" \
        --inplace \
        --keep-newline \
        --comma-start \
        --nogrouping \
        $(find "$slq_dir" -name '*.*sql')
    done

    echo "Success"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Run required backend pre-test setup: services, database cleanup, codegen...
# Can be called independently, e.g. before running tests through an IDE.
x.test.backend.setup() {
  backup-gen # Modification of vars inside would be local to subshell (caused by pipeline)
  { { {
    # NOTE: tests run independently in Go so we can't have a function be called and run
    # only once before any test starts
    run_shared_services up -d --build --remove-orphans --wait
    [[ -z $NO_GEN ]] && x.gen
    # no need to migrate, done on every test run internally
    drop_and_recreate_db $POSTGRES_TEST_DB
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
  backup-gen.cleanup
}

# Test backend. Accepts `go test` parameters.
# Args: [...]
x.test.backend() {
  { { {
    x.test.xo

    export POSTGRES_TRACE=false
    export GIN_MODE=release
    yes y 2>/dev/null | POSTGRES_DB=$POSTGRES_TEST_DB x.migrate down || true
    POSTGRES_DB=$POSTGRES_TEST_DB x.migrate up # for post-migration scripts
    echo "Running ${GREEN}\"APP_ENV=$env go test $* ./...\"${OFF}"
    APP_ENV="$env" go test "$@" ./...
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Test frontend.
# Args: [...]
x.test.frontend() {
  # TODO accept vitest args
  frontend_config_setup
  cd frontend
  pnpm run test:no-watch
}

# Test frontend on file changes.
# Args: [...]
x.test.frontend.watch() {
  frontend_config_setup
  cd frontend
  pnpm run test
}

# Run custom xo generation script tests
x.test.xo() {
  { { {
    POSTGRES_DB="$POSTGRES_TEST_DB"
    GEN_POSTGRES_DB="$POSTGRES_DB"

    drop_and_recreate_db $POSTGRES_DB # FIXME wrong gen when tables with equal names in public schema exist as well

    echo "Running xo template tests"

    test_dir="$XO_TEMPLATES_DIR/tests"
    dockerdb_psql -d $POSTGRES_DB <"$test_dir/schema.sql"
    echo "Schema loaded"

    rm -rf "$test_dir/got"
    mkdir -p "$test_dir/got"
    xo_schema -o "$test_dir/got" --debug \
      --schema xo_tests \
      --ignore "*.created_at" \
      --ignore "*.updated_at" >/dev/null || err "Failed xo xo_tests schema generation"

    files=$(find "$test_dir/got" \
      -name "*.go")
    goimports -w $files
    gofumpt -w $files

    go test github.com/danicc097/openapi-go-gin-postgres-sqlc/internal/repos/postgresql/xo-templates/tests

    if ! $test_dir/diff_check; then
      confirm "Do you want to update test snapshot with current changes?" </dev/tty >/dev/tty && {
        rm -rf "$test_dir/want"
        cp -r "$test_dir/got" "$test_dir/want"
      }
    fi

  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Test backend on file changes. Accepts `go test` parameters.
# Args: [...]
x.test.backend.watch() {
  clear

  latency=2
  while true; do
    inotifywait \
      --monitor ./internal/**/* \
      --event=close_write \
      --format='%T %f' \
      --timefmt='%s' |
      while read -r event_time event_file 2>/dev/null || sleep $latency; do
        clear
        APP_ENV="$env" go test "$@" ./... && echo "${GREEN}✓ All tests passing${OFF}"
      done
  done
}

# Build the entire project.
x.build() {
  { { {
    go build -o $BUILD_DIR/rest-server "$PWD"/cmd/rest-server
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Run backend with hot-reloading.
x.run.backend-hr() {
  # TODO wait until healthy, else might have errors connecting to mock oidc server on startup, etc.
  run_shared_services up -d --build --remove-orphans --wait
  # NOTE: building binary very unreliable, leads to bin not found.
  # [[ -z $NO_GEN ]] && pre_build_cmd="bin/project gen" # gen too slow at this point, always disable
  air \
    --build.pre_build_cmd "$pre_build_cmd" \
    --build.cmd "" \
    --build.bin "go run ./cmd/rest-server/ -env=.env.$env" \
    --build.include_ext "go" \
    --build.exclude_regex ".gen.go,_test.go" \
    --build.exclude_dir ".git,tmp,$PROTO_DIR,$PG_REPO_GEN,**/testdata,vendor,frontend,*.cache,$CACHE" \
    --build.stop_watch "internal/rest/,internal/services/" \
    --build.delay 1000 \
    --build.exclude_unchanged "true" |
    sed -e "s/^/${BLUE}[Air]${OFF} /"
}

# Run frontend with hot-reloading.
x.run.frontend() {
  frontend_config_setup
  cd frontend
  pnpm run dev |
    sed -e "s/^/${GREEN}[Vite]${OFF} /"
}

# Run all project services with hot reload enabled in dev mode.
x.run-dev() {
  env="dev"

  run_hot_reload

  while true; do
    sleep 1000
  done

  # TODO fix won't kill children
  # next_allowed_run=$(date +%s)
  # latency=3
  # close_write event, else duplicated, tripl. events -> race condition
  # while true; do
  #   inotifywait \
  #     --monitor $SPEC \
  #     --event=close_write \
  #     --format='%T %f' \
  #     --timefmt='%s' |
  #     while read -r event_time event_file 2>/dev/null || sleep $latency; do
  #       if [[ $event_time -ge $next_allowed_run ]]; then
  #         next_allowed_run=$(date --date="${latency}sec" +%s)

  #         kill_descendants || true

  #         run_hot_reload
  #       fi
  #     done
  # done
}

x.backend.sync-deps() {
  go mod tidy
  go mod vendor
}

# Run project in production mode.
x.run-prod() {
  env="prod"

  run_shared_services up -d --build --remove-orphans --wait
  x.db.recreate

  x.gen

  DOCKER_BUILDKIT=1 BUILDKIT_PROGRESS=plain docker compose \
    --project-name "$PROJECT_PREFIX"_"$env" \
    -f docker-compose."$env".yml \
    --env-file ".env.$env" \
    up -d --build --wait 2>&1 # https://github.com/docker/compose/issues/7346

  echo "Migrations:"
  x.migrate up
}

# Remove running project containers, including shared ones between environments.
x.stop-project() {
  { { {
    run_shared_services down --remove-orphans
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Recreates docker volumes for Postgres, Redis, etc. Unsaved data will be lost.
x.recreate-shared-services() {
  { { {
    run_shared_services up -d --build --force-recreate --wait
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Checks before release:
# - Magic keyword "STOPSHIP" not found in tracked files.
x.release() {
  { { {
    search_stopship "STOPSHIP" &
    # go mod verify & # TODO go.work issues solved in go 1.21 (https://github.com/golang/go/issues/54372)

    wait_without_error
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Shows existing user api keys.
x.dev-utils.api-keys() {
  dockerdb psql --no-psqlrc -d $POSTGRES_DB -c "select email, api_key from user_api_keys left join users using (user_id);"
}

# Setups a traefik container with predefined configuration in `install-dir`.
# Args: install-dir
x.setup.traefik() {
  git clone --depth=1 https://github.com/danicc097/traefik-bootstrap.git "$1"
  docker network create traefik-net || true
  mkdir -p "$1"/traefik/certificates
  cp $CERTIFICATES_DIR/* "$1"/traefik/certificates
  cd "$1" || exit
  ./compose-up
  cd - >/dev/null || exit
}

########################## migrations ##########################

# Wrapper for golang-migrate with predefined configuration.
x.migrate() {
  { { {
    migrate \
      -path $MIGRATIONS_DIR/ \
      -database "postgres://$POSTGRES_USER:$POSTGRES_PASSWORD@localhost:$DB_PORT/$POSTGRES_DB?sslmode=disable" \
      "$@" 2>&1 # migrate outputs everything to stderr

    if [[ "${*:1}" =~ (up)+ ]]; then
      echo "Running post-migration scripts"
      for file in $(find db/post-migration -maxdepth 1 -name '*.sql' | sort); do
        dockerdb_psql -U postgres -d "$POSTGRES_DB" <$file
      done
    fi
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Create a new migration file with the given `name`.
# Args: name
x.migrate.create() {
  { { {
    tmp="$*"
    tmp="${tmp// /_}"
    name="${tmp,,}"
    [[ -z $name ]] && err "Please provide a migration name"
    x.migrate create -ext sql -dir $MIGRATIONS_DIR/ -seq -digits 7 "$name"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

########################## db ##########################

# psql session for the current environment.
x.db.psql() {
  docker exec -it postgres_db_"$PROJECT_PREFIX" psql -d $POSTGRES_DB
}

# psql session for `database`.
# Args: database
x.db.psql-db() {
  docker exec -it postgres_db_"$PROJECT_PREFIX" psql -d $1
}

# Show active and max number of connections for the current environment.
x.db.conns() {
  { { {
    current_conns=$(dockerdb_psql -d $POSTGRES_DB -c "SELECT count(*) FROM pg_stat_activity WHERE datname = '$POSTGRES_DB';")
    max_conns=$(dockerdb_psql -d $POSTGRES_DB -c "SHOW max_connections;")
    echo "$current_conns/$max_conns active connections in '$POSTGRES_DB'"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Create a new database in the current environment if it doesn't exist
# and stops its running processes if any.
x.db.recreate() {
  { { {
    create_db_if_not_exists $POSTGRES_DB
    stop_db_processes $POSTGRES_DB
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Drop and recreate the database in the current environment.
x.db.drop() {
  [[ $env = "prod" ]] && confirm "This will drop all database data. Continue?"
  { { {
    drop_and_recreate_db "$POSTGRES_DB"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Drop and recreate the database used for code generation up to N, N-1 revisions or none.
# Args: {up|up-1|drop}
x.db.gen() {
  { { {
    latest_rev=$(find $MIGRATIONS_DIR/*.sql -maxdepth 0 | wc -l)
    second_latest_rev=$(((latest_rev - 2) / 2)) # up+down
    POSTGRES_DB=$GEN_POSTGRES_DB x.db.drop
    # TODO should be able to autocomplete x function nested cases
    # if it's called as $CMD
    case $1 in
    up)
      POSTGRES_DB=$GEN_POSTGRES_DB x.migrate up
      ;;
    up-1)
      POSTGRES_DB=$GEN_POSTGRES_DB x.migrate up $second_latest_rev
      ;;
    drop) ;;
    *)
      err "Valid options {up|up-1|drop}, got: $1"
      ;;
    esac
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Seed gen database.
x.db.gen.initial-data() {
  { { {
    POSTGRES_DB=$GEN_POSTGRES_DB x.db.initial-data
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Seed database.
x.db.initial-data() {
  # xlog eats up read -p (prompt)
  { { {
    x.db.drop
    x.migrate up
    echo "Loading initial data to $POSTGRES_DB"
    # dockerdb_psql -d $POSTGRES_DB <"./db/initial_data_$env.pgsql"
    go run cmd/initial-data/main.go -env .env.$env
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Backup the database for the current environment.
x.db.dump() {
  { { {
    running_dumps=$(dockerdb_psql -P pager=off -U postgres -d "postgres_$env" \
      -c "SELECT pid FROM pg_stat_activity WHERE application_name = 'pg_dump';")
    if [[ "$running_dumps" != "" ]]; then
      err "pg_dump is already running, aborting new dump"
    fi

    mkdir -p "$DUMPS_DIR"
    schema_v=$(dockerdb_psql -P pager=off -U postgres -d "postgres_$env" \
      -c "SELECT version FROM schema_migrations;")
    dump_file="${DUMP_PREFIX}$(date +%Y-%m-%dT%H-%M-%S)_version${schema_v}.gz"

    echo "Dumping database to $dump_file"
    dockerdb pg_dump -U postgres -d "postgres_$env" |
      gzip >"$DUMPS_DIR/$dump_file"
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Restore the database with the latest dump or `file` for the current environment.
# Args: [file]
x.db.restore() {
  # xlog eats up read -p (prompt)
  dump_file="$1"
  if [[ -n $dump_file ]]; then
    [[ ! -f $dump_file ]] && err "$dump_file does not exist"
    [[ "$dump_file" != *"$DUMP_PREFIX"* ]] && confirm "${RED}Dump doesn't match prefix '$DUMP_PREFIX'. Continue?${OFF}"
  else
    mkdir -p "$DUMPS_DIR"
    latest_dump_file=$(find "$DUMPS_DIR"/ -name "$DUMP_PREFIX*.gz" | sort -r | head -n 1)
    if [[ -z "$latest_dump_file" ]]; then
      err "No $DUMP_PREFIX* file found in $DUMPS_DIR"
    fi
    dump_file="$latest_dump_file"
  fi

  confirm "Do you want to restore ${YELLOW}$dump_file${OFF} in the ${RED}$env${OFF} environment?"

  x.db.drop
  gunzip -c "$dump_file" | dockerdb_psql -U postgres -d "postgres_$env"
  # sanity check, but probably better to do it before restoring...
  dump_schema_v=$(dockerdb_psql -P pager=off -U postgres -d "postgres_$env" -c "SELECT version FROM schema_migrations;")
  file_schema_v=$(echo "$dump_file" | sed -E 's/.*_version([0-9]+)\..*/\1/')
  echo "Migration revision: $dump_schema_v"
  if [[ "$dump_schema_v" != "$file_schema_v" ]]; then
    err "Schema version mismatch: dump $dump_schema_v != file $file_schema_v"
  fi
}

########################## e2e ##########################

# Setup E2E Python environment.
x.e2e.sync-deps() {
  { { {
    cd e2e
    python -m venv .venv
    source .venv/bin/activate
    pip install pip-tools
    pip-compile requirements.in
    pip-compile requirements-dev.in
    pip-sync requirements-dev.txt requirements.txt
    cd - >/dev/null
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# Run E2E tests. Accepts `pytest` parameters.
# Args: [...]
x.e2e.run() {
  { { {
    name="$PROJECT_PREFIX-e2e"
    cd e2e
    DOCKER_BUILDKIT=1 BUILDKIT_PROGRESS=plain \
      docker build -t "$name" .
    docker run -it --rm --ipc=host -v "$(pwd):/src" "$name" bash -c "pytest $*"
    cd - >/dev/null
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

########################## openapi ##########################

# Run a diff against the previous OpenAPI spec in the main branch.
# Can also be used to generate changelogs when upgrading major versions.
x.diff-oas() {
  { { {
    base_spec="/tmp/openapi.yaml"
    git show "main:$SPEC" >"$base_spec"

    tmp="$(yq .info.version "$base_spec")"
    base_v="${tmp%%.*}"
    tmp=$(yq .info.version "$SPEC")
    rev_v="${tmp%%.*}"
    ((rev_v != base_v)) &&
      echo "${YELLOW}Revision mismatch $rev_v and $base_v, skipping diff.${OFF}" && return

    args="-format text -breaking-only -fail-on-diff -exclude-description -exclude-examples"
    if oasdiff $args -base "$base_spec" -revision $SPEC; then
      echo "${GREEN}No breaking changes found in $SPEC${OFF}"
    else
      echo "${RED}Breaking changes found in $SPEC${OFF}"
      return 1
    fi
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

########################### helpers ###########################

# IMPORTANT: bug in declare -F returns line number of last nested function, if any.
# extracting function here instead...
run_hot_reload() {
  x.run.backend-hr &
  pids="$pids $!"
  x.run.frontend &
  pids="$pids $!"
}

run_shared_services() {
  docker network create traefik-net 2>/dev/null || true

  cd docker
  DOCKER_BUILDKIT=1 BUILDKIT_PROGRESS=plain docker compose \
    -p "$PROJECT_PREFIX" \
    -f docker-compose.shared.yml \
    -f docker-compose.oidc.yml \
    --env-file ../.env."$env" \
    "$@" 2>&1 # https://github.com/docker/compose/issues/7346
  cd - >/dev/null
}

# generate db structs for use with swaggest/openapi-go.
# no need for ast parsing since all code is predictable
# type grouping not supported.
generate_structs_map() {
  local structs=()
  local enums=()
  find_go_structs structs "$PG_REPO_GEN/db"
  find_go_enums enums "$PG_REPO_GEN/db"
  find_deleted_pkg_schemas structs enums Db
  for struct in ${structs[@]}; do
    map_fields+=("\"Db$struct\": db.$struct{},")
  done
  map_fields+=("
  //
  ")

  structs=()
  enums=()
  find_go_structs structs "internal/rest/models.go"
  find_go_enums enums "internal/rest/models.go"
  find_deleted_pkg_schemas structs enums Rest
  for struct in ${structs[@]}; do
    map_fields+=("\"Rest$struct\": rest.$struct{},")
  done
  map_fields+=("
  //
  ")

  structs=()
  enums=()
  find_go_structs structs "$REPOS_DIR/models"
  find_go_enums enums "$REPOS_DIR/models"
  find_deleted_pkg_schemas structs enums Models
  for struct in ${structs[@]}; do
    # openapi-go uses last dir name by default.
    # we can intercept and generate based on t.PkgPath()
    map_fields+=("\"Models$struct\": repomodels.$struct{},")
  done

  out="internal/postgen/structs.gen.go"
  cat <<EOF >$out
// Code generated by 'project gen.postgen'. DO NOT EDIT.

package postgen

import (
  db "$GOMOD_PKG/$PG_REPO_GEN/db"
	rest "$GOMOD_PKG/internal/rest"
	repomodels "$GOMOD_PKG/$REPOS_DIR/models"
  )
    var PublicStructs = map[string]any{
$(printf "%s\n" "${map_fields[@]}")
  }
EOF

  gofumpt -w $out
}

sync_db_enums_with_spec() {
  db_search_paths=("public" "other_schema")

  search_path_str=$(printf "'%s'," "${db_search_paths[@]}")
  search_path_str=${search_path_str%,}
  enum_query=$(printf "
    SELECT t.typname AS enum_name
    FROM pg_type t
    INNER JOIN pg_namespace n ON n.oid = t.typnamespace
    WHERE t.typtype = 'e' AND n.nspname IN (%s);" "$search_path_str")

  mapfile -t db_enum_names < <(dockerdb_psql -d $POSTGRES_DB -c "$enum_query" 2>/dev/null)
  for db_enum_name in "${db_enum_names[@]}"; do
    local schema_name="$(to_pascal $db_enum_name)"
    local enum_values=()
    mapfile -t enum_values < <(dockerdb_psql -d $POSTGRES_DB -c "SELECT unnest(enum_range(NULL::\"$db_enum_name\"));" 2>/dev/null)

    local schema_path=".components.schemas.$schema_name"
    if yq -e "$schema_path" $SPEC &>/dev/null; then # -e exits 1 if no match
      if ! yq -e "$schema_path | has(\"x-generated\")" $SPEC &>/dev/null; then
        err "Clashing schema name '$schema_name'. Please remove it before continuing."
      fi
    fi

    src_comment="database enum '$db_enum_name'"
    replace_enum_in_spec "$schema_name" enum_values "$src_comment"

  done

}

replace_enum_in_spec() {
  local enum="$1"
  local -n __arr="$2"
  local src_comment="$3"

  local schema_path=".components.schemas.$enum"

  local __enums
  __enums=$(printf ",\"%s\"" "${__arr[@]}")
  __enums="[${__enums:1}]"

  echo "Replacing '$enum' enum in $SPEC with values from $src_comment"
  __enums=$__enums yq e "
    $schema_path.type = \"string\" |
    $schema_path.enum = env(__enums) |
    $schema_path.x-generated = \"-\" |
    ($schema_path | key) line_comment=\"Generated from $src_comment. DO NOT EDIT.\"" -i $SPEC
}

awk_remove_go_comments='
     /\/\*/ { f=1 } # set flag that is a block comment

     f==0 && !/^\s*(\/\/|\/\*)/ {
        print  # print non-commented lines
     }
     /\*\// { f=0 } # reset flag at end of comment
'

find_go_structs() {
  local -n __arr="$1"
  local pkg="$2"
  mapfile -t __arr < <(find $pkg -maxdepth 1 -name "*.go" -exec awk "$awk_remove_go_comments" {} \; |
    sed -ne 's/[\s]*type\(.*\)struct.*/\1/p')
  if [[ ${#__arr[@]} -eq 0 ]]; then
    err "No structs found in package $pkg"
  fi
  mapfile -t __arr < <(LC_COLLATE=C sort < <(printf "%s\n" "${__arr[@]}"))
}

find_go_interfaces() {
  local -n __arr="$1"
  local pkg="$2"
  mapfile -t __arr < <(find $pkg -maxdepth 1 -name "*.go" -exec awk "$awk_remove_go_comments" {} \; |
    sed -ne 's/[\s]*type\(.*\)interface.*/\1/p')
  if [[ ${#__arr[@]} -eq 0 ]]; then
    err "No interfaces found in package $pkg"
  fi
  mapfile -t __arr < <(LC_COLLATE=C sort < <(printf "%s\n" "${__arr[@]}"))
}

find_go_enums() {
  local -n __arr="$1"
  local pkg="$2"
  mapfile -t __arr < <(find $pkg -maxdepth 1 -name "*.go" -exec awk "$awk_remove_go_comments" {} \; |
    sed -ne 's/.*type[[:space:]]\+\([^=[:space:]]\+\)[[:space:]]\+string.*/\1/p')
  if [[ ${#__arr[@]} -eq 0 ]]; then
    echo "No enums found in package $pkg"
  fi
  mapfile -t __arr < <(LC_COLLATE=C sort < <(printf "%s\n" "${__arr[@]}"))
}

find_deleted_pkg_schemas() {
  local -n __structs="$1"
  local -n __enums="$2"
  local pkg="$3"
  echo "Finding deleted structs or enums from package '$pkg'..."
  mapfile -t spec_schema < <(yq eval '.components.schemas[] | key' "$SPEC" | grep -E "^${pkg}" || true)

  local found=0
  for spec_schema in ${spec_schema[*]}; do
    for struct in ${__structs[@]}; do
      if [[ $spec_schema == "${pkg}$struct" ]]; then
        found=1
      fi
    done
    for enum in ${__enums[@]}; do
      if [[ $spec_schema == "${pkg}$enum" ]]; then
        found=1
      fi
    done
    ((found == 0)) && echo "${YELLOW}[WARNING] $SPEC schema $spec_schema no longer exists in package '$(to_lower $pkg)'. Remove if necessary.${OFF}"
    found=0
  done
}

update_spec_with_structs() {
  vext="x-postgen-struct"
  struct_names=$(yq e ".components.schemas[] | select(has(\"$vext\")).$vext" $SPEC)
  schema_names=$(yq e ".components.schemas[] | select(has(\"$vext\")) | key" $SPEC)
  mapfile -t struct_names <<<$struct_names
  mapfile -t schema_names <<<$schema_names

  declare -A schemas
  for i in ${!struct_names[@]}; do
    schemas["${struct_names[$i]}"]="${schema_names[$i]}" # keep track of custom structs per schema name
  done
  struct_names_list=$(join_by "," ${struct_names[*]})
  ((${#struct_names_list} == 0)) && return

  gen-schema --struct-names $struct_names_list | yq '
    with_entries(select(.key == "components"))' \
    >/tmp/openapi.yaml

  # prevent possible overriding if name clashes with generated schemas from structs
  skip_schemas=("Project")
  mapfile -t db_projects < <(dockerdb_psql -d $POSTGRES_DB -c "select name from projects;" 2>/dev/null)
  for project in ${db_projects[@]}; do
    skip_schemas+=("$(to_pascal $project)KanbanSteps")
  done

  # replace every schema back into the spec
  for schema in $(yq '.components.schemas[] | key' /tmp/openapi.yaml); do
    # prevent certain schemas from getting replaced by (in)direct usage in structs with clashing names
    if [[ " ${skip_schemas[*]} " =~ " $schema " ]]; then
      echo "Skipping $schema" && continue
    fi
    new_schema_key="${schemas[$schema]:-$schema}"
    yq eval-all "(
        select(fi == 1).components.schemas.$schema
        ) as \$new_schema
        | select(fi == 0)
        | .components.schemas.$new_schema_key = \$new_schema
        | (.components.schemas.$new_schema_key | key) line_comment=\"Generated from internal structs. DO NOT EDIT.\"
      " "$SPEC" /tmp/openapi.yaml >/tmp/final-spec

    mv /tmp/final-spec "$SPEC"
  done

  sed -i 's/!!merge //' $SPEC
}

delete_duplicate_schemas() {
  # generated by gen-schema,
  # since it cannot be aware of them beforehand and `ref` tag is not used by openapi-go
  # (maybe it could...)
  local keys=("ModelsRole" "ModelsScope" "ModelsProjectConfigField" "ModelsProjectConfig" "ModelsProject")
  local paths_arr=()
  for key in "${keys[@]}"; do
    echo "Deleting ${key} (duplicate)"
    paths_arr+=(".components.schemas.$key")
  done
  local paths=$(join_by "," "${paths_arr[@]}")
  yq e "del($paths)" -i "$SPEC"
}

xo_schema() {
  # xo cannot use db files as input, needs an up-to-date schema
  # not recreating db on every gen can lead to plain wrong generation based on an old dev schema.
  # Also use a unique db to prevent cosmic accidents
  xo schema "postgres://$POSTGRES_USER:$POSTGRES_PASSWORD@localhost:$DB_PORT/$GEN_POSTGRES_DB?sslmode=disable" \
    --src "$XO_TEMPLATES_DIR" \
    "$@"
}

drop-and-migrate-gen-db() {
  test -n "$gen_migrated" && return

  gen_migrated="${FUNCNAME[1]}"
  { { {
    POSTGRES_DB="$GEN_POSTGRES_DB"

    x.db.drop
    x.migrate up
  } 2>&4 | xlog >&3; } 4>&1 | xerr >&3; } 3>&1
}

# updates dynamic config with env vars
frontend_config_setup() {
  export ENV_REPLACE_GLOB=frontend/config.json
  # ensure config has all k:v as "<KEY>": "$<KEY>"
  # this has to always be run at startup
  jq \
    'to_entries | map_values({ (.key) : ("$" + .key) }) | reduce .[] as $item ({}; . + $item)' \
    frontend/config.template.json >/tmp/config.tmp.json && mv /tmp/config.tmp.json "$ENV_REPLACE_GLOB"
  envvars=$(printenv | awk -F= '{print $1}' | sed 's/^/\$/g' | paste -sd,)
  frontend/nginx/replace-envvars.sh "$envvars"
}

# saves the current gen state to allow restoring in the event of early exit
backup-gen() {
  test -n "$backup_gen_caller" && return

  backup_gen_caller="${FUNCNAME[1]}"

  rm -rf "$GEN_BACKUP"
  mkdir -p "$GEN_BACKUP"
  mkdir -p "$FRONTEND_GEN_BACKUP"
  mkdir -p "$PG_REPO_GEN_BACKUP"
  backup_gen_files internal "$GEN_BACKUP"

  cp "$SPEC" "$GEN_BACKUP"
  rsync -a --delete "$FRONTEND_GEN/" "$FRONTEND_GEN_BACKUP"
  rsync -a --delete "$PG_REPO_GEN/" "$PG_REPO_GEN_BACKUP"

  restore_cache=1
}

backup-gen.cleanup() {
  if [[ "$backup_gen_caller" = "${FUNCNAME[1]}" ]]; then
    restore_cache=0
  fi
}

backup-gen.restore() {
  echo "Restoring previously generated code..."
  wait
  mv "$GEN_BACKUP/$SPEC" "$SPEC"
  rsync -a --delete "$PG_REPO_GEN_BACKUP/" "$PG_REPO_GEN/"
  rsync -a --delete "$FRONTEND_GEN_BACKUP/" "$FRONTEND_GEN/"
  restore_gen_files "$GEN_BACKUP" internal

  rm -f "$GEN_BACKUP"/*.md5 # clear gen cache
}

backup_gen_files() {
  src_dir=$1
  backup_dir=$2

  mkdir -p "$backup_dir"

  # -print0 alongside --null handle file names with spaces
  find "$src_dir" -name "*.gen.go" -type f -print0 | tar czvf "$backup_dir/backup_gen_files.tar.gz" --null -T - &>/dev/null

}

restore_gen_files() {
  backup_dir=$1
  dest_dir=$2

  # -C changes cwd to dir before extracting
  tar xzvf "$backup_dir/backup_gen_files.tar.gz" -C "$dest_dir" --overwrite --strip-components=1 &>/dev/null
}

######################## ENTRYPOINT ########################

set -Eeo pipefail
set -o errtrace

env="dev"

# --------------------- completion and delegation --------------------
#      `complete -C foo foo` > `source <(foo bloated_completion)`

while IFS= read -r line; do
  [[ $line =~ ^declare\ -f\ x\. ]] || continue
  COMMANDS+=("${line##declare -f x.}")
done < <(declare -F)
# sort the array. Mimic file input to sort
mapfile -t COMMANDS < \
  <(LC_COLLATE=C sort < <(printf "%s\n" "${COMMANDS[@]}"))

MAX_XFN_LEN=0 # for logging purposes
for c in "${COMMANDS[@]}"; do
  len=${#c}
  ((len > MAX_XFN_LEN)) && MAX_XFN_LEN=$((len - 1)) # remove "x." but account for extra last space appended.
done

if [[ -n $COMP_LINE ]]; then
  pre="${COMP_LINE##* }" # the part after the last space in the current command
  cur_commands=(${COMP_LINE%"$pre"})

  for c in "${COMMANDS[@]}"; do
    if [[ " ${cur_commands[*]} " =~ " ${c} " ]]; then
      xfn_specified=true
      break
    fi
  done

  for c in "${COMMANDS[@]}"; do
    test -z "${xfn_specified}" || break
    test -z "${pre}" -o "${c}" != "${c#"${pre}"}" -a "${pre}" != "${c}" && echo "${c}"
  done

  test -z "${xfn_specified}" && exit

  declare __x_options x_options_lines

  parse_x_options x_options_lines

  for c in "${x_options_lines[@]}"; do
    tmp="${c%%)*}"
    xopt="${tmp//\*/}"
    __x_options+=("$xopt")
  done

  declare -A __x_opts_seen
  for cmd in "${cur_commands[@]}"; do
    for opt in ${__x_options[@]}; do
      if [[ "$cmd" == *"$opt"* ]]; then
        __x_opts_seen[$opt]=true
        break
      fi
    done
  done

  for opt in ${__x_options[@]}; do
    [[ -n "${__x_opts_seen[$opt]}" ]] && continue
    # TODO prevent opts that accept values having whitespace added
    [[ ${opt:0:${#pre}} == "${pre,,}" ]] && echo "${opt}"
  done

  exit
fi

declare CMD="$1"

# First comment lines automatically added to usage docs.
set +e
while [[ "$#" -gt 0 ]]; do
  case $1 in
  --x-help)
    # Show help for a particular x function.
    COMMANDS=("$CMD")
    usage
    exit
    ;;
  --x-force-regen)
    # Removes generation cache, forcing a new run.
    export FORCE_REGEN=1
    ;;
  --x-no-confirmation)
    # Bypasses confirmation messages. (WIP: Use `yes` in the meantime)
    export NO_CONFIRMATION=1
    ;;
  --x-no-gen)
    # Skips code generation steps.
    export NO_GEN=1
    ;;
  --x-no-cache)
    # Code generation backup is not restored on failure.
    export NO_CACHE=1
    ;;
  --x-env=*)
    # Environment to run commands in. Defaults to "dev".
    # Args: env
    env="${1#--x-env=}"
    valid_envs="dev staging prod ci"
    if [[ ! " ${valid_envs[*]} " =~ " $env " ]]; then
      err "Valid environments: $valid_envs"
    fi
    ;;
  *)
    # will set everything else back
    args+=("$1")
    ;;
  esac
  shift
done

set -e # was temporarily removed for arg parsing
for arg in ${args[@]}; do
  set -- "$@" "$arg"
done

######################## INIT ########################

# applicable to any command
ensure_envvars_set ".env.template" ".env.${env}"
ensure_envvars_set "docker/.env.template" "docker/.env"

set -a
# export to all subsequent commands or scripts
source ".env.$env"
set +a

trap 'show_tracebacks' ERR
trap killgroup SIGINT
trap errtrap SIGUSR1
trap 'exit-cleanup $LINENO' 0 # EXIT (0) executed on exit from the shell

exit-cleanup() {
  if [[ $restore_cache = 1 && -z $NO_CACHE ]]; then
    backup-gen.restore #
  fi
  cd "$starting_cwd" || true
}

killgroup() {
  printf "\nKilling spawned processes...\n\n"
  # kill $(jobs -p) 2>/dev/null # doesn't really kill all children, just process group leaders
  kill_descendants 2>/dev/null || true
  pgrep -P $PROC | xargs kill || true
  exit 1
}

errtrap() {
  printf "\nExiting due to propagated error...\n"
  killgroup
}

source ".envrc"

pids=""
DUMP_PREFIX="dump_${env}_"

readonly SPEC="openapi.yaml"

readonly BUILD_DIR="bin/build"
readonly PROTO_DIR="internal/pb"
readonly MIGRATIONS_DIR="db/migrations"
readonly CERTIFICATES_DIR="certificates"
readonly GOWRAP_TEMPLATES_DIR="internal/gowrap-templates"
readonly REPOS_DIR="internal/repos"
readonly PG_REPO_DIR="$REPOS_DIR/postgresql"
readonly XO_TEMPLATES_DIR="$PG_REPO_DIR/xo-templates"

readonly POSTGRES_TEST_DB="postgres_test"
readonly DUMPS_DIR="$HOME/openapi_go_gin_postgres_dumps"
pkg="$(head -1 go.mod)"
readonly GOMOD_PKG="${pkg#module *}"
readonly OPID_AUTH="operationAuth.gen.json"
readonly PG_REPO_GEN="$PG_REPO_DIR/gen"
readonly FRONTEND_SRC="frontend/src"
readonly FRONTEND_GEN="$FRONTEND_SRC/gen"
# common shared file dependencies for codegen checksums
readonly CACHE_DEP_FILES=("bin/project bin/.helpers.sh .env.$env")
readonly CACHE=".generate.cache"
readonly GEN_BACKUP="$CACHE/gen"
readonly PG_REPO_GEN_BACKUP="$GEN_BACKUP/pgrepo/gen"
readonly FRONTEND_GEN_BACKUP="$GEN_BACKUP/frontend/gen"

readonly MAX_FNAME_LOG_LEN=13

GEN_POSTGRES_DB="gen_db"

# determines if gen cache should be restore at program exit. 0|1
restore_cache=0
# stores the first executing function to track if caching gen is already running,
# to allow for nested backup-gen and cache-cleanup inside multiple functions.
backup_gen_caller=""

# stores the first executing function to determine if a migration
# is needed when running gen* functions which call each other
gen_migrated=""

################ handle executing x functions ################

if [[ -n "$1" ]]; then
  shift
  for c in "${COMMANDS[@]}"; do
    declare cmd=$(command -v "x.$c")
    if [[ $c == "$CMD" && -n "$cmd" ]]; then
      "x.$CMD" "$@"
      exit $?
    fi
  done
fi

# default to show usage if its a noop
usage
